<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>Category: 学术 | 心怀畏惧</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="chen hao&apos;s blog | machine learning | data mining">
<meta property="og:type" content="website">
<meta property="og:title" content="心怀畏惧">
<meta property="og:url" content="http://crescentluna.github.io/categories/学术/index.html">
<meta property="og:site_name" content="心怀畏惧">
<meta property="og:description" content="chen hao&apos;s blog | machine learning | data mining">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="心怀畏惧">
<meta name="twitter:description" content="chen hao&apos;s blog | machine learning | data mining">
  
    <link rel="alternative" href="/atom.xml" title="心怀畏惧" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link rel="stylesheet" href="/css/style.css">
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
<script type="text/javascript">
var _bdhmProtocol = (("https:" == document.location.protocol) ? " https://" : " http://");
document.write(unescape("%3Cscript src='" + _bdhmProtocol + "hm.baidu.com/h.js%3F1fc57dc5ccfaeae31f8e295269b6fa04' type='text/javascript'%3E%3C/script%3E"));
</script>


  
<!-- Google Analytics -->
<script type="text/javascript">
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-57404093-1', 'auto');
ga('send', 'pageview');

</script>
<!-- End Google Analytics -->

  <script src="https://cdn1.lncld.net/static/js/av-min-1.2.1.js"></script>
  <script>AV.initialize("57nirzk8g437patyg9434lcvmpkat6e5lu5mixmrwqa8l3ao", "rsqg6iiq58eyeg8c6c6uue48crtwt5quuq5dmjaahfcjlkq8");</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">心怀畏惧</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">Do not go gentle into that good night</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">主页</a>
        
          <a class="main-nav-link" href="/archives">所有文章</a>
        
          <a class="main-nav-link" href="/about">关于</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="http://www.baidu.com/baidu" method="get" accept-charset="utf-8" class="search-form">
          <input type="search" name="word" maxlength="20" class="search-form-input" placeholder="Search">
          <input type="submit" value="" class="search-form-submit">
          <input name=tn type=hidden value="bds">
          <input name=cl type=hidden value="3">
          <input name=ct type=hidden value="2097152">
          <input type="hidden" name="si" value="crescentluna.github.io">
        </form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-variational-inference-3" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2014/12/12/variational-inference-3/" class="article-date">
  <time datetime="2014-12-12T14:54:14.000Z" itemprop="datePublished">2014-12-12</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/学术/">学术</a>►<a class="article-category-link" href="/categories/学术/变分推断笔记/">变分推断笔记</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2014/12/12/variational-inference-3/">变分推断学习笔记(3)——三硬币问题的变分推断解法</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>变分推断学习笔记系列：</p>
<ol style="list-style-type: decimal">
<li><a href="http://crescentluna.github.io/2013/10/03/%E5%8F%98%E5%88%86%E6%8E%A8%E6%96%AD%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01%E2%80%94%E2%80%94%E6%A6%82%E5%BF%B5%E4%BB%8B%E7%BB%8D/">变分推断学习笔记(1)——概念介绍</a></li>
<li><a href="http://crescentluna.github.io/2013/10/11/variational-inference-2/">变分推断学习笔记(2)——一维高斯模型的例子</a></li>
<li><a href="http://crescentluna.github.io/2014/12/12/variational-inference-3/">变分推断学习笔记(3)——三硬币问题的变分推断解法</a>
<hr>
</hr></li>
</ol>
<p><del>其实三硬币的例子不写，前面的介绍也够了，写这个纯粹是吃撑了</del>。这次我们采取更加普遍的假设，将原先假设的3枚硬币拓展开来。现在假设有<span class="math inline">\(K+1\)</span>个骰子，第一个骰子有<span class="math inline">\(K\)</span>个面，其余的骰子有<span class="math inline">\(T\)</span>个面。进行如下实验：先掷第一个骰子，根据投出的结果<span class="math inline">\(Z_k\)</span>，选择第<span class="math inline">\(Z_k\)</span>个骰子再投，观测到投出的<span class="math inline">\(N\)</span>个结果，每个结果<span class="math inline">\(w_n\)</span>可能是 <span class="math display">\[
1，3，7，8，3，2，6，9，...
\]</span></p>
<p>可以看到现在第1个骰子投出的标签服从多项分布： <span class="math display">\[Z_k \sim Multinomial(\pi)\]</span> 然后剩余骰子投出的面也服从多项分布 <span class="math display">\[W_{Z_{kt}} \sim Multinomial(\theta_{Z_k})\]</span> 我们假设，随机变量<span class="math inline">\(\pi\)</span>和<span class="math inline">\(\theta\)</span>的先验分布为狄利克雷分布，超参分别为<span class="math inline">\(\alpha\)</span>和<span class="math inline">\(\beta\)</span>。
        
          <p class="article-more-link">
            <a href="/2014/12/12/variational-inference-3/#more">Read More</a>
          </p>
        
      
    </div>
    <footer class="article-footer">
      
        <a data-url="http://crescentluna.github.io/2014/12/12/variational-inference-3/" data-id="cizha1r0d0028g0o8qtrmqbjl" class="article-share-link" data-share="baidu">分享到</a>
      

      
        <a href="http://crescentluna.github.io/2014/12/12/variational-inference-3/#ds-thread" class="article-comment-link">Comments</a>
      

      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Variational-Inference/">Variational Inference</a></li></ul>

    </footer>
  </div>
  
</article>




  
    <article id="post-概率图模型简介" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2014/02/24/概率图模型简介/" class="article-date">
  <time datetime="2014-02-23T21:56:02.000Z" itemprop="datePublished">2014-02-24</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/学术/">学术</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2014/02/24/概率图模型简介/">概率图模型简介</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="介绍">介绍</h1>
<h2 id="定义">定义</h2>
<p>概率图模型(Graphical Model)是概率论和图论之间的桥梁，概率图模型的基本想法来自模块的概念，即一个复杂系统是由简单的部分所联合而成的。概率论部分告诉我们哪些部分是耦合在一起的，然后提供推断模型的方法，而图论部分给我们一个非常直观的认识，把拥有相互关系的变量看做数据结构，从而导出一般化的方法来解决这个问题。很多经典的多元概率系统，比如混合模型，因子分析，隐含马尔科夫模型,Kalman filter 和Ising model等，从概率图模型的角度来看，都可以看做普遍隐含形成过程的一种实例表现。这意味着，一旦在某个系统上有什么特别的方法发现，就很容易推广到一系列的模型中去。除此之外，概率图模型还非常自然地提供了设计新系统的方法。</p>
<p>在概率图模型中，点代表随机变量，点与点之间边的存在与否代表了点与点之间是存在条件依赖。点与边的组合描绘了联合概率分布的特征结构。假设有<span class="math inline">\(N\)</span>个二元随机变量，在没有任何信息帮助的情况下，联合分布<span class="math inline">\(P(X_1,\ldots,X_N)\)</span>，需要<span class="math inline">\(O(2^N)\)</span>个参数。而通过概率图描绘点与点之间的条件关系之后，表示联合分布，所需要的参数会减少很多，这对后面的模型的推断和学习是有帮助的。</p>
<p>概率图模型主要分为两种，无向图(也叫Markov random fields)和有向图(也叫Bayesian networks)，前者多用于物理和图像领域，后者多用于AI和机器学习，具体的基本就不多介绍了。下面给出一个简单贝叶斯网络的例子。</p>
<p><a href="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20140224133640.png"><img src="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20140224133640.png" alt="QQ截图20140224133640" /></a></p>
<p>这里Cloudy指是否多云，Sprinkler指洒水器是否打开，Rain指是否有雨， WetGrass指草地是否湿了。</p>
        
          <p class="article-more-link">
            <a href="/2014/02/24/概率图模型简介/#more">Read More</a>
          </p>
        
      
    </div>
    <footer class="article-footer">
      
        <a data-url="http://crescentluna.github.io/2014/02/24/概率图模型简介/" data-id="cizha1r0h002gg0o8zx0ffpiv" class="article-share-link" data-share="baidu">分享到</a>
      

      
        <a href="http://crescentluna.github.io/2014/02/24/概率图模型简介/#ds-thread" class="article-comment-link">Comments</a>
      

      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/概率图/">概率图</a></li></ul>

    </footer>
  </div>
  
</article>




  
    <article id="post-variational-inference-2" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2013/10/11/variational-inference-2/" class="article-date">
  <time datetime="2013-10-11T00:10:18.000Z" itemprop="datePublished">2013-10-11</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/学术/">学术</a>►<a class="article-category-link" href="/categories/学术/变分推断笔记/">变分推断笔记</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2013/10/11/variational-inference-2/">变分推断学习笔记(2)——一维高斯模型的例子</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>变分推断学习笔记系列：</p>
<ol style="list-style-type: decimal">
<li><a href="http://crescentluna.github.io/2013/10/03/%E5%8F%98%E5%88%86%E6%8E%A8%E6%96%AD%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01%E2%80%94%E2%80%94%E6%A6%82%E5%BF%B5%E4%BB%8B%E7%BB%8D/">变分推断学习笔记(1)——概念介绍</a></li>
<li><a href="http://crescentluna.github.io/2013/10/11/variational-inference-2/">变分推断学习笔记(2)——一维高斯模型的例子</a></li>
<li><a href="http://crescentluna.github.io/2014/12/12/variational-inference-3/">变分推断学习笔记(3)——三硬币问题的变分推断解法</a></li>
</ol>
<hr>
</hr>
<p>举一个一元高斯模型的例子。假设我们有数据<span class="math inline">\(\mathbf{X}=\{x_1,\ldots,x_M\}\)</span>，要推断平均值<span class="math inline">\(\mu\)</span>和精度<span class="math inline">\(\tau(1/\sigma)\)</span>的后验概率分布。 写出似然 <span class="math display">\[\begin{equation}
p(\mathbf{X}|\mu,\tau)=(\frac{\tau}{2\pi})^{N/2}\exp\{-\frac{\tau}{2}\sum^N_{n=1}(x_n-\mu)^2\}
\end{equation}\]</span> 其中<span class="math inline">\(\mu,\tau\)</span>各自服从先验分布 <span class="math display">\[\begin{equation}p(\mu|\tau)=N(\mu|\mu,(\lambda_0\tau)^{-1})\end{equation}\]</span> <span class="math display">\[\begin{equation}p(\tau)=Gam(\tau|a_0,b_0)\end{equation}\]</span> 其中Gam为Gamma分布（见备注1）。</p>
<h2 id="通用的估计方法">通用的估计方法</h2>
<p>好，我们现在假设<span class="math inline">\(q\)</span>之间的分布都独立。 <span class="math display">\[\begin{equation}q(\mu,\tau)=q_u(\mu)q_r(\tau)\end{equation}\]</span>
        
          <p class="article-more-link">
            <a href="/2013/10/11/variational-inference-2/#more">Read More</a>
          </p>
        
      
    </div>
    <footer class="article-footer">
      
        <a data-url="http://crescentluna.github.io/2013/10/11/variational-inference-2/" data-id="cizha1r0a0022g0o8f6cnp84q" class="article-share-link" data-share="baidu">分享到</a>
      

      
        <a href="http://crescentluna.github.io/2013/10/11/variational-inference-2/#ds-thread" class="article-comment-link">Comments</a>
      

      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Variational-Inference/">Variational Inference</a></li></ul>

    </footer>
  </div>
  
</article>




  
    <article id="post-变分推断学习笔记1——概念介绍" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2013/10/03/变分推断学习笔记1——概念介绍/" class="article-date">
  <time datetime="2013-10-03T04:25:24.000Z" itemprop="datePublished">2013-10-03</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/学术/">学术</a>►<a class="article-category-link" href="/categories/学术/变分推断笔记/">变分推断笔记</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2013/10/03/变分推断学习笔记1——概念介绍/">变分推断学习笔记(1)——概念介绍</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>变分推断学习笔记系列：</p>
<ol style="list-style-type: decimal">
<li><a href="http://crescentluna.github.io/2013/10/03/%E5%8F%98%E5%88%86%E6%8E%A8%E6%96%AD%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01%E2%80%94%E2%80%94%E6%A6%82%E5%BF%B5%E4%BB%8B%E7%BB%8D/">变分推断学习笔记(1)——概念介绍</a></li>
<li><a href="http://crescentluna.github.io/2013/10/11/variational-inference-2/">变分推断学习笔记(2)——一维高斯模型的例子</a></li>
<li><a href="http://crescentluna.github.io/2014/12/12/variational-inference-3/">变分推断学习笔记(3)——三硬币问题的变分推断解法</a></li>
</ol>
<hr>
</hr>
<h2 id="问题描述">问题描述</h2>
<p>变分推断是一类用于贝叶斯估计和机器学习领域中近似计算复杂（intractable）积分的技术，它广泛应用于各种复杂模型的推断。本文是学习PRML第10章的一篇笔记，错误或不足的地方敬请指出。</p>
<p>先给出问题描述。记得在上一篇EM的文章中，我们有一个观察变量<span class="math inline">\(\mathbf{X}=\{x^{\{1\}},\ldots,x^{\{m\}}\}\)</span>和隐藏变量<span class="math inline">\(\mathbf{Z}=\{z^{\{1\}},\ldots,z^{\{m\}}\}\)</span>, 整个模型<span class="math inline">\(p(\mathbf{X},\mathbf{Z})\)</span>是个关于变量<span class="math inline">\(\mathbf{X},\mathbf{Z}\)</span>的联合分布，我们的目标是得到后验分布<span class="math inline">\(P(\mathbf{Z}|\mathbf{X})\)</span>的一个近似分布。</p>
<p>在之前介绍过Gibbs Sampling这一类Monte Carlo算法，它们的做法就是通过抽取大量的样本估计真实的后验分布。而变分推断不同，与此不同的是，变分推断限制近似分布的类型，从而得到一种局部最优，但具有确定解的近似后验分布。</p>
之前<a href="http://www.crescentmoon.info/?p=171">在EM算法的介绍中</a>我们有似然的式子如下： <span class="math display">\[\begin{equation}\ln p(\mathbf{X})=L(q)+KL(q||p)\end{equation}\]</span> 其中
<div>
<span class="math display">\[\begin{equation}L(q)=\int q(\mathbf{Z})\ln{\frac{p(\mathbf{X},\mathbf{Z})}{q(\mathbf{Z})}}d\mathbf{Z}\end{equation}\]</span>
</div>
<div>
<span class="math display">\[\begin{equation}KL(q||p)=-\int q(\mathbf{Z}) \ln{\frac{p(\mathbf{Z}|\mathbf{X})}{q(\mathbf{Z})}}d\mathbf{Z}\end{equation}\]</span>
</div>
<p>这里公式中不再出现参数<span class="math inline">\(\theta\)</span>，因为参数不再是固定的值，而变成了随机变量，所以也是隐藏变量，包括在<span class="math inline">\(\mathbf{Z}\)</span>之内了。</p>
        
          <p class="article-more-link">
            <a href="/2013/10/03/变分推断学习笔记1——概念介绍/#more">Read More</a>
          </p>
        
      
    </div>
    <footer class="article-footer">
      
        <a data-url="http://crescentluna.github.io/2013/10/03/变分推断学习笔记1——概念介绍/" data-id="cizha1r09001zg0o8ja8ogjao" class="article-share-link" data-share="baidu">分享到</a>
      

      
        <a href="http://crescentluna.github.io/2013/10/03/变分推断学习笔记1——概念介绍/#ds-thread" class="article-comment-link">Comments</a>
      

      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/PRML/">PRML</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Variational-Inference/">Variational Inference</a></li></ul>

    </footer>
  </div>
  
</article>




  
    <article id="post-三硬币问题-一个EM算法和Gibbs Sampling的例子" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2013/07/03/三硬币问题-一个EM算法和Gibbs Sampling的例子/" class="article-date">
  <time datetime="2013-07-02T17:41:26.000Z" itemprop="datePublished">2013-07-03</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/学术/">学术</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2013/07/03/三硬币问题-一个EM算法和Gibbs Sampling的例子/">三硬币问题-一个EM算法和Gibbs Sampling的例子</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>讲一个EM算法和Gibbs 抽样的小例子，用于加深理解（变分推断版本请见<a href="http://crescentluna.github.io/2014/12/12/variational-inference-3/">变分推断学习笔记(3)——三硬币问题的变分推断解法</a>）。</p>
<p>题目(引用自参考1）：假设有3枚硬币，分别记做A，B，C。这些硬币正面出现的概率分别是<span class="math inline">\(\pi\)</span>,<span class="math inline">\(p\)</span>和<span class="math inline">\(q\)</span>。进行如下掷硬币实验：先掷硬币A，根据其结果选出硬币B或C，正面选B，反面选硬币C；然后投掷选重中的硬币，出现正面记作1，反面记作0；独立地重复<span class="math inline">\(n\)</span>次（n=10)，结果为 <span class="math display">\[1111110000\]</span> 我们只能观察投掷硬币的结果，而不知其过程，估计这三个参数<span class="math inline">\(\pi\)</span>,<span class="math inline">\(p\)</span>和<span class="math inline">\(q\)</span>。</p>
<h1 id="em算法">EM算法</h1>
<p>可以看到投掷硬币时到底选择了B或者C是未知的。我们设隐藏变量Z 来指示来自于哪个硬币，<span class="math inline">\(Z=\{z_1,z_2,\ldots,z_n \}\)</span>，令<span class="math inline">\(\theta=\{\pi,p,q\}\)</span>，观察数据<span class="math inline">\(X=\{x_1,x_2,\ldots,x_n \}\)</span>。</p>
<p>写出生成一个硬币时的概率： <span class="math display">\[\begin{split}P(x|\theta) &amp; =\sum_z P(x,z|\theta)=\sum_z P(z|\pi)P(x|z,\theta) \\&amp; =\pi p^x (1-p)^{1-x}+(1-\pi)q^x(1-q)^{1-x} \\\end{split}\]</span> 有了一个硬币的概率，我们就可以写出所有观察数据的log似然函数： <span class="math display">\[L(\theta|X)=\log P(X|\theta)=\sum^n_{j=1}\log[\pi p^{x_j} (1-p)^{1-{x_j}}+(1-\pi)q^{x_j}(1-q)^{1-{x_j}}]\]</span> 然后求极大似然 <span class="math display">\[\hat{\theta}=\arg \max L(\theta|X)\]</span> 其中<span class="math inline">\(L(\theta|X)=\log P(X|\theta)=\log \sum_Z P(X,Z|\theta)\)</span>。因为log里面带着加和所以这个极大似然是求不出解析解的。
        
          <p class="article-more-link">
            <a href="/2013/07/03/三硬币问题-一个EM算法和Gibbs Sampling的例子/#more">Read More</a>
          </p>
        
      
    </div>
    <footer class="article-footer">
      
        <a data-url="http://crescentluna.github.io/2013/07/03/三硬币问题-一个EM算法和Gibbs Sampling的例子/" data-id="cizha1r03001ng0o8dm3ighkf" class="article-share-link" data-share="baidu">分享到</a>
      

      
        <a href="http://crescentluna.github.io/2013/07/03/三硬币问题-一个EM算法和Gibbs Sampling的例子/#ds-thread" class="article-comment-link">Comments</a>
      

      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/EM/">EM</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Gibbs-Sampling/">Gibbs Sampling</a></li></ul>

    </footer>
  </div>
  
</article>




  
    <article id="post-Gibbs Sampling for the UniniTiated-3" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2013/06/29/Gibbs Sampling for the UniniTiated-3/" class="article-date">
  <time datetime="2013-06-29T04:59:37.000Z" itemprop="datePublished">2013-06-29</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/学术/">学术</a>►<a class="article-category-link" href="/categories/学术/Gibbs-Sampling笔记/">Gibbs Sampling笔记</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2013/06/29/Gibbs Sampling for the UniniTiated-3/">《Gibbs Sampling for the UniniTiated》阅读笔记(下)---连续型参数求积分的思考</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>《Gibbs Sampling for the UniniTiated》阅读笔记结构：</p>
<ol style="list-style-type: decimal">
<li> <a href="http://www.crescentmoon.info/?p=504">参数估计方法及Gibbs Sampling简介</a></li>
<li><a href="http://www.crescentmoon.info/?p=525">一个朴素贝叶斯文档模型例子</a><br />
</li>
<li><a href="http://www.crescentmoon.info/?p=548">连续型参数求积分的思考</a>
<hr>
</hr></li>
</ol>
<p>这篇是下篇，讨论中篇联合分布中对参数求积分来简化的问题。</p>
<p>之前存在的一个问题就是为啥我们可以对连续参数<span class="math inline">\(\pi\)</span>求积分消去它，而不能对词分布<span class="math inline">\(\theta_0\)</span>和<span class="math inline">\(\theta_1\)</span>求积分。这个主意看上去很美，但是实际做的时候，你会碰到一大把无法约掉的伽马函数。让我们看看具体的过程。</p>
        
          <p class="article-more-link">
            <a href="/2013/06/29/Gibbs Sampling for the UniniTiated-3/#more">Read More</a>
          </p>
        
      
    </div>
    <footer class="article-footer">
      
        <a data-url="http://crescentluna.github.io/2013/06/29/Gibbs Sampling for the UniniTiated-3/" data-id="cizha1r00001fg0o86g54jus3" class="article-share-link" data-share="baidu">分享到</a>
      

      
        <a href="http://crescentluna.github.io/2013/06/29/Gibbs Sampling for the UniniTiated-3/#ds-thread" class="article-comment-link">Comments</a>
      

      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Gibbs-Sampling/">Gibbs Sampling</a></li></ul>

    </footer>
  </div>
  
</article>




  
    <article id="post-Gibbs Sampling for the UniniTiated-2" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2013/06/29/Gibbs Sampling for the UniniTiated-2/" class="article-date">
  <time datetime="2013-06-29T04:49:12.000Z" itemprop="datePublished">2013-06-29</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/学术/">学术</a>►<a class="article-category-link" href="/categories/学术/Gibbs-Sampling笔记/">Gibbs Sampling笔记</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2013/06/29/Gibbs Sampling for the UniniTiated-2/">《Gibbs Sampling for the UniniTiated》阅读笔记(中)---一个朴素贝叶斯文档模型例子</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>《Gibbs Sampling for the UniniTiated》阅读笔记结构：</p>
<ol style="list-style-type: decimal">
<li> <a href="http://www.crescentmoon.info/?p=504">参数估计方法及Gibbs Sampling简介</a></li>
<li><a href="http://www.crescentmoon.info/?p=525">一个朴素贝叶斯文档模型例子</a></li>
<li><a href="http://www.crescentmoon.info/?p=548">连续型参数求积分的思考</a></li>
</ol>
<hr>
</hr>
<p>这篇是中篇，介绍一个非常简单的朴素贝叶斯文档模型生成的例子，用来说明Gibbs Sampler具体是如何构造的。</p>
<h2 id="文档生成的建模过程">文档生成的建模过程</h2>
<p>首先我们有一批文档，文档里面有很多单词，这些单词都是无顺序可交换的（词袋模型），这些文档分成两类，类标签为0或者1。给予一篇未标记的文档<span class="math inline">\(W_j\)</span>，我们要做的工作就是预测文档的类标签是<span class="math inline">\(L_j=0\)</span>还是<span class="math inline">\(L_j=1\)</span>。为了方便起见，我们定了类标签所表示的类<span class="math inline">\(\mathbb{C}_0={W_j|L_j=0}\)</span>和<span class="math inline">\(\mathbb{C}_1={W_j|L_j=1}\)</span>。一般来说预测这种事都是选择最有可能发生的，即找到<span class="math inline">\(W_j\)</span>的后验概率<span class="math inline">\(P(L_j|W_j)\)</span>最大的标签<span class="math inline">\(L_j\)</span>。使用贝叶斯公式 <span class="math display">\[\begin{equation}
\begin{split}
L_j=\arg \max \limits_{L}P(L|W_j)&amp; =\arg \max \limits_{L}\frac{P(W_j|L)P(L)}{P(W_j)}\\&amp; =\arg \max \limits_{L} P(W_j|L)P(L) \\\end{split}
\end{equation}\]</span> 因为分母<span class="math inline">\(P(W_j)\)</span>与<span class="math inline">\(L\)</span>无关所以删去了。 通过贝叶斯公式的转换，我们可以想象这些文档的生成过程。首先，我们选择文档的类标签<span class="math inline">\(L_j\)</span>;假设这个过程是通过投硬币完成的（正面概率为<span class="math inline">\(\pi=P(L_j=1)\)</span> )，正式地来说，就是服从贝努利分布 <span class="math display">\[\begin{equation}L_j \sim Bernoulli(\pi)\end{equation}\]</span> 然后，对于文档上<span class="math inline">\(R_j\)</span>个“词位”中的每一个，我们根据一个概率分布<span class="math inline">\(\theta\)</span>，随机独立地抽样一个词<span class="math inline">\(w_i\)</span>。因为每个类生成词的<span class="math inline">\(\theta\)</span>分布都不同，所以应该有<span class="math inline">\(\theta_1\)</span>和<span class="math inline">\(\theta_2\)</span>，具体地生成词的时候，我们根据文档的标签<span class="math inline">\(L_j\)</span>来决定由哪个类来生成 <span class="math display">\[\begin{equation}
W_j \sim Multinomial(R_j,\theta_{L_j})
\end{equation}\]</span>
        
          <p class="article-more-link">
            <a href="/2013/06/29/Gibbs Sampling for the UniniTiated-2/#more">Read More</a>
          </p>
        
      
    </div>
    <footer class="article-footer">
      
        <a data-url="http://crescentluna.github.io/2013/06/29/Gibbs Sampling for the UniniTiated-2/" data-id="cizha1r08001xg0o8m73d06b2" class="article-share-link" data-share="baidu">分享到</a>
      

      
        <a href="http://crescentluna.github.io/2013/06/29/Gibbs Sampling for the UniniTiated-2/#ds-thread" class="article-comment-link">Comments</a>
      

      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Gibbs-Sampling/">Gibbs Sampling</a></li></ul>

    </footer>
  </div>
  
</article>




  
    <article id="post-Gibbs Sampling for the UniniTiated-1" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2013/06/29/Gibbs Sampling for the UniniTiated-1/" class="article-date">
  <time datetime="2013-06-29T00:59:53.000Z" itemprop="datePublished">2013-06-29</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/学术/">学术</a>►<a class="article-category-link" href="/categories/学术/Gibbs-Sampling笔记/">Gibbs Sampling笔记</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2013/06/29/Gibbs Sampling for the UniniTiated-1/">《Gibbs Sampling for the UniniTiated》阅读笔记(上)---参数估计方法及Gibbs Sampling简介</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>前一阵子折腾的事儿太多，写了点东西都没有传上来，是我偷懒了- -，下不为例。</p>
<p>这篇文章基本上是来自于《Gibbs Sampling for the UniniTiated》，说是笔记其实和翻译也差不多了。</p>
<p>整个结构分为上中下三部分：</p>
<ol style="list-style-type: decimal">
<li> <a href="http://www.crescentmoon.info/?p=504">参数估计方法及Gibbs Sampling简介</a></li>
<li><a href="http://www.crescentmoon.info/?p=525">一个朴素贝叶斯文档模型例子</a></li>
<li><a href="http://www.crescentmoon.info/?p=548">连续型参数求积分的思考</a></li>
</ol>
<hr>
</hr>
<p>这篇是上部分，介绍基础参数估计和Gibbs Sampling概念。</p>
<h2 id="为什么求积分参数估计方法">为什么求积分—参数估计方法</h2>
<p>很多概率模型的算法并不需要使用积分，只要对概率求和就行了（比如隐马尔科夫链的Baum-Welch算法），那么什么时候用到求积分呢？—— 当为了获得概率密度估计的时候，比如说根据一句话前面部分的文本估计下一个词的概率，根据email的内容估计它是否是垃圾邮件的概率等等。为了估计概率密度，一般有MLE（最大似然估计），MAP（最大后验估计），bayesian estimation（贝叶斯估计）三种方法。</p>
<h3 id="最大似然估计">最大似然估计</h3>
<p>这里举一个例子来讲最大似然估计。假设我们有一个硬币，它扔出正面的概率<span class="math inline">\(\pi\)</span>不确定，我们扔了10次，结果为HHHHTTTTTT（H为正面，T为反面）。利用最大似然估计的话，很容易得到下一次为正面的概率为0.4,因为它估计的是使观察数据产生的概率最大的参数。 <a href="http://7sbo5n.com1.z0.glb.clouddn.com/first.png"><img src="http://7sbo5n.com1.z0.glb.clouddn.com/first.png" alt="first" /></a></p>
<p>令<span class="math inline">\(\chi=\{HHHHTTTTTT\}\)</span>代表观察到的数据,<span class="math inline">\(y\)</span>为下一次抛硬币可能的结果,估计公式如下: <span class="math display">\[\begin{equation}\begin{split}\tilde{\pi}_{MLE} &amp;=\arg \max \limits_{\pi}P(\chi|\pi) \\P(y|\chi) &amp; \approx \int_{\pi} p(y|\tilde{\pi}_{MLE})P(\pi|\chi) d\pi = p(y|\tilde{\pi}_{MLE})\end{split}\end{equation}\]</span></p>
        
          <p class="article-more-link">
            <a href="/2013/06/29/Gibbs Sampling for the UniniTiated-1/#more">Read More</a>
          </p>
        
      
    </div>
    <footer class="article-footer">
      
        <a data-url="http://crescentluna.github.io/2013/06/29/Gibbs Sampling for the UniniTiated-1/" data-id="cizha1qzz001cg0o8ugcko05a" class="article-share-link" data-share="baidu">分享到</a>
      

      
        <a href="http://crescentluna.github.io/2013/06/29/Gibbs Sampling for the UniniTiated-1/#ds-thread" class="article-comment-link">Comments</a>
      

      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Gibbs-Sampling/">Gibbs Sampling</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/参数估计/">参数估计</a></li></ul>

    </footer>
  </div>
  
</article>




  
    <article id="post-高斯混合模型的matlab实现（转）" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2013/04/11/高斯混合模型的matlab实现（转）/" class="article-date">
  <time datetime="2013-04-10T23:20:16.000Z" itemprop="datePublished">2013-04-11</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/学术/">学术</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2013/04/11/高斯混合模型的matlab实现（转）/">高斯混合模型的matlab实现（转）</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>高斯混合函数实现部分是基本上是转载的的pluskid大神<a href="http://blog.pluskid.org/?p=39">文章里</a>的里的代码，加了一点注释，并根据他给的<a href="http://freemind.pluskid.org/machine-learning/regularized-gaussian-covariance-estimation/#7de08bf962fca45b9699432818b939067d7c7327">方法二</a>解决 covariance 矩阵 singular 的问题。</p>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">function</span> <span class="title">varargout</span> = <span class="title">gmm</span><span class="params">(X, K_or_centroids)</span></span></div><div class="line"><span class="comment">% ============================================================</span></div><div class="line"><span class="comment">%转载自http://blog.pluskid.org/?p=39</span></div><div class="line"><span class="comment">% Expectation-Maximization iteration implementation of</span></div><div class="line"><span class="comment">% Gaussian Mixture Model.</span></div><div class="line"><span class="comment">%</span></div><div class="line"><span class="comment">% PX = GMM(X, K_OR_CENTROIDS)</span></div><div class="line"><span class="comment">% [PX MODEL] = GMM(X, K_OR_CENTROIDS)</span></div><div class="line"><span class="comment">%</span></div><div class="line"><span class="comment">%  - X: N-by-D data matrix.%需要注意的是这里的X包括了全部</span></div><div class="line"><span class="comment">%  - K_OR_CENTROIDS: either K indicating the number of</span></div><div class="line"><span class="comment">%       components or a K-by-D matrix indicating the</span></div><div class="line"><span class="comment">%       choosing of the initial K centroids.</span></div><div class="line"><span class="comment">%</span></div><div class="line"><span class="comment">%  - PX: N-by-K matrix indicating the probability of each</span></div><div class="line"><span class="comment">%       component generating each point.</span></div><div class="line"><span class="comment">%  - MODEL: a structure containing the parameters for a GMM:</span></div><div class="line"><span class="comment">%       MODEL.Miu: a K-by-D matrix.</span></div><div class="line"><span class="comment">%       MODEL.Sigma: a D-by-D-by-K matrix.</span></div><div class="line"><span class="comment">%       MODEL.Pi: a 1-by-K vector.</span></div><div class="line"><span class="comment">% ============================================================</span></div><div class="line">    threshold = <span class="number">1e-15</span>;</div><div class="line">    [N, D] = <span class="built_in">size</span>(X);</div><div class="line">    </div><div class="line">    <span class="keyword">if</span> <span class="built_in">isscalar</span>(K_or_centroids)</div><div class="line">        K = K_or_centroids;</div><div class="line">        <span class="comment">% randomly pick centroids</span></div><div class="line">        rndp = randperm(N);</div><div class="line">        centroids = X(rndp(<span class="number">1</span>:K),:);</div><div class="line">    <span class="keyword">else</span></div><div class="line">        K = <span class="built_in">size</span>(K_or_centroids, <span class="number">1</span>);</div><div class="line">        centroids = K_or_centroids;</div><div class="line">    <span class="keyword">end</span></div><div class="line">    </div><div class="line">    <span class="comment">% initial values</span></div><div class="line">    [pMiu pPi pSigma] = init_params();</div><div class="line">        </div><div class="line">    Lprev = -<span class="built_in">inf</span>;</div><div class="line">    <span class="keyword">while</span> true</div><div class="line">        Px = calc_prob();<span class="comment">%计算N(x|mu,sigma)</span></div><div class="line">        </div><div class="line">        <span class="comment">% new value for pGamma</span></div><div class="line">        pGamma = Px .* <span class="built_in">repmat</span>(pPi, N, <span class="number">1</span>);<span class="comment">%估计 gamma 是个N*K的矩阵</span></div><div class="line">        pGamma = pGamma ./ <span class="built_in">repmat</span>(sum(pGamma, <span class="number">2</span>), <span class="number">1</span>, K);<span class="comment">%对矩阵的理解真是出神入化,</span></div><div class="line">   </div><div class="line">        <span class="comment">% new value for parameters of each Component</span></div><div class="line">        Nk = sum(pGamma, <span class="number">1</span>);<span class="comment">%N_K</span></div><div class="line">        pMiu = <span class="built_in">diag</span>(<span class="number">1.</span>/Nk) * pGamma' * X;          <span class="comment">%数字 *( K-by-N * N-by-D)加个括号有助理解</span></div><div class="line">        pPi = Nk/N;</div><div class="line">        <span class="keyword">for</span> kk = <span class="number">1</span>:K</div><div class="line">            Xshift = X-<span class="built_in">repmat</span>(pMiu(kk, : ), N, <span class="number">1</span>);<span class="comment">%x-u</span></div><div class="line">            pSigma(:, :, kk) = (Xshift' * ...</div><div class="line">                (<span class="built_in">diag</span>(pGamma(:, kk)) * Xshift)) / Nk(kk);<span class="comment">%更新sigma</span></div><div class="line">   </div><div class="line">             <span class="keyword">end</span></div><div class="line">        <span class="comment">% check for convergence</span></div><div class="line">        L = sum(<span class="built_in">log</span>(Px*pPi'));</div><div class="line">        <span class="keyword">if</span> L-Lprev &lt; threshold</div><div class="line">            <span class="keyword">break</span>;</div><div class="line">        <span class="keyword">end</span></div><div class="line">        Lprev = L;</div><div class="line">    <span class="keyword">end</span></div><div class="line">    </div><div class="line">    <span class="keyword">if</span> nargout == <span class="number">1</span></div><div class="line">        varargout = &#123;Px&#125;;</div><div class="line">    <span class="keyword">else</span></div><div class="line">        model = [];</div><div class="line">        model.Miu = pMiu;</div><div class="line">        model.Sigma = pSigma;</div><div class="line">        model.Pi = pPi;</div><div class="line">        varargout = &#123;pGamma, model&#125;;<span class="comment">%注意！！！！！这里和大神代码不同，他返回的是px，而我是 pGamma</span></div><div class="line">    <span class="keyword">end</span></div><div class="line">        </div><div class="line">    <span class="function"><span class="keyword">function</span> <span class="params">[pMiu pPi pSigma]</span> = <span class="title">init_params</span><span class="params">()</span>%初始化参数</span></div><div class="line">        pMiu = centroids;<span class="comment">% K-by-D matrix</span></div><div class="line">        pPi = <span class="built_in">zeros</span>(<span class="number">1</span>, K);<span class="comment">%1-by-K matrix</span></div><div class="line">        pSigma = <span class="built_in">zeros</span>(D, D, K);<span class="comment">%</span></div><div class="line">        </div><div class="line">        <span class="comment">% hard assign x to each centroids</span></div><div class="line">        distmat = <span class="built_in">repmat</span>(sum(X.*X, <span class="number">2</span>), <span class="number">1</span>, K) + ... <span class="comment">% X is a N-by-D data matrix.</span></div><div class="line">            <span class="built_in">repmat</span>(sum(pMiu.*pMiu, <span class="number">2</span>)', N, <span class="number">1</span>) - ...<span class="comment">% X-&gt;K列 U-&gt;N行 XU^T is N-by-K</span></div><div class="line">            <span class="number">2</span>*X*pMiu';<span class="comment">%计算每个点到K个中心的距离</span></div><div class="line">        [~, labels] = min(distmat, [], <span class="number">2</span>);<span class="comment">%找到离X最近的pMiu，[C,I] labels代表这个最小值是从那列选出来的</span></div><div class="line">    </div><div class="line">        <span class="keyword">for</span> k=<span class="number">1</span>:K</div><div class="line">            Xk = X(labels == k, : );<span class="comment">% Xk是所有被归到K类的X向量构成的矩阵</span></div><div class="line">            pPi(k) = <span class="built_in">size</span>(Xk, <span class="number">1</span>)/N;<span class="comment">% 数一数几个归到K类的</span></div><div class="line">            pSigma(:, :, k) = cov(Xk); <span class="comment">%计算协方差矩阵，D-by-D matrix,最小方差无偏估计</span></div><div class="line">        <span class="keyword">end</span></div><div class="line">    <span class="keyword">end</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">function</span> <span class="title">Px</span> = <span class="title">calc_prob</span><span class="params">()</span></span></div><div class="line">        Px = <span class="built_in">zeros</span>(N, K);</div><div class="line">        <span class="keyword">for</span> k = <span class="number">1</span>:K</div><div class="line">            Xshift = X-<span class="built_in">repmat</span>(pMiu(k, : ), N, <span class="number">1</span>);<span class="comment">%x-u</span></div><div class="line">            lemda=<span class="number">1e-5</span>;</div><div class="line">            conv=pSigma(:, :, k)+lemda*<span class="built_in">diag</span>(<span class="built_in">diag</span>(<span class="built_in">ones</span>(D)));<span class="comment">%这里处理singular问题，为协方差矩阵加上一个很小lemda*I</span></div><div class="line">            inv_pSigma = inv(conv);<span class="comment">%协方差的逆</span></div><div class="line">            tmp = sum((Xshift*inv_pSigma) .* Xshift, <span class="number">2</span>);<span class="comment">%(X-U_k)sigma.*(X-U_k),tmp是个N*1的向量</span></div><div class="line">            coef = (<span class="number">2</span>*<span class="built_in">pi</span>)^(-D/<span class="number">2</span>) * <span class="built_in">sqrt</span>(det(inv_pSigma));<span class="comment">%前面的参数</span></div><div class="line">            Px(:, k) = coef * <span class="built_in">exp</span>(<span class="number">-0.5</span>*tmp);<span class="comment">%把数据点 x 带入到 Gaussian model 里得到的值</span></div><div class="line">        <span class="keyword">end</span></div><div class="line">    <span class="keyword">end</span></div><div class="line"><span class="keyword">end</span></div><div class="line"><span class="comment">%repmat 通过拓展向量到矩阵</span></div><div class="line"><span class="comment">%inv 求逆</span></div><div class="line"><span class="comment">%min 求矩阵最小值，可以返回标签</span></div><div class="line"><span class="comment">%X(labels == k, : ) 对行做筛选</span></div><div class="line"><span class="comment">% size(Xk, 1) 求矩阵的长或宽</span></div><div class="line"><span class="comment">%scatter 对二维向量绘图</span></div></pre></td></tr></table></figure>
<p><span style="color: #ff0000;">注意：</span></p>
<p>pluskid大神这里最后返回的是px，我觉得非常奇怪，因为PRML里对点做hard assignment时是根据后验概率来判别的。于是我在大神博客上问了一下，他的解释是最大似然和最大后验的区别，前者是挑x被各个模型产生的概率最大的那个，而后者加上了先验知识，各有道理。一句话就茅塞顿开，真大神也~
        
          <p class="article-more-link">
            <a href="/2013/04/11/高斯混合模型的matlab实现（转）/#more">Read More</a>
          </p>
        
      
    </div>
    <footer class="article-footer">
      
        <a data-url="http://crescentluna.github.io/2013/04/11/高斯混合模型的matlab实现（转）/" data-id="cizha1qzw0016g0o8vyxnv0zy" class="article-share-link" data-share="baidu">分享到</a>
      

      
        <a href="http://crescentluna.github.io/2013/04/11/高斯混合模型的matlab实现（转）/#ds-thread" class="article-comment-link">Comments</a>
      

      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/matlab/">matlab</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/高斯混合模型/">高斯混合模型</a></li></ul>

    </footer>
  </div>
  
</article>




  
    <article id="post-高斯混合模型参数估计详细推导过程" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2013/04/02/高斯混合模型参数估计详细推导过程/" class="article-date">
  <time datetime="2013-04-02T03:44:10.000Z" itemprop="datePublished">2013-04-02</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/学术/">学术</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2013/04/02/高斯混合模型参数估计详细推导过程/">高斯混合模型参数估计详细推导过程</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>已知多元高斯分布的公式: <span class="math display">\[N(x|\mu,\Sigma)=\frac{1}{(2\pi)^{D/2}|\Sigma|^{1/2}}\exp(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu))\]</span> 其中<span class="math inline">\(D\)</span>为维度，<span class="math inline">\(x\)</span>和<span class="math inline">\(\mu\)</span>均为<span class="math inline">\(D\)</span>维向量，协方差<span class="math inline">\(\Sigma\)</span>为D维矩阵。我们求得后验概率： <span class="math display">\[w^{(i)}_j=Q_i(Z^{i}=j)=P(z^{(i)}=j|x^{(i)};\phi,\mu,\Sigma)\]</span> 在E步，<span class="math inline">\(w^{(i)}_j\)</span>是一个固定值，然后我们用它来估计似然函数<span class="math inline">\(L(X,Z;\theta)\)</span>(这里<span class="math inline">\(\theta=(\phi,\mu,\Sigma)\)</span>)在分布<span class="math inline">\(Z\sim P(Z|X;\theta)\)</span>上的期望<span class="math inline">\(E_{Z|X,\theta_t}[L(X,Z;\theta)]\)</span>（式子1）: <span class="math display">\[\begin{split} &amp; \sum^m_{i=1}\sum_{z^{(i)}} Q_i(z^{(i)})\log{\frac{p(x^{(i)},z^{(i)};\phi,\mu,\Sigma)}{Q_i(z^{(i)})}} \\&amp; =\sum^m_{i=1}\sum^k_{j=1} Q_i(z^{(i)}=j)\log{\frac{p(x^{(i)}|z^{(i)}=j;\mu,\Sigma)p(z^{(i)}=j;\phi)}{Q_i(z^{(i)})}} \\&amp; =\sum^m_{i=1}\sum^k_{j=1} w^{(i)}_j\log{\frac{\frac{1}{(2\pi)^{D/2}|\Sigma|^{1/2}}\exp(-\frac{1}{2}(x^{(i)}-\mu_j)^T\Sigma_j^{-1}(x^{(i)}-\mu_j))\cdot\phi_j}{ w^{(i)}_j}} \\\end{split}\]</span> 由于分母<span class="math inline">\(w^{(i)}_j\)</span>在取对数之后是常数，与参数无关，求导时自然会变成0，所以我们写公式的时候为了简便舍去分母。
        
          <p class="article-more-link">
            <a href="/2013/04/02/高斯混合模型参数估计详细推导过程/#more">Read More</a>
          </p>
        
      
    </div>
    <footer class="article-footer">
      
        <a data-url="http://crescentluna.github.io/2013/04/02/高斯混合模型参数估计详细推导过程/" data-id="cizha1qzv0014g0o81dj119w9" class="article-share-link" data-share="baidu">分享到</a>
      

      
        <a href="http://crescentluna.github.io/2013/04/02/高斯混合模型参数估计详细推导过程/#ds-thread" class="article-comment-link">Comments</a>
      

      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/EM/">EM</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/高斯混合模型/">高斯混合模型</a></li></ul>

    </footer>
  </div>
  
</article>




  
  
    <nav id="page-nav">
      <span class="page-number current">1</span><a class="page-number" href="/categories/学术/page/2/">2</a><a class="extend next" rel="next" href="/categories/学术/page/2/">Next &raquo;</a>
    </nav>
  
</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">分类</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/学术/">学术</a><span class="category-list-count">18</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/学术/Gibbs-Sampling笔记/">Gibbs Sampling笔记</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/学术/变分推断笔记/">变分推断笔记</a><span class="category-list-count">3</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/机器学习/">机器学习</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/杂项/">杂项</a><span class="category-list-count">6</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/杂项/闲话/">闲话</a><span class="category-list-count">2</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/算法/">算法</a><span class="category-list-count">3</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签云</h3>
    <div class="widget tagcloud">
      <a href="/tags/DP/" style="font-size: 13.33px;">DP</a> <a href="/tags/EM/" style="font-size: 20px;">EM</a> <a href="/tags/Gibbs-Sampling/" style="font-size: 20px;">Gibbs Sampling</a> <a href="/tags/Hexo/" style="font-size: 13.33px;">Hexo</a> <a href="/tags/LDA/" style="font-size: 10px;">LDA</a> <a href="/tags/PRML/" style="font-size: 13.33px;">PRML</a> <a href="/tags/Variational-Inference/" style="font-size: 16.67px;">Variational Inference</a> <a href="/tags/kNN/" style="font-size: 10px;">kNN</a> <a href="/tags/matlab/" style="font-size: 10px;">matlab</a> <a href="/tags/tensorflow/" style="font-size: 10px;">tensorflow</a> <a href="/tags/wordpress/" style="font-size: 10px;">wordpress</a> <a href="/tags/二叉树/" style="font-size: 10px;">二叉树</a> <a href="/tags/决策树/" style="font-size: 10px;">决策树</a> <a href="/tags/动态规划/" style="font-size: 10px;">动态规划</a> <a href="/tags/参数估计/" style="font-size: 10px;">参数估计</a> <a href="/tags/多项式分布/" style="font-size: 10px;">多项式分布</a> <a href="/tags/数据挖掘/" style="font-size: 16.67px;">数据挖掘</a> <a href="/tags/机器学习/" style="font-size: 16.67px;">机器学习</a> <a href="/tags/概率图/" style="font-size: 10px;">概率图</a> <a href="/tags/盘子/" style="font-size: 10px;">盘子</a> <a href="/tags/社区发现/" style="font-size: 10px;">社区发现</a> <a href="/tags/背包问题/" style="font-size: 13.33px;">背包问题</a> <a href="/tags/贝叶斯方法/" style="font-size: 10px;">贝叶斯方法</a> <a href="/tags/高斯混合模型/" style="font-size: 16.67px;">高斯混合模型</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">归档</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/02/">二月 2017</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2014/12/">十二月 2014</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2014/02/">二月 2014</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2013/10/">十月 2013</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2013/08/">八月 2013</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2013/07/">七月 2013</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2013/06/">六月 2013</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2013/04/">四月 2013</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2013/03/">三月 2013</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2013/02/">二月 2013</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2013/01/">一月 2013</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2012/12/">十二月 2012</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2012/10/">十月 2012</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2012/09/">九月 2012</a><span class="archive-list-count">2</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">浏览数目</h3>
    <div class="widget">
      <ul class="popularlist">
      </ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">近期文章</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2017/02/23/install-tensorflow-with-gpu-support-for-ubuntu/">在Ubuntu上搭建GPU加速的TensorFlow环境</a>
          </li>
        
          <li>
            <a href="/2014/12/12/variational-inference-3/">变分推断学习笔记(3)——三硬币问题的变分推断解法</a>
          </li>
        
          <li>
            <a href="/2014/12/11/popular-widget/">使用LeanCloud平台为Hexo博客添加文章浏览量统计组件</a>
          </li>
        
          <li>
            <a href="/2014/12/11/relocation/">博客迁移小记</a>
          </li>
        
          <li>
            <a href="/2014/02/24/概率图模型简介/">概率图模型简介</a>
          </li>
        
      </ul>
    </div>
  </div>


  
    
<div class="widget-wrap">
  <h3 class="widget-title">最近评论</h3>
  <ul class="widget ds-recent-comments" data-num-items="5" data-show-avatars="0" data-show-title="1" data-show-time="1"></ul>
</div>
<!-- 需要多说的公用代码 -->


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">友情链接</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="https://github.com/xiangming/landscape-plus" target="_blank">landscape-plus主题</a>
          </li>
        
          <li>
            <a href="http://www.smallqiao.com/" target="_blank">小桥流水</a>
          </li>
        
          <li>
            <a href="http://www.ahathinking.com/" target="_blank">勇幸|Thinking</a>
          </li>
        
          <li>
            <a href="http://www.socona.me/" target="_blank">socona的博客</a>
          </li>
        
          <li>
            <a href="http://www.clarkchen.com/" target="_blank">陈曦师兄的博客</a>
          </li>
        
          <li>
            <a href="http://www.fengyafei.com/" target="_blank">小飞的博客</a>
          </li>
        
          <li>
            <a href="http://ariwaranosai.xyz/" target="_blank">今天拒绝负能量!</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2017 Chen Hao<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/xiangming/landscape-plus" target="_blank">Landscape-plus</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">主页</a>
  
    <a href="/archives" class="mobile-nav-link">所有文章</a>
  
    <a href="/about" class="mobile-nav-link">关于</a>
  
</nav>
    
<!-- 多说公共js代码 start -->
<script type="text/javascript">
var duoshuoQuery = {short_name:"crescent"};
  (function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0] 
     || document.getElementsByTagName('body')[0]).appendChild(ds);
  })();
  </script>
<!-- 多说公共js代码 end -->


<!-- 百度分享 start -->

<div id="article-share-box" class="article-share-box">
  <div id="bdshare" class="bdsharebuttonbox article-share-links">
    <a class="article-share-weibo" data-cmd="tsina" title="分享到新浪微博"></a>
    <a class="article-share-weixin" data-cmd="weixin" title="分享到微信"></a>
    <a class="article-share-qq" data-cmd="sqq" title="分享到QQ"></a>
    <a class="article-share-renren" data-cmd="renren" title="分享到人人网"></a>
    <a class="article-share-more" data-cmd="more" title="更多"></a>
  </div>
</div>
<script>window._bd_share_config={"common":{},"share":{"bdCustomStyle":"nocss.css"}};with(document)0[(getElementsByTagName("head")[0]||body).appendChild(createElement("script")).src="http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion="+~(-new Date()/36e5)];</script>

<!-- 百度分享 end -->

<script src="//libs.baidu.com/jquery/1.11.1/jquery.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<! -- mathjax config similar to math.stackexchange -->
<! -- add autoNumber settting -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
                processEscapes: true
                    
}
  
        });
</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
                  
}
    
        });
</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Queue(function() {
            var all = MathJax.Hub.getAllJax(), i;
            for(i=0; i < all.length; i += 1) {
                            all[i].SourceElement().parentNode.className += ' has-jax';
                                    
            }
                
        });
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<script src="/js/script.js"></script>

<!--page counter part-->
<script>
function addCount (Counter) {
        url=$('.article-date').attr('href').trim();
        title = $('.article-title').text().trim();
       // alert(title);
        var query=new AV.Query(Counter);
        //use url as unique idnetfication
        query.equalTo("url",url);
        query.find({
            success: function(results){
                if(results.length>0)
                {
                    var counter=results[0];
                    counter.fetchWhenSave(true); //get recent result
                    counter.increment("time");
                    counter.save();
                    //alert('find '+title+' and visit time is now '+ counter.get("time"));
                }
                else
                {
                    var newcounter=new Counter();
                    newcounter.set("title",title);
                    newcounter.set("url",url);
                    newcounter.set("time",1);
                    newcounter.save(null,{
                        success: function(newcounter){
                        //alert('New object created');
                        },
                        error: function(newcounter,error){
                        alert('Failed to create');
                        }
                        });
                }
            },
            error: function(error){
                //find null is not a error
                alert('Error:'+error.code+" "+error.message);
            }
        });
}

$(function(){
        var Counter=AV.Object.extend("Counter");
        //only increse visit counting when intering a page
        if ($('.article-title').length == 1)
           addCount(Counter);
        var query=new AV.Query(Counter);
        query.descending("time");
        query.limit(10);
        query.find({
            success: function(results){
                    for(var i=0;i<results.length;i++)    
                    {
                        var counter=results[i];
                        //alert(counter.get("title")+'-'+counter.get("url"));
                        title=counter.get("title");
                        url=counter.get("url");
                        time=counter.get("time");
                        // add to the popularlist widget
                        showcontent=title+" ("+time+")";
                        //notice the "" in href
                        $('.popularlist').append('<li><a href="'+url+'">'+showcontent+'</a></li>');
                    }
                },
            error: function(error){
                alert("Error:"+error.code+" "+error.message);
            }
            }
        )
        });
</script>


  </div><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="custom_mathjax_source">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
</body>
</html>