<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>心怀畏惧</title>
  <subtitle>Do not go gentle into that good night</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://crescentluna.github.io/"/>
  <updated>2017-02-22T18:03:32.572Z</updated>
  <id>http://crescentluna.github.io/</id>
  
  <author>
    <name>Chen Hao</name>
    <email>chenhaoac@gmail.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>在Ubuntu上搭建GPU加速的TensorFlow环境</title>
    <link href="http://crescentluna.github.io/2017/02/23/install-tensorflow-with-gpu-support-for-ubuntu/"/>
    <id>http://crescentluna.github.io/2017/02/23/install-tensorflow-with-gpu-support-for-ubuntu/</id>
    <published>2017-02-22T16:24:13.000Z</published>
    <updated>2017-02-22T18:03:32.572Z</updated>
    
    <content type="html"><![CDATA[<h2 id="硬件软件环境">硬件软件环境</h2>
<ul>
<li><p>Ubuntu 16.10</p></li>
<li><p>GTX 750ti（需要一张NVIDIA的显卡，越新越好，新卡的Compute Capability版本高）</p></li>
<li><p>NVIDA CUDA 8.0</p></li>
<li><p>NVIDIA 驱动 375.26</p></li>
<li><p>gcc version 4.9</p></li>
</ul>
<p>​</p>
<h2 id="基础环境配置">1. 基础环境配置</h2>
<p>因为Ubuntu是机子新装的，所以我安装了Linux自己用的一些基本环境和python科学计算的库，请各取所需。</p>
<h3 id="基本开发">基本开发</h3>
<ul>
<li>安装vim <code>sudo apt-get install vim</code></li>
<li>安装zsh</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">sudo apt-get install zsh</div><div class="line">chsh -s /usr/bin/zsh</div></pre></td></tr></table></figure>
<ul>
<li>安装git <code>sudo apt-get install git</code></li>
<li>安装 <a href="http://ohmyz.sh/" target="_blank" rel="external">oh-my-zsh</a> <code>sh -c &quot;$(curl -fsSL https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/install.sh)&quot;</code></li>
<li>安装 autojump <code>sudo apt-get install autojump</code></li>
</ul>
<h3 id="python科学计算库安装">Python科学计算库安装</h3>
<ul>
<li>安装<a href="https://www.continuum.io/downloads" target="_blank" rel="external">Anaconda</a></li>
</ul>
<p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"># 下载Anaconda</div><div class="line">bash Anaconda2-4.3.0-Linux-x86_64.sh </div><div class="line"># 切换成清华镜像，用于conda加速</div><div class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/</div><div class="line">conda config --set show_channel_urls yes</div></pre></td></tr></table></figure></p>
<ul>
<li><p>安装open-jdk <code>sudo apt-get install openjdk-8-jdk</code></p></li>
<li><p>安装<a href="http://www.jetbrains.com/pycharm/" target="_blank" rel="external">pycharm</a></p></li>
</ul>
<h2 id="nvida环境安装">2. NVIDA环境安装</h2>
<p>首先贴一段<a href="https://www.tensorflow.org/install/install_linux" target="_blank" rel="external">Tensorflow</a>官网上GPU支持对NVIDIA的环境需求：If you are installing TensorFlow with GPU support using one of the mechanisms described in this guide, then the following NVIDIA software must be installed on your system:</p>
<ul>
<li><p>CUDA® Toolkit 8.0. For details, see <a href="http://docs.nvidia.com/cuda/cuda-installation-guide-linux/#axzz4VZnqTJ2A" target="_blank" rel="external">NVIDIA’s documentation</a>. Ensure that you append the relevant Cuda pathnames to the <code>LD_LIBRARY_PATH</code> environment variable as described in the NVIDIA documentation.</p></li>
<li><p>The NVIDIA drivers associated with CUDA Toolkit 8.0.</p></li>
<li><p>cuDNN v5.1. For details, see <a href="https://developer.nvidia.com/cudnn" target="_blank" rel="external">NVIDIA’s documentation</a>. Ensure that you create the <code>CUDA_HOME</code> environment variable as described in the NVIDIA documentation.</p></li>
<li><p>GPU card with CUDA Compute Capability 3.0 or higher. See <a href="https://developer.nvidia.com/cuda-gpus" target="_blank" rel="external">NVIDIA documentation</a> for a list of supported GPU cards.</p></li>
<li><p>The libcupti-dev library, which is the NVIDIA CUDA Profile Tools Interface. This library provides advanced profiling support. To install this library, issue the following command:</p></li>
</ul>
<p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ sudo apt-get install libcupti-dev</div></pre></td></tr></table></figure></p>
<p>除了最后的libcupti-dev库可以直接apt-get，我们需要装的大头就是CUDA® Toolkit和cuDNN两个东西，各种坑从这里开始了囧。</p>
<h3 id="cuda安装">CUDA安装</h3>
<p>按照 <a href="http://docs.nvidia.com/cuda/cuda-installation-guide-linux/#axzz4VZnqTJ2A" target="_blank" rel="external">NVIDIA’s documentation</a> 给出的步骤：</p>
<ul>
<li>在安装之前首先逐一验证系统是否符合条件（Pre-installation Actions)</li>
<li>下载<a href="https://developer.nvidia.com/cuda-toolkit" target="_blank" rel="external">CUDA Toolkit</a>，UBuntu推荐下载deb(local)版，安装过程比较方便</li>
<li>把deb包加入到包管理中，然后apt-get安装</li>
<li>安装后的验证过程</li>
</ul>
<p>在安装后的验证过程中需要注意的几个点如下：</p>
<h4 id="cuda环境变量配置">CUDA环境变量配置</h4>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">export PATH=/usr/local/cuda-8.0/bin$&#123;PATH:+:$&#123;PATH&#125;&#125;</div><div class="line">export LD_LIBRARY_PATH=/usr/local/cuda-8.0/lib64/$&#123;LD_LIBRARY_PATH:+:$&#123;LD_LIBRARY_PATH&#125;&#125;</div><div class="line"># 注意这里要路径要和Nvida驱动版本一致</div><div class="line">export LPATH=/usr/lib/nvidia-375:$LPATH</div><div class="line">export LIBRARY_PATH=/usr/lib/nvidia-375:$LIBRARY_PATH</div><div class="line"># Tensorflow 要求的环境变量</div><div class="line">export CUDA_HOME=/usr/local/cuda-8.0</div></pre></td></tr></table></figure>
<p>这里最坑爹的一点是LIBRARY_PATH这个环境变量配置，官方的文档上一点没提，如果不写的话，在编译cuda的samples时，会在3_Imaging这个samples下报这个错误</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">/usr/bin/ld: cannot find -lnvcuvid</div><div class="line">collect2: error: ld returned 1 exit status</div><div class="line">Makefile:346: recipe for target &apos;cudaDecodeGL&apos; failed</div></pre></td></tr></table></figure>
<h4 id="切换成低版本的gcc编译器">切换成低版本的gcc编译器</h4>
<p>因为Ubuntu 16.10自带的gcc编译器版本是6.2，对于CUDA来说太新了，所以会报错</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">error -- unsupported GNU version! gcc versions later than 5 are not supported!</div></pre></td></tr></table></figure>
<p>可以看到CUDA 8.0 能够支持的gcc最新版本不能超过5。网上给出的比较好的解决办法是利用Ubutnu的<a href="https://www.udacity.com/wiki/cs344/troubleshoot_gcc47" target="_blank" rel="external">update-alternatives</a> 命令来切换版本，具体命令如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">sudo apt-get install gcc-4.9 g++-4.9</div><div class="line">sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-4.9 40 --slave /usr/bin/g++ g++ /usr/bin/g++-4.9</div><div class="line">sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-6 30 --slave /usr/bin/g++ g++ /usr/bin/g++-6 </div><div class="line">sudo update-alternatives --config gcc</div></pre></td></tr></table></figure>
<p>敲完<code>sudo update-alternatives --config gcc</code>之后，你就可以看到不同版本的gcc优先级了。</p>
<h4 id="samples编译测试">Samples编译测试</h4>
<p>根据Recommended Actions](http://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#recommended-post)步骤编译Cuda的那些samples，如果出现<code>Finished building CUDA samples</code>，说明所有samples的编译通过了。可以敲<code>NVIDIA_CUDA-8.0_Samples ./bin/x86_64/linux/release/nbody</code>，可以看到以下效果</p>
<div class="figure">
<img src="http://crescent.qiniudn.com/static/images/tensorflow_gpu_2017-02-22_01.png" alt="tensorflow_gpu_2017-02-22_01">
<p class="caption">tensorflow_gpu_2017-02-22_01</p>
</div>
<h3 id="cudnn配置">cuDNN配置</h3>
<p>下载 <a href="https://developer.nvidia.com/cudnn" target="_blank" rel="external">cuDNN</a>之前需要注册一下，成为NVIDIA的开发者，然后把下载的包解压拷贝到CUDA的链接库和头文件目录就行了。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">tar -xzvf cudnn-8.0-linux-x64-v5.1.tgz </div><div class="line"># 解压得到cuda文件</div><div class="line">sudo cp cuda/lib64/* /usr/local/cuda/lib64 </div><div class="line">sudo cp cuda/include/cudnn.h /usr/local/cuda/include/</div></pre></td></tr></table></figure>
<h2 id="tensorflow安装">3. Tensorflow安装</h2>
<p>安装<a href="https://www.tensorflow.org/install/install_linux" target="_blank" rel="external">Tensorflow</a>有多种方式，这里我直接用的pip安装，python版本是2.7。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.0.0-cp27-none-linux_x86_64.whl</div><div class="line">sudo pip install --upgrade TF_BINARY_URL</div></pre></td></tr></table></figure>
<p>都搞定之后，启动ipython，输入</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf </div><div class="line">hello = tf.constant(<span class="string">'Hello, TensorFlow!'</span>)</div><div class="line">sess = tf.Session()</div><div class="line">print(sess.run(hello))</div></pre></td></tr></table></figure>
<p>能看到输出的结果，说明GPU加速安装成功了。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: </div><div class="line">name: GeForce GTX 750 Ti</div><div class="line">major: 5 minor: 0 memoryClockRate (GHz) 1.0845</div><div class="line">pciBusID 0000:01:00.0</div><div class="line">Total memory: 1.95GiB</div><div class="line">Free memory: 1.53GiB</div><div class="line">I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 </div><div class="line">I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y </div><div class="line">I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: GeForce GTX 750 Ti, pci bus id: 0000:01:00.0)</div><div class="line">Hello, TensorFlow!</div></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;硬件软件环境&quot;&gt;硬件软件环境&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Ubuntu 16.10&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;GTX 750ti（需要一张NVIDIA的显卡，越新越好，新卡的Compute Capability版本高）&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://crescentluna.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="tensorflow" scheme="http://crescentluna.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>变分推断学习笔记(3)——三硬币问题的变分推断解法</title>
    <link href="http://crescentluna.github.io/2014/12/12/variational-inference-3/"/>
    <id>http://crescentluna.github.io/2014/12/12/variational-inference-3/</id>
    <published>2014-12-12T14:54:14.000Z</published>
    <updated>2017-02-21T18:24:27.407Z</updated>
    
    <content type="html"><![CDATA[<p>变分推断学习笔记系列：</p>
<ol style="list-style-type: decimal">
<li><a href="http://crescentluna.github.io/2013/10/03/%E5%8F%98%E5%88%86%E6%8E%A8%E6%96%AD%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01%E2%80%94%E2%80%94%E6%A6%82%E5%BF%B5%E4%BB%8B%E7%BB%8D/">变分推断学习笔记(1)——概念介绍</a></li>
<li><a href="http://crescentluna.github.io/2013/10/11/variational-inference-2/">变分推断学习笔记(2)——一维高斯模型的例子</a></li>
<li><a href="http://crescentluna.github.io/2014/12/12/variational-inference-3/">变分推断学习笔记(3)——三硬币问题的变分推断解法</a>
<hr>
</li>
</ol>
<p><del>其实三硬币的例子不写，前面的介绍也够了，写这个纯粹是吃撑了</del>。这次我们采取更加普遍的假设，将原先假设的3枚硬币拓展开来。现在假设有<span class="math inline">\(K+1\)</span>个骰子，第一个骰子有<span class="math inline">\(K\)</span>个面，其余的骰子有<span class="math inline">\(T\)</span>个面。进行如下实验：先掷第一个骰子，根据投出的结果<span class="math inline">\(Z_k\)</span>，选择第<span class="math inline">\(Z_k\)</span>个骰子再投，观测到投出的<span class="math inline">\(N\)</span>个结果，每个结果<span class="math inline">\(w_n\)</span>可能是 <span class="math display">\[
1，3，7，8，3，2，6，9，...
\]</span></p>
<p>可以看到现在第1个骰子投出的标签服从多项分布： <span class="math display">\[Z_k \sim Multinomial(\pi)\]</span> 然后剩余骰子投出的面也服从多项分布 <span class="math display">\[W_{Z_{kt}} \sim Multinomial(\theta_{Z_k})\]</span> 我们假设，随机变量<span class="math inline">\(\pi\)</span>和<span class="math inline">\(\theta\)</span>的先验分布为狄利克雷分布，超参分别为<span class="math inline">\(\alpha\)</span>和<span class="math inline">\(\beta\)</span>。 <a id="more"></a></p>
<p><img src="http://7sbo5n.com1.z0.glb.clouddn.com/变分3.jpg" alt="图１：三硬币模型的变分族"> 让我们写出模型的联合概率： <span class="math display">\[
\begin{split}P(W,Z,\Pi,\Theta)&amp;=p(\pi|\alpha)\prod^K_{k=1}p(\theta_k|\beta)\prod^N_{n=1}\prod^K_{k=1}\prod^T_{t=1}p(z_{nkt}|\pi_{nkt})p(w_{n}|\theta_{z_{nkt}})\\
\end{split}
\]</span> 相应地，我们利用平均场理论切断模型直接耦合的地方（见图１），设定一个近似真实后验的分布族<span class="math inline">\(q\)</span>。 <span class="math display">\[
\begin{split}P(W,Z,\Pi,\Theta)&amp;=q(\pi|\nu)\prod^K_{k=1}q(\theta_k|\lambda_k)\prod^N_{n=1}\prod^K_{k=1}\prod^T_{t=1}q(z_{nkt}|\phi_{nkt})\\
\end{split}
\]</span></p>
<p>然后我们最小化<span class="math inline">\(q\)</span> 与真实后验之间的KL 散度，也就是最大化证据下界<span class="math inline">\(\mathcal{L}\)</span>(ELBO)。 证据下界写出来是这样的: <span class="math display">\[
\begin{split}
\mathcal{L}
&amp;= E_q[\log p(\pi|\alpha)]-E_q[\log q(\pi|\nu)] \\
&amp;+\sum_k E_q[\log p(\theta_k|\beta)]-\sum_k E_q[\log q(\theta_k|\lambda_k)] \\
&amp;+\sum_n\sum_t\sum_k E_q[\log p(z_{nkt}|\pi_{nkt})]-\sum_n\sum_t\sum_k E_q[\log q(z_{nkt}|\phi_{nkt})] \\
&amp;+ \sum_n\sum_t\sum_k E_q[\log p(w_{nt}|\theta_{z_{nkt}})]
\end{split}
\]</span></p>
<p>因为Dirichlet分布为 <span class="math display">\[
Dir(\vec{p}|\vec{\alpha})=\frac{\Gamma(\sum^K_{k=1}\alpha_k)}{\prod^K_{k=1}\Gamma{(\alpha_k)}}\prod^K_{k=1}p_k^{\alpha_k-1}
\]</span></p>
<p>由LDA原论文的Appendix A.1可知，Dirichlet的某个分布（single probability component ）的log期望为 <span class="math display">\[
\mathbb{E}[\log p_k|\alpha_k]=\psi(\alpha_k)-\psi(\sum_k \alpha_k)
\]</span> 其中<span class="math inline">\(\psi(\alpha)=\frac{d}{d\alpha}\log \Gamma(\alpha)\)</span>。 根据这个公式，计算<span class="math inline">\(\mathcal{L}\)</span>关于<span class="math inline">\(q\)</span>的期望,我们可以得到 <span class="math display">\[
\begin{split}
\mathcal{L}
&amp;=\log\Gamma(\sum_k \alpha_k)-\log\Gamma(\alpha_k)+\sum_{k}(\alpha_k-1)[\Psi(\nu_{k})-\Psi(\sum_v \nu_{k})]\\
&amp;-\log\Gamma(\sum_k \nu_k)+\log\Gamma(\nu_k)-\sum_{k}(\nu_k-1)[\Psi(\nu_{k})-\Psi(\sum_v \nu_{k})]\\
&amp;+\sum_k \log\Gamma(\sum_t \beta_{k,t})-\sum_{k,t}\log\Gamma(\beta_{k,t})+\sum_{k,t}(\beta_t-1)[\Psi(\lambda_{k,t})-\Psi(\sum_k \lambda_{k,t})]\\
&amp;-\sum_k \log\Gamma(\sum_t \lambda_{k,t})+\sum_{k,t}\log\Gamma(\lambda_{k,t})-\sum_{k,t}(\lambda_{k,t}-1)[\Psi(\lambda_{k,t})-\Psi(\sum_k \lambda_{k,t})]\\
&amp;+\sum_n\sum_k\sum_t \phi_{nkt}[\Psi(\alpha_{k})-\Psi(\sum_k \alpha_{k})]-\sum_n\sum_k\sum_t \phi_{nkt}\log \phi_{nkt} \\
&amp;+\sum_n\sum_k\sum_t \phi_{nkt}\delta_t(w_{n})[\Psi(\lambda_{k,t})-\Psi(\sum_t \lambda_{k,t})] \\
\end{split}
\]</span> 其中<span class="math inline">\(\delta_t(w_{n})\)</span>当且仅当<span class="math inline">\(w_n=t\)</span>时为1，其余的时候均为0。因为多项分布<span class="math inline">\(p(x)=\prod^K_{k=1}p_k^{x_k}\)</span>的期望<span class="math inline">\(E[x_k]=p_k\)</span>，所以这里有<span class="math inline">\(E[z_{nkt}]=\phi_{nkt}\)</span>，<span class="math inline">\(\phi_{nkt}\)</span>代表隐藏变量的期望值。</p>
<p>将<span class="math inline">\(\mathcal{L}\)</span>分别对各自的参数求导，解得 <span class="math display">\[
\begin{split}
 &amp;\nu_k=\alpha_k \\
 &amp;\phi_{dnk} \propto \exp\{\Psi(\lambda_{k,t})-\Psi(\sum_t \lambda_{k,t})\} \\
 &amp;\lambda_{k,t}=\beta_t+\sum_n\phi_ {nkt}\delta_t(w_n)
\end{split}
\]</span> 相互迭代到收敛就好啦。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;变分推断学习笔记系列：&lt;/p&gt;
&lt;ol style=&quot;list-style-type: decimal&quot;&gt;
&lt;li&gt;&lt;a href=&quot;http://crescentluna.github.io/2013/10/03/%E5%8F%98%E5%88%86%E6%8E%A8%E6%96%AD%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01%E2%80%94%E2%80%94%E6%A6%82%E5%BF%B5%E4%BB%8B%E7%BB%8D/&quot;&gt;变分推断学习笔记(1)——概念介绍&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://crescentluna.github.io/2013/10/11/variational-inference-2/&quot;&gt;变分推断学习笔记(2)——一维高斯模型的例子&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://crescentluna.github.io/2014/12/12/variational-inference-3/&quot;&gt;变分推断学习笔记(3)——三硬币问题的变分推断解法&lt;/a&gt;
&lt;hr&gt;
&lt;/hr&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;del&gt;其实三硬币的例子不写，前面的介绍也够了，写这个纯粹是吃撑了&lt;/del&gt;。这次我们采取更加普遍的假设，将原先假设的3枚硬币拓展开来。现在假设有&lt;span class=&quot;math inline&quot;&gt;\(K+1\)&lt;/span&gt;个骰子，第一个骰子有&lt;span class=&quot;math inline&quot;&gt;\(K\)&lt;/span&gt;个面，其余的骰子有&lt;span class=&quot;math inline&quot;&gt;\(T\)&lt;/span&gt;个面。进行如下实验：先掷第一个骰子，根据投出的结果&lt;span class=&quot;math inline&quot;&gt;\(Z_k\)&lt;/span&gt;，选择第&lt;span class=&quot;math inline&quot;&gt;\(Z_k\)&lt;/span&gt;个骰子再投，观测到投出的&lt;span class=&quot;math inline&quot;&gt;\(N\)&lt;/span&gt;个结果，每个结果&lt;span class=&quot;math inline&quot;&gt;\(w_n\)&lt;/span&gt;可能是 &lt;span class=&quot;math display&quot;&gt;\[
1，3，7，8，3，2，6，9，...
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;可以看到现在第1个骰子投出的标签服从多项分布： &lt;span class=&quot;math display&quot;&gt;\[Z_k \sim Multinomial(\pi)\]&lt;/span&gt; 然后剩余骰子投出的面也服从多项分布 &lt;span class=&quot;math display&quot;&gt;\[W_{Z_{kt}} \sim Multinomial(\theta_{Z_k})\]&lt;/span&gt; 我们假设，随机变量&lt;span class=&quot;math inline&quot;&gt;\(\pi\)&lt;/span&gt;和&lt;span class=&quot;math inline&quot;&gt;\(\theta\)&lt;/span&gt;的先验分布为狄利克雷分布，超参分别为&lt;span class=&quot;math inline&quot;&gt;\(\alpha\)&lt;/span&gt;和&lt;span class=&quot;math inline&quot;&gt;\(\beta\)&lt;/span&gt;。
    
    </summary>
    
      <category term="学术" scheme="http://crescentluna.github.io/categories/%E5%AD%A6%E6%9C%AF/"/>
    
      <category term="变分推断笔记" scheme="http://crescentluna.github.io/categories/%E5%AD%A6%E6%9C%AF/%E5%8F%98%E5%88%86%E6%8E%A8%E6%96%AD%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Variational Inference" scheme="http://crescentluna.github.io/tags/Variational-Inference/"/>
    
  </entry>
  
  <entry>
    <title>使用LeanCloud平台为Hexo博客添加文章浏览量统计组件</title>
    <link href="http://crescentluna.github.io/2014/12/11/popular-widget/"/>
    <id>http://crescentluna.github.io/2014/12/11/popular-widget/</id>
    <published>2014-12-11T11:25:03.000Z</published>
    <updated>2017-02-21T18:24:27.407Z</updated>
    
    <content type="html"><![CDATA[<p>在原来的wordpress博客中有一个WP-PostViews Plus插件，可以统计每篇文章的浏览量，可以为游客提供热门文章的信息，<del>(顺便满足一下作者的虚荣心)</del>。现在切换到静态博客Hexo了，就需要第三方服务来实现这样的动态数据处理。这里要感谢师弟<a href="http://ariwaranosai.xyz/" target="_blank" rel="external">ariwaranosai</a>给我推荐的<a href="https://cn.avoscloud.com/" target="_blank" rel="external">LeanCloud</a>平台，以及<a href="http://ibruce.info/2013/12/22/count-views-of-hexo/" target="_blank" rel="external">为hexo博客添加访问次数统计功能</a>（基于BAE）提供的思路。使用LeanCloud的优点是它自己实现了一个AV.view 类，不需要考虑JavaScript的跨站访问问题。</p>
<h2 id="创建lean-cloud应用">创建Lean Cloud应用</h2>
<p>首先一句话介绍Lean Cloud:</p>
<blockquote>
<p><a href="https://leancloud.cn/" target="_blank" rel="external">LeanCloud</a>（aka. AVOS Cloud）提供一站式后端云服务，从数据存储、实时聊天、消息推送到移动统计，涵盖应用开发的多方面后端需求。</p>
</blockquote>
<p>我们只用到它的数据存储部分,具体步骤如下：</p>
<ol style="list-style-type: decimal">
<li>首先到『控制台』创建一个应用，名字随便取。</li>
<li>点击新建应用的『数据』选项，选择『创建Class』，取名为”Counter“。</li>
<li>点击新建应用右上角的齿轮，在『应用Key』选项里得到APP ID 和 APP Key，在后面会用到。</li>
</ol>
<h2 id="修改hexo页面">修改Hexo页面</h2>
<h3 id="新建popular_posts.ejs">新建popular_posts.ejs</h3>
<p>首先在<code>theme/你的主题/layout/_widget</code>目录下新建<code>popular_posts.ejs</code>文件,其内容为</p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">%</span> <span class="attr">if</span> (<span class="attr">site.posts.length</span>)&#123; %&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"widget-wrap"</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">h3</span> <span class="attr">class</span>=<span class="string">"widget-title"</span>&gt;</span>浏览数目<span class="tag">&lt;/<span class="name">h3</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"widget"</span>&gt;</span></div><div class="line">      <span class="tag">&lt;<span class="name">ul</span> <span class="attr">class</span>=<span class="string">"popularlist"</span>&gt;</span></div><div class="line">      <span class="tag">&lt;/<span class="name">ul</span>&gt;</span></div><div class="line">    <span class="tag">&lt;/<span class="name">div</span>&gt;</span></div><div class="line">  <span class="tag">&lt;/<span class="name">div</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">%</span> &#125; %&gt;</span></div></pre></td></tr></table></figure>
<h3 id="修改head.ejs">修改head.ejs</h3>
<p>修改<code>theme/你的主题/layout/_partial/head.ejs</code>文件,在head标签的最后插入：</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&lt;script src=<span class="string">"https://cdn1.lncld.net/static/js/av-min-1.2.1.js"</span>&gt;<span class="xml"><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span></div><div class="line">  &lt;script&gt;AV.initialize(<span class="string">"你的APP ID"</span>, <span class="string">"你的APP Key"</span>);<span class="xml"><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span></div></pre></td></tr></table></figure>
<p>注意Lean Cloud引用的js文件位置可能会变化，如果代码里位置打不开的话，记得去官方的<a href="https://leancloud.cn/docs/sdk_setup-js.html#npm_安装" target="_blank" rel="external">JavaScript SDK文档</a>找一下最新的位置。</p>
<h3 id="修改after-footer.ejs">修改after-footer.ejs</h3>
<p>修改<code>theme/你的主题/layout/_partial/after-footer.ejs</code>文件,在最后插入： <a id="more"></a></p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div></pre></td><td class="code"><pre><div class="line">&lt;!--page counter part--&gt;</div><div class="line"><span class="xml"><span class="tag">&lt;<span class="name">script</span>&gt;</span><span class="javascript"></span></span></div><div class="line"><span class="function"><span class="keyword">function</span> <span class="title">addCount</span> (<span class="params">Counter</span>) </span>&#123;</div><div class="line">        url=$(<span class="string">'.article-date'</span>).attr(<span class="string">'href'</span>).trim();</div><div class="line">        title = $(<span class="string">'.article-title'</span>).text().trim();</div><div class="line">        <span class="keyword">var</span> query=<span class="keyword">new</span> AV.Query(Counter);</div><div class="line">        <span class="comment">//use url as unique idnetfication</span></div><div class="line">        query.equalTo(<span class="string">"url"</span>,url);</div><div class="line">        query.find(&#123;</div><div class="line">            success: <span class="function"><span class="keyword">function</span>(<span class="params">results</span>)</span>&#123;</div><div class="line">                <span class="keyword">if</span>(results.length&gt;<span class="number">0</span>)</div><div class="line">                &#123;</div><div class="line">                    <span class="keyword">var</span> counter=results[<span class="number">0</span>];</div><div class="line">                    counter.fetchWhenSave(<span class="literal">true</span>); <span class="comment">//get recent result</span></div><div class="line">                    counter.increment(<span class="string">"time"</span>);</div><div class="line">                    counter.save();</div><div class="line">                &#125;</div><div class="line">                <span class="keyword">else</span></div><div class="line">                &#123;</div><div class="line">                    <span class="keyword">var</span> newcounter=<span class="keyword">new</span> Counter();</div><div class="line">                    newcounter.set(<span class="string">"title"</span>,title);</div><div class="line">                    newcounter.set(<span class="string">"url"</span>,url);</div><div class="line">                    newcounter.set(<span class="string">"time"</span>,<span class="number">1</span>);</div><div class="line">                    newcounter.save(<span class="literal">null</span>,&#123;</div><div class="line">                        success: <span class="function"><span class="keyword">function</span>(<span class="params">newcounter</span>)</span>&#123;</div><div class="line">                        <span class="comment">//alert('New object created');</span></div><div class="line">                        &#125;,</div><div class="line">                        error: <span class="function"><span class="keyword">function</span>(<span class="params">newcounter,error</span>)</span>&#123;</div><div class="line">                        alert(<span class="string">'Failed to create'</span>);</div><div class="line">                        &#125;</div><div class="line">                        &#125;);</div><div class="line">                &#125;</div><div class="line">            &#125;,</div><div class="line">            error: <span class="function"><span class="keyword">function</span>(<span class="params">error</span>)</span>&#123;</div><div class="line">                <span class="comment">//find null is not a error</span></div><div class="line">                alert(<span class="string">'Error:'</span>+error.code+<span class="string">" "</span>+error.message);</div><div class="line">            &#125;</div><div class="line">        &#125;);</div><div class="line">&#125;</div><div class="line"></div><div class="line">$(<span class="function"><span class="keyword">function</span>(<span class="params"></span>)</span>&#123;</div><div class="line">        <span class="keyword">var</span> Counter=AV.Object.extend(<span class="string">"Counter"</span>);</div><div class="line">        <span class="comment">//only increse visit counting when intering a page</span></div><div class="line">        <span class="keyword">if</span> ($(<span class="string">'.article-title'</span>).length == <span class="number">1</span>)</div><div class="line">           addCount(Counter);</div><div class="line">        <span class="keyword">var</span> query=<span class="keyword">new</span> AV.Query(Counter);</div><div class="line">        query.descending(<span class="string">"time"</span>);</div><div class="line">        <span class="comment">// the sum of popular posts</span></div><div class="line">        query.limit(<span class="number">10</span>); </div><div class="line">        query.find(&#123;</div><div class="line">            success: <span class="function"><span class="keyword">function</span>(<span class="params">results</span>)</span>&#123;</div><div class="line">                    <span class="keyword">for</span>(<span class="keyword">var</span> i=<span class="number">0</span>;i&lt;results.length;i++)    </div><div class="line">                    &#123;</div><div class="line">                        <span class="keyword">var</span> counter=results[i];</div><div class="line">                        title=counter.get(<span class="string">"title"</span>);</div><div class="line">                        url=counter.get(<span class="string">"url"</span>);</div><div class="line">                        time=counter.get(<span class="string">"time"</span>);</div><div class="line">                        <span class="comment">// add to the popularlist widget</span></div><div class="line">                        showcontent=title+<span class="string">" ("</span>+time+<span class="string">")"</span>;</div><div class="line">                        <span class="comment">//notice the "" in href</span></div><div class="line">                        $(<span class="string">'.popularlist'</span>).append(<span class="string">'&lt;li&gt;&lt;a href="'</span>+url+<span class="string">'"&gt;'</span>+showcontent+<span class="string">'&lt;/a&gt;&lt;/li&gt;'</span>);</div><div class="line">                    &#125;</div><div class="line">                &#125;,</div><div class="line">            error: <span class="function"><span class="keyword">function</span>(<span class="params">error</span>)</span>&#123;</div><div class="line">                alert(<span class="string">"Error:"</span>+error.code+<span class="string">" "</span>+error.message);</div><div class="line">            &#125;</div><div class="line">            &#125;</div><div class="line">        )</div><div class="line">        &#125;);</div><div class="line"><span class="tag">&lt;/<span class="name">script</span>&gt;</span></div></pre></td></tr></table></figure>
<p>这段代码的核心逻辑就是对Counter对象的增加和查询，每一篇文章都会有一个time字段来记录访问次数。这里查询的时候我用的是文章通过Hexo生成的URL作为主键的，所以post文件夹目录下的文件名一旦取好就不要轻易修改了，要不然访问次数会重新计算的:)。</p>
<h3 id="修改config.yml">修改config.yml</h3>
<p>最后，修改<code>theme/你的主题/config.yml</code>文件,在<code>widgets:</code>选项找个位置下添加<code>- popular_posts</code>即可。</p>
<h2 id="小结">小结</h2>
<p>插件的效果可以见我博客的右侧”浏览数目“插件，样式和文章显示数目可以直接在代码里改,浏览量数据都存在“Counter”表中，见图（顺便测试下七牛云）：</p>
<p><img src="http://crescent.qiniudn.com/LeanCloud%E6%95%B0%E6%8D%AE%E7%AE%A1%E7%90%86.jpg" alt="LeanCould数据管理"> PS：这个浏览量数据是可以自己改的，我是原来是照搬的wordpress的数据，它包括了Robot和人访问量的总和，感觉不太靠谱，不利于新文章上榜，现在全都除以10了:)。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在原来的wordpress博客中有一个WP-PostViews Plus插件，可以统计每篇文章的浏览量，可以为游客提供热门文章的信息，&lt;del&gt;(顺便满足一下作者的虚荣心)&lt;/del&gt;。现在切换到静态博客Hexo了，就需要第三方服务来实现这样的动态数据处理。这里要感谢师弟&lt;a href=&quot;http://ariwaranosai.xyz/&quot;&gt;ariwaranosai&lt;/a&gt;给我推荐的&lt;a href=&quot;https://cn.avoscloud.com/&quot;&gt;LeanCloud&lt;/a&gt;平台，以及&lt;a href=&quot;http://ibruce.info/2013/12/22/count-views-of-hexo/&quot;&gt;为hexo博客添加访问次数统计功能&lt;/a&gt;（基于BAE）提供的思路。使用LeanCloud的优点是它自己实现了一个AV.view 类，不需要考虑JavaScript的跨站访问问题。&lt;/p&gt;
&lt;h2 id=&quot;创建lean-cloud应用&quot;&gt;创建Lean Cloud应用&lt;/h2&gt;
&lt;p&gt;首先一句话介绍Lean Cloud:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://leancloud.cn/&quot;&gt;LeanCloud&lt;/a&gt;（aka. AVOS Cloud）提供一站式后端云服务，从数据存储、实时聊天、消息推送到移动统计，涵盖应用开发的多方面后端需求。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;我们只用到它的数据存储部分,具体步骤如下：&lt;/p&gt;
&lt;ol style=&quot;list-style-type: decimal&quot;&gt;
&lt;li&gt;首先到『控制台』创建一个应用，名字随便取。&lt;/li&gt;
&lt;li&gt;点击新建应用的『数据』选项，选择『创建Class』，取名为”Counter“。&lt;/li&gt;
&lt;li&gt;点击新建应用右上角的齿轮，在『应用Key』选项里得到APP ID 和 APP Key，在后面会用到。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;修改hexo页面&quot;&gt;修改Hexo页面&lt;/h2&gt;
&lt;h3 id=&quot;新建popular_posts.ejs&quot;&gt;新建popular_posts.ejs&lt;/h3&gt;
&lt;p&gt;首先在&lt;code&gt;theme/你的主题/layout/_widget&lt;/code&gt;目录下新建&lt;code&gt;popular_posts.ejs&lt;/code&gt;文件,其内容为&lt;/p&gt;
&lt;figure class=&quot;highlight html&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;8&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;9&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;name&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;attr&quot;&gt;if&lt;/span&gt; (&lt;span class=&quot;attr&quot;&gt;site.posts.length&lt;/span&gt;)&amp;#123; %&amp;gt;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  &lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;name&quot;&gt;div&lt;/span&gt; &lt;span class=&quot;attr&quot;&gt;class&lt;/span&gt;=&lt;span class=&quot;string&quot;&gt;&quot;widget-wrap&quot;&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;name&quot;&gt;h3&lt;/span&gt; &lt;span class=&quot;attr&quot;&gt;class&lt;/span&gt;=&lt;span class=&quot;string&quot;&gt;&quot;widget-title&quot;&lt;/span&gt;&amp;gt;&lt;/span&gt;浏览数目&lt;span class=&quot;tag&quot;&gt;&amp;lt;/&lt;span class=&quot;name&quot;&gt;h3&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;name&quot;&gt;div&lt;/span&gt; &lt;span class=&quot;attr&quot;&gt;class&lt;/span&gt;=&lt;span class=&quot;string&quot;&gt;&quot;widget&quot;&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      &lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;name&quot;&gt;ul&lt;/span&gt; &lt;span class=&quot;attr&quot;&gt;class&lt;/span&gt;=&lt;span class=&quot;string&quot;&gt;&quot;popularlist&quot;&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      &lt;span class=&quot;tag&quot;&gt;&amp;lt;/&lt;span class=&quot;name&quot;&gt;ul&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;span class=&quot;tag&quot;&gt;&amp;lt;/&lt;span class=&quot;name&quot;&gt;div&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  &lt;span class=&quot;tag&quot;&gt;&amp;lt;/&lt;span class=&quot;name&quot;&gt;div&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;name&quot;&gt;%&lt;/span&gt; &amp;#125; %&amp;gt;&lt;/span&gt;&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h3 id=&quot;修改head.ejs&quot;&gt;修改head.ejs&lt;/h3&gt;
&lt;p&gt;修改&lt;code&gt;theme/你的主题/layout/_partial/head.ejs&lt;/code&gt;文件,在head标签的最后插入：&lt;/p&gt;
&lt;figure class=&quot;highlight javascript&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;&amp;lt;script src=&lt;span class=&quot;string&quot;&gt;&quot;https://cdn1.lncld.net/static/js/av-min-1.2.1.js&quot;&lt;/span&gt;&amp;gt;&lt;span class=&quot;xml&quot;&gt;&lt;span class=&quot;tag&quot;&gt;&amp;lt;/&lt;span class=&quot;name&quot;&gt;script&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  &amp;lt;script&amp;gt;AV.initialize(&lt;span class=&quot;string&quot;&gt;&quot;你的APP ID&quot;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;&quot;你的APP Key&quot;&lt;/span&gt;);&lt;span class=&quot;xml&quot;&gt;&lt;span class=&quot;tag&quot;&gt;&amp;lt;/&lt;span class=&quot;name&quot;&gt;script&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;注意Lean Cloud引用的js文件位置可能会变化，如果代码里位置打不开的话，记得去官方的&lt;a href=&quot;https://leancloud.cn/docs/sdk_setup-js.html#npm_安装&quot;&gt;JavaScript SDK文档&lt;/a&gt;找一下最新的位置。&lt;/p&gt;
&lt;h3 id=&quot;修改after-footer.ejs&quot;&gt;修改after-footer.ejs&lt;/h3&gt;
&lt;p&gt;修改&lt;code&gt;theme/你的主题/layout/_partial/after-footer.ejs&lt;/code&gt;文件,在最后插入：
    
    </summary>
    
      <category term="杂项" scheme="http://crescentluna.github.io/categories/%E6%9D%82%E9%A1%B9/"/>
    
    
      <category term="Hexo" scheme="http://crescentluna.github.io/tags/Hexo/"/>
    
  </entry>
  
  <entry>
    <title>博客迁移小记</title>
    <link href="http://crescentluna.github.io/2014/12/11/relocation/"/>
    <id>http://crescentluna.github.io/2014/12/11/relocation/</id>
    <published>2014-12-11T02:17:06.000Z</published>
    <updated>2017-02-21T18:24:27.407Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一点闲话">一点闲话</h2>
<p>扯一点闲话，这半年发生了不少事,导致博客断更了半年多。一言难尽，就上一句心灵鸡汤来概括吧，顺便弘扬一下正能量：</p>
<blockquote>
<p><strong>所有的事情最后都会有好结局，如果你觉得结局不好，那是因为还没有到最后。</strong></p>
</blockquote>
<h2 id="迁移原因">迁移原因</h2>
<p>进入正题，这两天闲下来之后，也是起了心思，想把博客从wordpress迁移到静态博客上，原因如下：</p>
<ol style="list-style-type: decimal">
<li>最近用惯了markdown，想更方便地写东西，更加专注于内容。而wordpress没有好用的markdown插件，以前都是写tex里然后往博客上粘贴，太麻烦了。</li>
<li>作为一个不称职的程序员，用了这么久的wordpress，现在终于想要有一点control every thing的感觉，自己动手折腾个静态博客，顺便学习一下javascript，jquery，css基础。</li>
<li>还有就是静态博客的普遍优点了，比如不用服务器，省了买空间的钱（免费的<a href="https://pages.github.com/" target="_blank" rel="external">Github Pages</a>），访问速度快，容易被搜索引擎抓取等。</li>
</ol>
<p>静态博客先使用了Github推荐的<a href="http://jekyllrb.com/" target="_blank" rel="external">Jekyll</a>，发现需要自己搞的东西太多，于是转用了基于node.js的<a href="http://hexo.io/" target="_blank" rel="external">Hexo</a>。Hexo有以下优点：</p>
<ol style="list-style-type: decimal">
<li>生成文章速度非常快。</li>
<li>命令简单，只需要会<code>hexo n</code>，<code>hexo g</code>，<code>hexo s</code>，<code>hexo d</code>就能 搞定一切。</li>
<li>中文用户多，文档和插件比较全。</li>
<li>能通过插件解决markdwon和mathjax冲突问题。</li>
</ol>
<h2 id="搭建参考">搭建参考</h2>
<p>在搭建过程，主要参考了以下文章:</p>
<ul>
<li>官方文档：<a href="http://hexo.io/docs/" class="uri" target="_blank" rel="external">http://hexo.io/docs/</a></li>
<li>hexo作者tommy351的博客：<a href="http://zespia.tw/blog/2012/10/11/hexo-debut/" target="_blank" rel="external">Hexo 颯爽登場！</a></li>
<li>不如的博客：<a href="http://ibruce.info/2013/11/22/hexo-your-blog/" target="_blank" rel="external">hexo你的博客</a></li>
<li><a href="http://zipperary.com/categories/hexo/" target="_blank" rel="external">Zippera的hexo系列教程</a></li>
<li>官方主题：<a href="https://github.com/hexojs/hexo/wiki/Themes" class="uri" target="_blank" rel="external">https://github.com/hexojs/hexo/wiki/Themes</a></li>
<li>官方插件：<a href="https://github.com/tommy351/hexo/wiki/Plugins" class="uri" target="_blank" rel="external">https://github.com/tommy351/hexo/wiki/Plugins</a></li>
<li>(20160828更新)<a href="http://bblove.me/2016/03/06/migrate-pages-from-gitcafe-to-coding/" target="_blank" rel="external">解决 Github Pages 禁止百度爬虫的方法</a>，基本思路是把博客同时发布到github pages和coding pages.然后使用dnspod设置域名解析,国内线路解析到coding.net,国外线路解析到github</li>
</ul>
<p>以上的文章已经足够基本了解hexo博客的整个搭建过程了，只需要照做即可。我的博客主题是基于<a href="https://github.com/xiangming/landscape-plus" target="_blank" rel="external">landscape-plus</a>改的，另外参考了howiefh的 <a href="https://github.com/howiefh/hexo-theme-landscape-f" target="_blank" rel="external">landscape-f</a>主题，添加了多说最近评论小部件以及我自己写的浏览计数小部件。图片都托管在<a href="http://www.qiniu.com/" target="_blank" rel="external">七牛云存储</a>上。</p>
<h2 id="迁移问题">迁移问题</h2>
<p>在博客从wordpress迁移到hexo的过程中，主要碰到的问题及解决办法如下:</p>
<ol style="list-style-type: decimal">
<li>原wordpress博客链接的死链问题。原先我wordpress文章链接用的是坑爹的默认生成方式，是通过?传p的参数来获取文章的URL。问题是p是个随机数字，我又没办法在静态页面上动态地获取p的值做重定向。现在折衷的解决办法是暂时不改域名，先用着github提供的二级域名，然后在原域名上用wordpress的Redirect插件对文章链接做301重定向，看看搜索引擎能不能把原文章的URL给改了。 <del>（我会尽快把这个域名的DNS解析转到github的博客上来）。目前你可以通过<a href="www.crescentmoon.info" class="uri">www.crescentmoon.info</a>访问原wordpress博客，但是点击任何文章都会跳转到新博客来:-D。</del>这个没有成功，请从搜索引擎点文章名过来的读者在博客页面寻找一下那篇文章吧囧。</li>
<li>mathjax和markdown渲染冲突问题。因为markdown的解析要优先于mathjax，所以经常会导致mathjax渲染失败，需要玩一些tricks，比如latex语法中的下标’_‘要改成转义的’\_’, equation环境需要套一个div标签或者rawblock环境等，带来了无穷无尽的麻烦。还好hexo上有大神写的<a href="https://github.com/wzpan/hexo-renderer-pandoc" target="_blank" rel="external">hexo-renderer-pandoc插件</a>，用pandoc去代替默认的markdown渲染器，完美地解决了这个问题。</li>
<li>文章的浏览数小部件问题。原先我的wordpress有个显示文章浏览数的小插件，现在换成静态网页了，就必须用第三方的服务实现这个功能，具体请见下一篇博客啦。</li>
<li>多说评论没有跟过来的问题，这个正在解决中。</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;一点闲话&quot;&gt;一点闲话&lt;/h2&gt;
&lt;p&gt;扯一点闲话，这半年发生了不少事,导致博客断更了半年多。一言难尽，就上一句心灵鸡汤来概括吧，顺便弘扬一下正能量：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;所有的事情最后都会有好结局，如果你觉得结局不好，那是因为还
    
    </summary>
    
      <category term="杂项" scheme="http://crescentluna.github.io/categories/%E6%9D%82%E9%A1%B9/"/>
    
      <category term="闲话" scheme="http://crescentluna.github.io/categories/%E6%9D%82%E9%A1%B9/%E9%97%B2%E8%AF%9D/"/>
    
    
      <category term="Hexo" scheme="http://crescentluna.github.io/tags/Hexo/"/>
    
  </entry>
  
  <entry>
    <title>概率图模型简介</title>
    <link href="http://crescentluna.github.io/2014/02/24/%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B%E7%AE%80%E4%BB%8B/"/>
    <id>http://crescentluna.github.io/2014/02/24/概率图模型简介/</id>
    <published>2014-02-23T21:56:02.000Z</published>
    <updated>2017-02-21T18:24:27.407Z</updated>
    
    <content type="html"><![CDATA[<h1 id="介绍">介绍</h1>
<h2 id="定义">定义</h2>
<p>概率图模型(Graphical Model)是概率论和图论之间的桥梁，概率图模型的基本想法来自模块的概念，即一个复杂系统是由简单的部分所联合而成的。概率论部分告诉我们哪些部分是耦合在一起的，然后提供推断模型的方法，而图论部分给我们一个非常直观的认识，把拥有相互关系的变量看做数据结构，从而导出一般化的方法来解决这个问题。很多经典的多元概率系统，比如混合模型，因子分析，隐含马尔科夫模型,Kalman filter 和Ising model等，从概率图模型的角度来看，都可以看做普遍隐含形成过程的一种实例表现。这意味着，一旦在某个系统上有什么特别的方法发现，就很容易推广到一系列的模型中去。除此之外，概率图模型还非常自然地提供了设计新系统的方法。</p>
<p>在概率图模型中，点代表随机变量，点与点之间边的存在与否代表了点与点之间是存在条件依赖。点与边的组合描绘了联合概率分布的特征结构。假设有<span class="math inline">\(N\)</span>个二元随机变量，在没有任何信息帮助的情况下，联合分布<span class="math inline">\(P(X_1,\ldots,X_N)\)</span>，需要<span class="math inline">\(O(2^N)\)</span>个参数。而通过概率图描绘点与点之间的条件关系之后，表示联合分布，所需要的参数会减少很多，这对后面的模型的推断和学习是有帮助的。</p>
<p>概率图模型主要分为两种，无向图(也叫Markov random fields)和有向图(也叫Bayesian networks)，前者多用于物理和图像领域，后者多用于AI和机器学习，具体的基本就不多介绍了。下面给出一个简单贝叶斯网络的例子。</p>
<p><a href="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20140224133640.png" target="_blank" rel="external"><img src="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20140224133640.png" alt="QQ截图20140224133640"></a></p>
<p>这里Cloudy指是否多云，Sprinkler指洒水器是否打开，Rain指是否有雨， WetGrass指草地是否湿了。</p>
<a id="more"></a>
<h1 id="推断inference">推断(Inference)</h1>
<p>推断的主要目的是在知道观察变量的值的情况下估计隐藏变量的值。如果我们观察到生成模型的“叶子”，然后可以尝试推断隐藏的原因(diagnosis)，反之如果观察到生成模型的“根”，就可以尝试预测它的节点(predicition)。</p>
<p>使用的方法就是贝叶斯公式</p>
<p><span class="math display">\[P(X|y)=\frac{P(y|X)P(X)}{P(y)}\]</span></p>
<p>其中<span class="math inline">\(X\)</span>是隐藏变量，<span class="math inline">\(y\)</span>是观察到的值。公式可以这么解释:</p>
<p><span class="math display">\[\text{posterior}=\frac{\text{conditional likelihood} \times \text{prior}}{\text{likelihood}}\]</span></p>
<p>以前面的图为例，当我们观察到草地是湿的(<span class="math inline">\(W=1\)</span>)时，有可能有2个原因:下雨(<span class="math inline">\(R=1\)</span>)或者开着洒水器(<span class="math inline">\(S=1\)</span>)。我们根据贝叶斯公式求得它们各自的概率</p>
<p><span class="math display">\[P(S=1|W=1)=\frac{P(S=1,W=1)}{P(W=1)}=\frac{\sum_{c,r}P(C=c,S=1,W=1,R=r)}{P(W=1)}=0.430\]</span></p>
<p><span class="math display">\[P(R=1|W=1)=\frac{P(R=1,W=1)}{P(W=1)}=\frac{\sum_{c,s}P(C=c,S=1,W=1,R=r)}{P(W=1)}=0.708\]</span></p>
<p>分母作为归一化常数，同时也是似然</p>
<p><span class="math display">\[P(W=1)=\sum_{c,s,r}P(C=c,S=r,R=r,W=1)=0.6471\]</span></p>
<p>可以看到草地湿因为下雨的概率要比因为洒水器的概率高。一般来说在实际情况中，利用后验概率算积分的时候不会像例子这么容易。比如分母<span class="math inline">\(Z\)</span>，有可能是指数级别的加和（可以参见<a href="http://freemind.pluskid.org/machine-learning/probabilistic-graphical-model/" target="_blank" rel="external">pluskid大神的文章</a>），而且，在连续型隐藏变量里，很有可能是求一个无解析解的积分。</p>
<p>因为推断的问题一般都比较复杂，我们自然希望能用一些方法来加速推断的过程。</p>
<h2 id="参数消去">参数消去</h2>
<p>根据概率图的结构，我们可以将条件独立的节点的生成概率区分开来写，比如:</p>
<p><span class="math display">\[
\begin{split}
&amp;P(W=w) =\sum_c\sum_s\sum_r P(C=c,S=s,R=r,W=w) \\
&amp;= \sum_c \sum_s \sum_r P(C=c)\times P(S=s|C=c)\times P(R=r|C=c)\times P(W=w|S=s,R=r) \\
\end{split}
\]</span></p>
<p>参数消去算法的主要目的就是尽量可能的合并子项来减少计算量，我们将上面的式子化为：</p>
<p><span class="math display">\[ P(W=w)=\sum_c P(C=c)\times \sum_s P(S=s|C=c)\times \sum_r P(R=r|C=c)\times P(W=w|S=s,R=r) \]</span></p>
<p>可以看到第三项与前两个求和项无关。这样我们可以得到合并之后的图： <a href="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20140224133826.png" target="_blank" rel="external"><img src="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20140224133826.png" alt="QQ截图20140224133826"></a></p>
<p>如果概率图比较复杂，存在重复统计的情况，我们可以使用动态规划算法来避免重复的计算。如果概率图是无环的(比如一棵树)，可以用局部信息传播算法(Belief Propagation，HMM的Sum-Product算法的泛化版本)来做推断。如果概率图有环就比较麻烦，因为局部信息传播可能会被重复计算，从而导致算法不收敛。这个时候可以用Junction Tree方法把概率图转化成一棵树（可能会很复杂，见<a href="http://freemind.pluskid.org/machine-learning/probabilistic-graphical-model/" target="_blank" rel="external">pluskid 的例子</a>）。</p>
<h2 id="近似推断">近似推断</h2>
<p>推断算法的时间复杂度是指数级别的（取决于概率图中的最大簇的大小），优化它是个NP-hard 问题，所以就导致了推断的近似方法产生。除了图复杂以外还有另外一个原因，即使图的复杂度很低，如果一些节点是连续随机变量，在很多情况下它们对应的积分是没有解析解的。</p>
<p>流行的近似推断方法主要有三种：</p>
<ul>
<li><p>Sampling(Monte Carlo) methods。采样做法就是通过抽取大量的样本来逼近真实的分布。最简单的是importance sampling，根据出现的结果的比例采样。在高维空间中更有效的方法叫Markov Chain Monte Carlo，是利用马科夫链的性质来生成某个分布的样本，包括Metropolis-Hastings算法和Gibbs Samppling算法等。</p></li>
<li><p>Variational Infernece。变分推断采取的是另一种做法，通过限制近似分布的类型，得到一种局部最优，但具有确定解的近似后验分布。最简单的方法叫做mean-field approximation，就是把概率图里的点全部解耦，看做相互独立的，然后对每个点引进一个变分参数，通过循环地迭代参数来最小化近似分布和真实分布的KL距离。</p></li>
<li><p>Loopy Belief Propagation。之前我们讲到Belief Propagation 算法，是应用在无环的图上的，然后LBP就是不管图有没有环，都直接使用这个算法去求解。</p></li>
</ul>
<h1 id="学习learning">学习(Learning)</h1>
<p>Learning可以分很多情况，比如估计参数，或者估计模型的结构，或者两者都有。根据变量是否被观察到和结构是否已知可以对learning方法分类（见下图)：</p>
<p><a href="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20140224134911.png" target="_blank" rel="external"><img src="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20140224134911.png" alt="QQ截图20140224134911"></a></p>
<p>还有一个区别在于，我们的目标在于寻找一个最有可能的模型（点估计），还是在所有可能的模型上做贝叶斯估计。在结构已知的时候，贝叶斯参数估计和推断是等价的，这时候隐藏的节点就代表了这些参数。如果结构未知，还要有对概率图结构的学习过程。</p>
<h2 id="known-structurefull-observability">Known structure,full observability</h2>
<p>这种最简单，因为结构已知，变量全部都观察到了，所以直接求极大似然值(MLE)就好了。令数据为<span class="math inline">\(D=\{D_1,\ldots,D_M\}\)</span>，那么似然为</p>
<p><span class="math display">\[
L=\frac{1}{M}\log \prod^M_{m=1}P(D_m|G)=\frac{1}{M}\sum^n_{i=1}\sum^M_{m=1}\log P(X_i|Pa(X_i),D_m)
\]</span></p>
<p>这里<span class="math inline">\(Pa(X_i)\)</span>是<span class="math inline">\(X_i\)</span>的父亲节点。我们依据概率图的结构分解整个似然式子，然后独立地最大化每个参数。以之前的图为例，我们有</p>
<p><span class="math display">\[ P_{ML}(W=w|S=s,R=r)=\frac{(W=w,S=s,R=r)}{(S=s,R=r)} \]</span></p>
<p>这里<span class="math inline">\((S=s,R=r)\)</span>和<span class="math inline">\((W=w,S=s,R=r)\)</span>分别为下雨而且洒水器开着的次数和满足前两个条件的同时，草地湿了的次数。可以看到极大似然是根据观察到的数据得出最有可能的结果。如果我们有一些训练数据的话，也可以加上先验条件，做最大后验估计(MAP).</p>
<h2 id="known-structurepartial-observability">Known structure,partial observability</h2>
<p>如果结构已知，但是有些节点是隐藏的话，我们可以通过EM算法来寻找一个局部最优的MLE。EM的算法的思想就是求那些隐藏节点的期望值，然后把期望值看做观察到的值。以<span class="math inline">\(W\)</span>节点为例，我们用期望代替观察到的次数，得到</p>
<p><span class="math display">\[P(W=w|S=s,R=r)=\frac{E(W=w,S=s,R=r)}{E(S=s,R=r)}\]</span></p>
<p>这里<span class="math inline">\(E(e)\)</span>就是在当前参数下，事件<span class="math inline">\(e\)</span>发生次数的期望值。这个期望可以由单次发生事件<span class="math inline">\(e\)</span>的期望多次加和得到。</p>
<p><span class="math display">\[E  (e)=E \sum_m I(e|D_m)=\sum_m P(e|D_m)\]</span></p>
<p>这里<span class="math inline">\(I(e|D_m)\)</span>是个指示函数，当事件<span class="math inline">\(e\)</span>在第<span class="math inline">\(m\)</span>个数据时发生为1，其余都为0。得到期望值后，我们可以重新最大化参数，然后重新计算期望，反复迭代达到一个局部的最优解。</p>
<h2 id="unknown-structurefull-observability">Unknown structure,full observability</h2>
<p>这种情况虽然我们可以观察到概率图中每个节点的数据，但是节点与节点之间的边关系未知。完全图能够给出最大的似然值，那是因为这种情况最复杂，含有最多的参数，很明显过拟合了。 为了解决过拟合的问题，我们可以加上对模型选择的先验条件，使用贝叶斯公式最大化后验</p>
<p><span class="math display">\[P(G|D)=\frac{P(D|G)P(G)}{P(D)}\]</span></p>
<p>求log我们得到</p>
<p><span class="math display">\[L=\log P(G|D)=\log P(D|G) + \log P(G)+ c\]</span></p>
<p>这里<span class="math inline">\(c=-\log P(D)\)</span>是个与模型<span class="math inline">\(G\)</span>独立的常数。如果先验偏向于简单的模型，那么<span class="math inline">\(P(G)\)</span>就能对复杂模型起到惩罚作用。事实上就算先验不这么做，使用贝叶斯原理计算的边缘似然(marginal likelihood)</p>
<p><span class="math display">\[P(D|G)=\int P(D|G,\theta)P(\theta|G)d \theta\]</span></p>
<p>(<span class="math inline">\(\theta\)</span>为模型的参数)能够自动地倾向解释数据的最简单模型，如果模型参数太多的话，它只能在一个很宽的数据范围内预测，而没办法给小范围数据一个很大的发生概率（也就是说复杂模型更可能“对”，但是不够“准”），而简单模型刚好反之。这个现象被叫做奥卡姆剃刀(Ockham’razor)。</p>
<p><a href="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20140224134256.png" target="_blank" rel="external"><img src="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20140224134256.png" alt="QQ截图20140224134256"></a></p>
<p>尽管我们可以通过上面那个<span class="math inline">\(L\)</span>来衡量模型的好坏（被称作Bayesian Score），我们依然还是需要一个方法来寻找得分最高的概率图。穷举的复杂度是超指数级别的，所以我们一般用一些局部的搜索算法（比如多重启的爬山法）或者划分区域来搜索图空间。另外地，由于我们求的是模型生成数据的后验概率<span class="math inline">\(P(G|D)\)</span>，我们可以直接从这个后验概率分布中抽样一些图出来，这叫MCMC search。</p>
<h2 id="unknown-structurepartial-observability">Unknown structure,partial observability</h2>
<p>最后也是最难的情况，就是图结构未知，而且存在隐藏变量。这导致了边缘似然非常难求，需要对隐藏变量<span class="math inline">\(Z\)</span>积分才能得到。</p>
<p><span class="math display">\[P(D|G)=\int_Z P(D,Z|G,\theta)P(\theta|G)d \theta\]</span></p>
<p>一种办法是做拉普拉斯近似(Laplace Approximation)，计算得到</p>
<p><span class="math display">\[\log P(D|G) \approx \log P(D|G,\hat{\theta_G})-\frac{d}{2}\log M\]</span></p>
<p>这里<span class="math inline">\(M\)</span>是样本个数，<span class="math inline">\(\hat{\theta_G}\)</span>是参数的最大似然估计(由EM算法得到),<span class="math inline">\(d\)</span>是模型的维度（在全观察到的情况下，模型维度等于自由参数的个数，在存在隐藏变量的情况下,模型维度会少于参数个数）。这个式子被叫做Bayesian Information Criterion，和Minimum Description Length (MDL) 是等价的。</p>
<p>式子的第一项就是似然，而第二项减去一个代表模型复杂度的量，所以BIC score是对复杂模型有惩罚的。尽管BIC是可以分解的(decomposable)，局部的搜索算法还是非常昂贵，因为我们每一步都要跑一次EM 算法来得到<span class="math inline">\(\theta\)</span>。一种可选的办法是在EM的M 步直接做一个局部搜索，得到一个局部最优的BIC score，这个方法被称作Structural EM。</p>
<p>当然，结构学习不仅仅包括寻找已经存在节点的连通性，还包括在必要情况下增加隐藏节点等等更为复杂的东西。</p>
<p>参考文献：</p>
<ol style="list-style-type: decimal">
<li>《An introduction to graphical models》 Kevin P. Murphy</li>
<li>http://freemind.pluskid.org/machine-learning/probabilistic-graphical-model/</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;介绍&quot;&gt;介绍&lt;/h1&gt;
&lt;h2 id=&quot;定义&quot;&gt;定义&lt;/h2&gt;
&lt;p&gt;概率图模型(Graphical Model)是概率论和图论之间的桥梁，概率图模型的基本想法来自模块的概念，即一个复杂系统是由简单的部分所联合而成的。概率论部分告诉我们哪些部分是耦合在一起的，然后提供推断模型的方法，而图论部分给我们一个非常直观的认识，把拥有相互关系的变量看做数据结构，从而导出一般化的方法来解决这个问题。很多经典的多元概率系统，比如混合模型，因子分析，隐含马尔科夫模型,Kalman filter 和Ising model等，从概率图模型的角度来看，都可以看做普遍隐含形成过程的一种实例表现。这意味着，一旦在某个系统上有什么特别的方法发现，就很容易推广到一系列的模型中去。除此之外，概率图模型还非常自然地提供了设计新系统的方法。&lt;/p&gt;
&lt;p&gt;在概率图模型中，点代表随机变量，点与点之间边的存在与否代表了点与点之间是存在条件依赖。点与边的组合描绘了联合概率分布的特征结构。假设有&lt;span class=&quot;math inline&quot;&gt;\(N\)&lt;/span&gt;个二元随机变量，在没有任何信息帮助的情况下，联合分布&lt;span class=&quot;math inline&quot;&gt;\(P(X_1,\ldots,X_N)\)&lt;/span&gt;，需要&lt;span class=&quot;math inline&quot;&gt;\(O(2^N)\)&lt;/span&gt;个参数。而通过概率图描绘点与点之间的条件关系之后，表示联合分布，所需要的参数会减少很多，这对后面的模型的推断和学习是有帮助的。&lt;/p&gt;
&lt;p&gt;概率图模型主要分为两种，无向图(也叫Markov random fields)和有向图(也叫Bayesian networks)，前者多用于物理和图像领域，后者多用于AI和机器学习，具体的基本就不多介绍了。下面给出一个简单贝叶斯网络的例子。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20140224133640.png&quot;&gt;&lt;img src=&quot;http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20140224133640.png&quot; alt=&quot;QQ截图20140224133640&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;这里Cloudy指是否多云，Sprinkler指洒水器是否打开，Rain指是否有雨， WetGrass指草地是否湿了。&lt;/p&gt;
    
    </summary>
    
      <category term="学术" scheme="http://crescentluna.github.io/categories/%E5%AD%A6%E6%9C%AF/"/>
    
    
      <category term="概率图" scheme="http://crescentluna.github.io/tags/%E6%A6%82%E7%8E%87%E5%9B%BE/"/>
    
  </entry>
  
  <entry>
    <title>变分推断学习笔记(2)——一维高斯模型的例子</title>
    <link href="http://crescentluna.github.io/2013/10/11/variational-inference-2/"/>
    <id>http://crescentluna.github.io/2013/10/11/variational-inference-2/</id>
    <published>2013-10-11T00:10:18.000Z</published>
    <updated>2017-02-21T18:24:27.407Z</updated>
    
    <content type="html"><![CDATA[<p>变分推断学习笔记系列：</p>
<ol style="list-style-type: decimal">
<li><a href="http://crescentluna.github.io/2013/10/03/%E5%8F%98%E5%88%86%E6%8E%A8%E6%96%AD%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01%E2%80%94%E2%80%94%E6%A6%82%E5%BF%B5%E4%BB%8B%E7%BB%8D/">变分推断学习笔记(1)——概念介绍</a></li>
<li><a href="http://crescentluna.github.io/2013/10/11/variational-inference-2/">变分推断学习笔记(2)——一维高斯模型的例子</a></li>
<li><a href="http://crescentluna.github.io/2014/12/12/variational-inference-3/">变分推断学习笔记(3)——三硬币问题的变分推断解法</a></li>
</ol>
<hr>

<p>举一个一元高斯模型的例子。假设我们有数据<span class="math inline">\(\mathbf{X}=\{x_1,\ldots,x_M\}\)</span>，要推断平均值<span class="math inline">\(\mu\)</span>和精度<span class="math inline">\(\tau(1/\sigma)\)</span>的后验概率分布。 写出似然 <span class="math display">\[\begin{equation}
p(\mathbf{X}|\mu,\tau)=(\frac{\tau}{2\pi})^{N/2}\exp\{-\frac{\tau}{2}\sum^N_{n=1}(x_n-\mu)^2\}
\end{equation}\]</span> 其中<span class="math inline">\(\mu,\tau\)</span>各自服从先验分布 <span class="math display">\[\begin{equation}p(\mu|\tau)=N(\mu|\mu,(\lambda_0\tau)^{-1})\end{equation}\]</span> <span class="math display">\[\begin{equation}p(\tau)=Gam(\tau|a_0,b_0)\end{equation}\]</span> 其中Gam为Gamma分布（见备注1）。</p>
<h2 id="通用的估计方法">通用的估计方法</h2>
<p>好，我们现在假设<span class="math inline">\(q\)</span>之间的分布都独立。 <span class="math display">\[\begin{equation}q(\mu,\tau)=q_u(\mu)q_r(\tau)\end{equation}\]</span><a id="more"></a></p>
<p>对于<span class="math inline">\(q_u(\mu)\)</span>我们有 <span class="math display">\[\begin{equation}
\begin{split}
\ln q^*_u(\mu)&amp;= \mathbb{E}_r[\ln p(\mathbf{X}|\mu,\tau)+\ln p(\mu|\tau)]+const \\\
&amp;= -\frac{\mathbb{E}[\tau]}{2}\{\lambda_0(\mu-u_0)^2+\sum^N_{n=1}(x_n-\mu)^2\}+const \\\
\end{split}
\end{equation}\]</span></p>
<p>我们把未知数<span class="math inline">\(\mu\)</span>的项加和起来，就可以看出<span class="math inline">\(q^*_u(\mu)\)</span>恰好是个高斯分布 <span class="math inline">\(N(\mu|u_N,\lambda_N^{-1})\)</span>，其中 <span class="math display">\[\begin{equation}
\begin{split}u_N &amp;=\frac{\lambda_0u_0+N\bar{x}}{\lambda_0+N} \\\lambda_N &amp;=(\lambda_0+N)\mathbb{E}[\tau] \\\end{split}\end{equation}\]</span></p>
<p>同样对于<span class="math inline">\(q_r(\tau)\)</span>，我们有 <span class="math display">\[\begin{equation}\begin{split}\ln q^*_r(\tau)&amp;=\mathbb{E}_u[\ln p(\mathbf{X}|\mu,\tau)+\ln p(\mu|\tau)]+\ln p(\tau)+const \\&amp;=(a_0-1)\ln \tau-b_o \tau+\frac{1}{2}\ln \tau+\frac{N}{2}\ln \tau \\&amp; -\frac{\tau}{2}\mathbb{E}_u[\sum^N_{n=1}(x_n-\mu)^2+\lambda_0(\mu-u_0)^2]+const \\\end{split}\end{equation}\]</span> 这里<span class="math inline">\(q^*_r(\tau)\)</span>也恰好是个Gamma分布 <span class="math inline">\(Gam(\tau|a_N,b_N)\)</span>,其中 <span class="math display">\[\begin{equation}\begin{split}a_N&amp;=a_0+\frac{N}{2} \\b_N&amp;=b_0+\frac{1}{2}\mathbb{E}_u[\sum^N_{n=1}(x_n-\mu)^2+\lambda_0(\mu-u_0)^2]\end{split}\end{equation}\]</span> 首先，要注意我们并未对<span class="math inline">\(q_u(\mu)\)</span>或<span class="math inline">\(q_r(\tau)\)</span>的最佳形式作出任何假设，它们就自然地形成了似然函数的形式（高斯分布）和它的先验分布形式（Gamma分布）。 然后可以看到这里<span class="math inline">\(q_u(\mu)\)</span>与<span class="math inline">\(q_r(\tau)\)</span>通过<span class="math inline">\(\mathbb{E}_r\)</span>与<span class="math inline">\(\mathbb{E}_u\)</span>相互依赖。我们展开这些式子，使用高斯分布与Gamma分布的性质(见备注1）计算它们的期望: <span class="math display">\[\begin{equation}\begin{split}&amp; E[\tau|a_N,b_N]=\frac{a_N}{b_N} \\&amp; E[\mu|u_N,\lambda^{-1}_N]=u_N \\&amp; E[X^2]=Var(X)+(E[X])^2 \\&amp; E[\mu^2|u_N,\lambda^{-1}_N]=\lambda^{-1}_N+u_N^2 \\\end{split}\end{equation}\]</span> 将式子（9）带入之前的式子（7）消去期望，最终得到: <span class="math display">\[\begin{equation}\begin{split}&amp; u_N =\frac{\lambda_0u_0+N\bar{x}}{\lambda_0+N} \\&amp; \lambda_N =(\lambda_0+N)\frac{a_N}{b_N} \\&amp; a_N=a_0+\frac{N+1}{2}\\&amp;b_N=b_0+\frac{1}{2}[(\lambda_0+N)(\lambda^{-1}_N+\mu^2_N)\\&amp; -2(\lambda_0u_0+\sum^N_{n=1}x_n)u_N+(\sum^N_{n=1}{x_n}^2)+\lambda_0{u_0}^2)]\\\end{split}\end{equation}\]</span> 所以这时候循环依赖的对象变成了<span class="math inline">\(\lambda_N\)</span>和<span class="math inline">\(b_N\)</span>。然后我们迭代计算这些值</p>
<ol style="list-style-type: decimal">
<li>利用x的值，计算<span class="math inline">\(a_N\)</span>和<span class="math inline">\(u_N\)</span>。</li>
<li>给<span class="math inline">\(\lambda_N\)</span>赋一个初始值</li>
<li>利用<span class="math inline">\(\lambda_N\)</span>,获得新的<span class="math inline">\(b_N\)</span>。</li>
<li>利用<span class="math inline">\(b_N\)</span>,获得新的<span class="math inline">\(\lambda_N\)</span>。</li>
<li>反复迭代3，4步，直到收敛为止。</li>
</ol>
<p>最后我们就得到了近似分布<span class="math inline">\(Q(Z)\)</span>的所有超参数的值。</p>
<h2 id="另一种估计方法">另一种估计方法</h2>
<p>首先我们看到，之前这个<span class="math inline">\(\ln p(X)\)</span>（也就是似然）难求是因为<span class="math inline">\(Z\)</span>未知，在我们这个例子里的具体表现为未知参数<span class="math inline">\(\mu\)</span>与<span class="math inline">\(\tau\)</span>之间存在耦合关系，即<span class="math inline">\(\mu\)</span>是由<span class="math inline">\(\tau\)</span>生成的(<span class="math inline">\(p(\mu|\tau)\)</span>。由于原模型存在共轭先验，所以变分后验分布的因子函数形式也可以用同样的共轭结构。因为我们定义<span class="math inline">\(Q(Z)\)</span>分布的目的是要获得tractable的分布，所以可以在原模型的分布上作小修改，只要斩断耦合的部分即可。（这部分论述可能有问题，还需要多看书才行）</p>
<p>所以我们假设<span class="math inline">\(q(\mu)\)</span>与<span class="math inline">\(q(\tau)\)</span>之间相互独立，即<span class="math inline">\(q(\mu)\)</span>的参数不受<span class="math inline">\(\tau\)</span>的控制。但它依旧是个高斯分布，<span class="math inline">\(q(\tau)\)</span>依旧是个Gamma分布，只是各自的参数未知。所以我们只要把下界看成这些分布的未知参数的函数形式，然后通过对各自参数的求导就能获得下界的极大值。(可能是因为指数家族的关系，未知参数的期望都有固定的函数形式，所以比较好求）</p>
<p>以之前为例，我们假设 <span class="math display">\[\begin{equation}\begin{split}&amp;q(\mu)=N(\mu|u_N,\lambda_N^{-1}) \\&amp;q(\tau)=Gam(\tau|a_N,b_N) \\\end{split}\end{equation}\]</span> 其中，<span class="math inline">\(a_N,b_N,u_N,\lambda_N^{-1}\)</span>均为未知参数。</p>
<p>写出变分下界 <span class="math display">\[\begin{equation}\begin{split}L &amp;=\int\int q(\mu,\tau)\ln \frac{p(X,\mu,\tau)}{q(\mu,\tau)} \text{du} \text{dr} \\&amp;=\mathbb{E}_q[\ln p(X,\mu,\tau)]-\mathbb{E}_q[\ln q(\mu,\tau)] \\&amp;=\mathbb{E}_q[\ln p(X|\mu,\tau)]+\mathbb{E}_q[\ln p(\mu|\tau)]+\mathbb{E}_q[\ln p(\tau)] \\&amp;-\mathbb{E}_q[\ln q(\mu)]-\mathbb{E}_q[\ln q(\tau)] \\\end{split}\end{equation}\]</span> 其中 <span class="math display">\[\begin{equation}\begin{split}&amp;\mathbb{E}_q[\ln p(X|\mu,\tau)]= \frac{N}{2}\mathbb{E}_r[\ln \tau]-\frac{\tau}{2}\mathbb{E}_u[\sum^N_{n=1}(x_n-\mu)^2] \\&amp;\mathbb{E}_q[\ln p(\mu|\tau)]= \frac{1}{2}\mathbb{E}_r[\ln \tau]- \frac{\tau}{2}\mathbb{E}_u[\lambda_0(\mu-u_0)^2]\\&amp;\mathbb{E}_q[\ln p(\tau)]= (a_0-1)\mathbb{E}_r[\ln \tau]-b_o \mathbb{E}_r[\tau]\\&amp;\mathbb{E}_q[\ln q(\mu)]= u_N\\&amp;\mathbb{E}_q[\ln p(\tau)]= \frac{a_N}{b_N}\\\end{split}\end{equation}\]</span> 根据Gamma分布的性质，将消去式(13)中的期望，最后我们获得的式子将只包括<span class="math inline">\(a_N,b_N,u_N,\lambda_N^{-1}\)</span>这4个变量，分别对其求导，就可以得到每个参数的更新公式了（同式（10））。</p>
<p>备注： 1.Gamma分布 <span class="math display">\[\begin{equation}Gam(\lambda|a,b)=\frac{1}{\Gamma(a)}b^a\lambda^{a-1}\exp(-b\lambda)\end{equation}\]</span> 它的一些期望 <span class="math display">\[\begin{equation}\begin{split}&amp; \mathbb{E}[\lambda]=\frac{a}{b} \\&amp; var[\lambda]=\frac{a}{b^2} \\&amp; \mathbb{E}[\ln \lambda]=\Psi(a)-\ln(b)\\\end{split}\end{equation}\]</span> 其中<span class="math inline">\(\Psi(a)=\frac{d}{da}\ln \Gamma(a)\)</span></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;变分推断学习笔记系列：&lt;/p&gt;
&lt;ol style=&quot;list-style-type: decimal&quot;&gt;
&lt;li&gt;&lt;a href=&quot;http://crescentluna.github.io/2013/10/03/%E5%8F%98%E5%88%86%E6%8E%A8%E6%96%AD%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01%E2%80%94%E2%80%94%E6%A6%82%E5%BF%B5%E4%BB%8B%E7%BB%8D/&quot;&gt;变分推断学习笔记(1)——概念介绍&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://crescentluna.github.io/2013/10/11/variational-inference-2/&quot;&gt;变分推断学习笔记(2)——一维高斯模型的例子&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://crescentluna.github.io/2014/12/12/variational-inference-3/&quot;&gt;变分推断学习笔记(3)——三硬币问题的变分推断解法&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;/hr&gt;
&lt;p&gt;举一个一元高斯模型的例子。假设我们有数据&lt;span class=&quot;math inline&quot;&gt;\(\mathbf{X}=\{x_1,\ldots,x_M\}\)&lt;/span&gt;，要推断平均值&lt;span class=&quot;math inline&quot;&gt;\(\mu\)&lt;/span&gt;和精度&lt;span class=&quot;math inline&quot;&gt;\(\tau(1/\sigma)\)&lt;/span&gt;的后验概率分布。 写出似然 &lt;span class=&quot;math display&quot;&gt;\[\begin{equation}
p(\mathbf{X}|\mu,\tau)=(\frac{\tau}{2\pi})^{N/2}\exp\{-\frac{\tau}{2}\sum^N_{n=1}(x_n-\mu)^2\}
\end{equation}\]&lt;/span&gt; 其中&lt;span class=&quot;math inline&quot;&gt;\(\mu,\tau\)&lt;/span&gt;各自服从先验分布 &lt;span class=&quot;math display&quot;&gt;\[\begin{equation}p(\mu|\tau)=N(\mu|\mu,(\lambda_0\tau)^{-1})\end{equation}\]&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[\begin{equation}p(\tau)=Gam(\tau|a_0,b_0)\end{equation}\]&lt;/span&gt; 其中Gam为Gamma分布（见备注1）。&lt;/p&gt;
&lt;h2 id=&quot;通用的估计方法&quot;&gt;通用的估计方法&lt;/h2&gt;
&lt;p&gt;好，我们现在假设&lt;span class=&quot;math inline&quot;&gt;\(q\)&lt;/span&gt;之间的分布都独立。 &lt;span class=&quot;math display&quot;&gt;\[\begin{equation}q(\mu,\tau)=q_u(\mu)q_r(\tau)\end{equation}\]&lt;/span&gt;
    
    </summary>
    
      <category term="学术" scheme="http://crescentluna.github.io/categories/%E5%AD%A6%E6%9C%AF/"/>
    
      <category term="变分推断笔记" scheme="http://crescentluna.github.io/categories/%E5%AD%A6%E6%9C%AF/%E5%8F%98%E5%88%86%E6%8E%A8%E6%96%AD%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Variational Inference" scheme="http://crescentluna.github.io/tags/Variational-Inference/"/>
    
  </entry>
  
  <entry>
    <title>变分推断学习笔记(1)——概念介绍</title>
    <link href="http://crescentluna.github.io/2013/10/03/%E5%8F%98%E5%88%86%E6%8E%A8%E6%96%AD%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01%E2%80%94%E2%80%94%E6%A6%82%E5%BF%B5%E4%BB%8B%E7%BB%8D/"/>
    <id>http://crescentluna.github.io/2013/10/03/变分推断学习笔记1——概念介绍/</id>
    <published>2013-10-03T04:25:24.000Z</published>
    <updated>2017-02-21T18:24:27.407Z</updated>
    
    <content type="html"><![CDATA[<p>变分推断学习笔记系列：</p>
<ol style="list-style-type: decimal">
<li><a href="http://crescentluna.github.io/2013/10/03/%E5%8F%98%E5%88%86%E6%8E%A8%E6%96%AD%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01%E2%80%94%E2%80%94%E6%A6%82%E5%BF%B5%E4%BB%8B%E7%BB%8D/">变分推断学习笔记(1)——概念介绍</a></li>
<li><a href="http://crescentluna.github.io/2013/10/11/variational-inference-2/">变分推断学习笔记(2)——一维高斯模型的例子</a></li>
<li><a href="http://crescentluna.github.io/2014/12/12/variational-inference-3/">变分推断学习笔记(3)——三硬币问题的变分推断解法</a></li>
</ol>
<hr>

<h2 id="问题描述">问题描述</h2>
<p>变分推断是一类用于贝叶斯估计和机器学习领域中近似计算复杂（intractable）积分的技术，它广泛应用于各种复杂模型的推断。本文是学习PRML第10章的一篇笔记，错误或不足的地方敬请指出。</p>
<p>先给出问题描述。记得在上一篇EM的文章中，我们有一个观察变量<span class="math inline">\(\mathbf{X}=\{x^{\{1\}},\ldots,x^{\{m\}}\}\)</span>和隐藏变量<span class="math inline">\(\mathbf{Z}=\{z^{\{1\}},\ldots,z^{\{m\}}\}\)</span>, 整个模型<span class="math inline">\(p(\mathbf{X},\mathbf{Z})\)</span>是个关于变量<span class="math inline">\(\mathbf{X},\mathbf{Z}\)</span>的联合分布，我们的目标是得到后验分布<span class="math inline">\(P(\mathbf{Z}|\mathbf{X})\)</span>的一个近似分布。</p>
<p>在之前介绍过Gibbs Sampling这一类Monte Carlo算法，它们的做法就是通过抽取大量的样本估计真实的后验分布。而变分推断不同，与此不同的是，变分推断限制近似分布的类型，从而得到一种局部最优，但具有确定解的近似后验分布。</p>
之前<a href="http://www.crescentmoon.info/?p=171" target="_blank" rel="external">在EM算法的介绍中</a>我们有似然的式子如下： <span class="math display">\[\begin{equation}\ln p(\mathbf{X})=L(q)+KL(q||p)\end{equation}\]</span> 其中
<div>
<span class="math display">\[\begin{equation}L(q)=\int q(\mathbf{Z})\ln{\frac{p(\mathbf{X},\mathbf{Z})}{q(\mathbf{Z})}}d\mathbf{Z}\end{equation}\]</span>
</div>
<div>
<span class="math display">\[\begin{equation}KL(q||p)=-\int q(\mathbf{Z}) \ln{\frac{p(\mathbf{Z}|\mathbf{X})}{q(\mathbf{Z})}}d\mathbf{Z}\end{equation}\]</span>
</div>
<p>这里公式中不再出现参数<span class="math inline">\(\theta\)</span>，因为参数不再是固定的值，而变成了随机变量，所以也是隐藏变量，包括在<span class="math inline">\(\mathbf{Z}\)</span>之内了。</p>
<a id="more"></a>
<p>这里的KL散度<span class="math inline">\(KL(q||p)\)</span>描述了估计分布与真实分布的差别。当<span class="math inline">\(KL(q||p)=0\)</span>时，俩分布就是相同的。因为我们不知道真实的后验分布是啥，所以直接最小化KL是做不到的，但是我们可以通过最大化<span class="math inline">\(L(q)\)</span> 来达到这个目的。可以认为<span class="math inline">\(L(q)\)</span>越大，则模型对数据拟合程度越好。由于<span class="math inline">\(\ln p(X) \geq L (q)\)</span>始终成立，所以<span class="math inline">\(L (q)\)</span>被称作证据下界(evidence lower bound)，见图1。</p>
<p><a href="http://crescent.qiniudn.com/ELBO.png" target="_blank" rel="external"><img src="http://crescent.qiniudn.com/ELBO.png"></a></p>
<p>为了极大化<span class="math inline">\(L(q)\)</span>,我们需要选择合适的函数<span class="math inline">\(q\)</span>，使其便于计算。要注意到<span class="math inline">\(L(q)\)</span>并非普通的函数，而是以函数<span class="math inline">\(q\)</span>为自变量的函数，这就是泛函。泛函可以看成是函数概念的推广，而变分方法是处理泛函的数学领域，和处理函数的普通微积分相对。变分法最终寻求的是极值函数：它们使得泛函取得极大或极小值。</p>
<h2 id="条件独立假设">条件独立假设</h2>
如果参数之间具有相互依赖关系(mutually dependent)，求积分时会比较麻烦。所以我们为<span class="math inline">\(q(\mathbf{Z})\)</span>的分布加一个限制条件，将<span class="math inline">\(\mathbf{Z}\)</span>分为<span class="math inline">\(M\)</span>组变量，组与组之间变量相互独立，这样<span class="math inline">\(q\)</span>的分布就可以分解为
<div>
<span class="math display">\[\begin{equation}q(\mathbf{Z})=\prod^M_{i=1}q_i(\mathbf{Z}_i)\end{equation}\]</span>
</div>
<p>要注意我们对每个<span class="math inline">\(q(\mathbf{Z}_i)\)</span>的函数形式并没有做任何限制。这种将<span class="math inline">\(q\)</span>分解的方法叫做平均场理论(mean field theory)，主要基于基于系统中个体的局部相互作用可以产生宏观层面较为稳定的行为这个物理思想。</p>
<h2 id="求解过程">求解过程</h2>
<p>根据以上假设，我们来最大化下界<span class="math inline">\(L(q)\)</span>，因为假设<span class="math inline">\(q_i(\mathbf{Z}_i)\)</span>分布之间都是独立的，所以我们依次轮流来优化，以<span class="math inline">\(q_j(\mathbf{Z}_j)\)</span>为例(为了简单起见，缩写为<span class="math inline">\(q_j\)</span>)。</p>
<div>
<span class="math display">\[\begin{equation}
\begin{split}
L(q_j) &amp; =\int \prod_{i}q_j\{\ln p(\mathbf{X},\mathbf{Z})-\sum_i \ln q_i\} d\mathbf{Z} \\\
&amp;= \int q_j\{\int \ln p(\mathbf{X},\mathbf{Z})\prod_{i\neq j}q_i d\mathbf{Z}_i\}d\mathbf{Z}_j-\int q_j\ln q_j d\mathbf{Z}_j+const \\\
&amp;=\int q_j \ln \tilde{p}(\mathbf{X},\mathbf{Z}_j)d\mathbf{Z}_j-\int q_j\ln q_j d\mathbf{Z}_j +const \\
\end{split}
\end{equation}\]</span>
</div>
这里我们定义一个新分布<span class="math inline">\(\ln \tilde{p}(\mathbf{X},\mathbf{Z}_j)\)</span>为
<div>
<span class="math display">\[\begin{equation}\ln \tilde{p}(\mathbf{X},\mathbf{Z}_j)=\int \ln p(\mathbf{X},\mathbf{Z})\prod_{i\neq j}q_i d\mathbf{Z}_i\end{equation}\]</span>
</div>
我们发现它刚好是除去与<span class="math inline">\(q_j\)</span>分布相关的<span class="math inline">\(z_j\)</span>之后原似然的期望值，有
<div>
<span class="math display">\[\begin{equation}\ln \tilde{p}(\mathbf{X},\mathbf{Z}_j)=\mathbb{E}_{i\neq j}[\ln p(\mathbf{X},\mathbf{Z})]+const\end{equation}\]</span>
</div>
然后看式(5)的最后部分，就是<span class="math inline">\(q_j(\mathbf{Z}_j)\)</span>和<span class="math inline">\(\tilde{p}(\mathbf{X},\mathbf{Z}_j)\)</span>的KL散度的负值，这里我们固定<span class="math inline">\(q_{i\neq j}\)</span>不变，那么最大化<span class="math inline">\(L(q)\)</span>就变成了最小化这个KL散度，而KL 散度的最小值在<span class="math inline">\(q_j(\mathbf{Z}_j)=\ln \tilde{p}(\mathbf{X},\mathbf{Z}_j)\)</span>时取到。所以，最优解<span class="math inline">\(q^*_j(\mathbf{Z}_j)\)</span>为
<div>
<span class="math display">\[\begin{equation}\ln q^*_j(\mathbf{Z}_j)=\mathbb{E}_{i\neq j}[\ln p(\mathbf{X},\mathbf{Z})]+C\end{equation}\]</span>
</div>
<p>另加的这个C是为了归一化整个分布，有<span class="math inline">\(C=\int exp(\mathbb{E}_{i\neq j}[\ln p(\mathbf{X},\mathbf{Z})])d\mathbf{Z}_j\)</span> 。然后依次更新其他<span class="math inline">\(\mathbf{Z}_j\)</span>，最终相互迭代达到稳定。</p>
<h2 id="变分下界">变分下界</h2>
<p>我们也可以直接衡量模型的下界。在实际应用中，变分下界可以直接判断算法是否收敛，也可以通过每次迭代都不会降低这一点来判断算法推导和实现的部分是否存在问题。 <span class="math display">\[\begin{split}L(q)&amp; =\int q(\mathbf{Z})\ln{\frac{p(\mathbf{X},\mathbf{Z})}{p(\mathbf{Z})}}d\mathbf{Z} \\&amp; =\mathbf{E}_q[\ln p(\mathbf{X},\mathbf{Z})]-\mathbf{E}_q[\ln q(\mathbf{Z})] \\\end{split}\]</span> 值得一提的是，如果我们能知道变分后验每个因子的函数形式的话，我们还有另一种估计参数的方法，这个详见<a href="http://www.crescentmoon.info/?p=745" target="_blank" rel="external">例子</a>。</p>
<h2 id="变分推断和gibbs-sampling之间的联系">变分推断和Gibbs Sampling之间的联系</h2>
<p>变分推断和Gibbs sampling其实挺相似的：</p>
<ul>
<li>在Gibbs Sampling中，我们从条件分布<span class="math inline">\(P(\mathbf{Z}_j|\mathbf{Z}_{\neg j})\)</span>中抽样。</li>
<li>在变分推断中，我们迭代<span class="math inline">\(\mathbf{Z}_j\)</span>的分布<span class="math inline">\(Q(\mathbf{Z}_j)\propto\frac{1}{C}\exp\{\mathbb{E}_{i\neq j}[\ln p(\mathbf{X},\mathbf{Z})]\}\)</span></li>
</ul>
<h2 id="参考">参考</h2>
<ol style="list-style-type: decimal">
<li>《Pattern_Recognition_and_Machine_Learning》第10章</li>
<li>http://en.wikipedia.org/wiki/Variational_Bayesian_methods</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;变分推断学习笔记系列：&lt;/p&gt;
&lt;ol style=&quot;list-style-type: decimal&quot;&gt;
&lt;li&gt;&lt;a href=&quot;http://crescentluna.github.io/2013/10/03/%E5%8F%98%E5%88%86%E6%8E%A8%E6%96%AD%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01%E2%80%94%E2%80%94%E6%A6%82%E5%BF%B5%E4%BB%8B%E7%BB%8D/&quot;&gt;变分推断学习笔记(1)——概念介绍&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://crescentluna.github.io/2013/10/11/variational-inference-2/&quot;&gt;变分推断学习笔记(2)——一维高斯模型的例子&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://crescentluna.github.io/2014/12/12/variational-inference-3/&quot;&gt;变分推断学习笔记(3)——三硬币问题的变分推断解法&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;/hr&gt;
&lt;h2 id=&quot;问题描述&quot;&gt;问题描述&lt;/h2&gt;
&lt;p&gt;变分推断是一类用于贝叶斯估计和机器学习领域中近似计算复杂（intractable）积分的技术，它广泛应用于各种复杂模型的推断。本文是学习PRML第10章的一篇笔记，错误或不足的地方敬请指出。&lt;/p&gt;
&lt;p&gt;先给出问题描述。记得在上一篇EM的文章中，我们有一个观察变量&lt;span class=&quot;math inline&quot;&gt;\(\mathbf{X}=\{x^{\{1\}},\ldots,x^{\{m\}}\}\)&lt;/span&gt;和隐藏变量&lt;span class=&quot;math inline&quot;&gt;\(\mathbf{Z}=\{z^{\{1\}},\ldots,z^{\{m\}}\}\)&lt;/span&gt;, 整个模型&lt;span class=&quot;math inline&quot;&gt;\(p(\mathbf{X},\mathbf{Z})\)&lt;/span&gt;是个关于变量&lt;span class=&quot;math inline&quot;&gt;\(\mathbf{X},\mathbf{Z}\)&lt;/span&gt;的联合分布，我们的目标是得到后验分布&lt;span class=&quot;math inline&quot;&gt;\(P(\mathbf{Z}|\mathbf{X})\)&lt;/span&gt;的一个近似分布。&lt;/p&gt;
&lt;p&gt;在之前介绍过Gibbs Sampling这一类Monte Carlo算法，它们的做法就是通过抽取大量的样本估计真实的后验分布。而变分推断不同，与此不同的是，变分推断限制近似分布的类型，从而得到一种局部最优，但具有确定解的近似后验分布。&lt;/p&gt;
之前&lt;a href=&quot;http://www.crescentmoon.info/?p=171&quot;&gt;在EM算法的介绍中&lt;/a&gt;我们有似然的式子如下： &lt;span class=&quot;math display&quot;&gt;\[\begin{equation}\ln p(\mathbf{X})=L(q)+KL(q||p)\end{equation}\]&lt;/span&gt; 其中
&lt;div&gt;
&lt;span class=&quot;math display&quot;&gt;\[\begin{equation}L(q)=\int q(\mathbf{Z})\ln{\frac{p(\mathbf{X},\mathbf{Z})}{q(\mathbf{Z})}}d\mathbf{Z}\end{equation}\]&lt;/span&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;span class=&quot;math display&quot;&gt;\[\begin{equation}KL(q||p)=-\int q(\mathbf{Z}) \ln{\frac{p(\mathbf{Z}|\mathbf{X})}{q(\mathbf{Z})}}d\mathbf{Z}\end{equation}\]&lt;/span&gt;
&lt;/div&gt;
&lt;p&gt;这里公式中不再出现参数&lt;span class=&quot;math inline&quot;&gt;\(\theta\)&lt;/span&gt;，因为参数不再是固定的值，而变成了随机变量，所以也是隐藏变量，包括在&lt;span class=&quot;math inline&quot;&gt;\(\mathbf{Z}\)&lt;/span&gt;之内了。&lt;/p&gt;
    
    </summary>
    
      <category term="学术" scheme="http://crescentluna.github.io/categories/%E5%AD%A6%E6%9C%AF/"/>
    
      <category term="变分推断笔记" scheme="http://crescentluna.github.io/categories/%E5%AD%A6%E6%9C%AF/%E5%8F%98%E5%88%86%E6%8E%A8%E6%96%AD%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="PRML" scheme="http://crescentluna.github.io/tags/PRML/"/>
    
      <category term="Variational Inference" scheme="http://crescentluna.github.io/tags/Variational-Inference/"/>
    
  </entry>
  
  <entry>
    <title>背包问题学习笔记(2)-问法的变化</title>
    <link href="http://crescentluna.github.io/2013/08/08/bag-problem-2/"/>
    <id>http://crescentluna.github.io/2013/08/08/bag-problem-2/</id>
    <published>2013-08-08T06:03:10.000Z</published>
    <updated>2017-02-21T18:24:27.407Z</updated>
    
    <content type="html"><![CDATA[<p>所有问法均以01背包问题为例，问题如下： 有<span class="math inline">\(N\)</span>种物品和一个容量为<span class="math inline">\(V\)</span>的背包，每种物品都有无限件可用。放入第i种物品的费用是<span class="math inline">\(C_i\)</span>，价值是<span class="math inline">\(W_i\)</span>，求背包所装物体的价值最大。 基本代码：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#<span class="meta-keyword">include</span></span></div><div class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</div><div class="line"><span class="keyword">const</span> <span class="keyword">int</span> N=<span class="number">4</span>,V=<span class="number">10</span>;</div><div class="line"><span class="keyword">int</span> F[N+<span class="number">1</span>][V+<span class="number">1</span>];</div><div class="line"><span class="keyword">int</span> C[N+<span class="number">1</span>]=&#123;<span class="number">0</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">4</span>,<span class="number">5</span>&#125;;</div><div class="line"><span class="keyword">int</span> W[N+<span class="number">1</span>]=&#123;<span class="number">0</span>,<span class="number">4</span>,<span class="number">6</span>,<span class="number">6</span>,<span class="number">7</span>&#125;;</div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">BasicZeroOnePack</span><span class="params">()</span>  <span class="comment">//O(NV)</span></span></div><div class="line">&#123;</div><div class="line">	<span class="built_in">memset</span>(F,<span class="number">0</span>,<span class="keyword">sizeof</span>(F));</div><div class="line">	<span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>;i&lt;=N;i++)</div><div class="line">	&#123;</div><div class="line"></div><div class="line">		<span class="keyword">for</span>(<span class="keyword">int</span> v=<span class="number">0</span>;v&lt;=V;v++) 		</div><div class="line">		&#123;</div><div class="line">			F[i][v]=F[i<span class="number">-1</span>][v];  <span class="comment">//默认不加入物品 			</span></div><div class="line">			<span class="keyword">if</span>(v&gt;=C[i])  <span class="comment">//可以加入的时候考虑一下</span></div><div class="line">				F[i][v]=max(F[i<span class="number">-1</span>][v],F[i<span class="number">-1</span>][v-C[i]]+W[i]);</div><div class="line">		&#125;</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>改动都是基于这个代码。</p>
<h3 id="恰好装满的要求">恰好装满的要求</h3>
<p>如果要求恰好装满的话,初始化时除了<span class="math inline">\(F[0]=0\)</span>以外，其余的F[1,…V]均设为负无穷。因为如果要求恰好装满，初始化的时候只有容量为0的背包装容量为0的物体才是合法状态，其余的状态全都不合法，用负无穷表示。 <a id="more"></a></p>
<h3 id="输出方案">输出方案</h3>
<p>如果需要输出方案，我们只需逆推状态转移方程，确定这个状态是由哪一个策略所推出的，从而找到上一个状态即可。可以用<span class="math inline">\(G[i,v]\)</span>记录来自哪个状态，代码如下:</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">int</span> G[N+<span class="number">1</span>][V+<span class="number">1</span>];</div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">BasicZeroOnePackCase</span><span class="params">()</span><span class="comment">//O(NV)</span></span></div><div class="line">&#123;</div><div class="line">	<span class="built_in">memset</span>(F,<span class="number">0</span>,<span class="keyword">sizeof</span>(F));</div><div class="line">	<span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>;i&lt;=N;i++)</div><div class="line">	&#123;</div><div class="line">		<span class="keyword">for</span>(<span class="keyword">int</span> v=<span class="number">0</span>;v&lt;=V;v++) 		</div><div class="line">		&#123;</div><div class="line">			F[i][v]=F[i<span class="number">-1</span>][v];  <span class="comment">//默认不加入物品 			</span></div><div class="line">			G[i][v]=<span class="number">0</span>; 			</div><div class="line">			<span class="keyword">if</span>(v&gt;=C[i])  <span class="comment">//可以加入的时候考虑一下</span></div><div class="line">			&#123;</div><div class="line">				F[i][v]=max(F[i<span class="number">-1</span>][v],F[i<span class="number">-1</span>][v-C[i]]+W[i]);</div><div class="line">				<span class="keyword">if</span>(F[i<span class="number">-1</span>][v-C[i]]+W[i]&gt;F[i<span class="number">-1</span>][v])</div><div class="line">					G[i][v]=<span class="number">1</span>;</div><div class="line">			&#125;</div><div class="line">		&#125;</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>其实不记录也可以，我们可以直接根据<span class="math inline">\(F[i][v]\)</span>是否等于<span class="math inline">\(F[i-1][v-C[i]]+W[i]\)</span>的值判断得到。</p>
<h3 id="输出字典序最小">输出字典序最小</h3>
<p>所谓的”字典序最小“就是按字典排序方式，1到N号物体的选择方案排列最靠前。很明显的1肯定比2靠前，也就是说我们尽量要选择前面的物体，由于我们原先的子问题”1..i个物体,背包容量j“会分解为”1…,i-1，背包容量j“和”1…i-1,背包容量j-C[i]“两个子问题并不符合字典序的假设，因为即使第i个物体服从字典序，也无法确定前i-1个物体是否服从字典序，因为1…i-1这个子问题已经被处理过了。</p>
<p>于是我们把循环顺序倒过来，子问题变成了“i..1个物体,背包容量j”会分解为“i-1…,1，背包容量j”和“i-1…1,背包容量<span class="math inline">\(j-C[i]\)</span>”两个子问题，这样只要i做了服从字典序的选择，那我们只要去继续探究i-1…1这个子问题就好了。 所以我们把状态转移方程改成这样 <span class="math display">\[ F[i,v]=\max\{F[i+1][v],F[i+1][v-C_i]+W_i\}\]</span> 代码也只要把i逆序，i-1变成i+1即可。在输出的时候，如果<span class="math inline">\(F[i+1][v]==F[i+1][v-C_i]+W_i\)</span>相等，尽量选加入i的策略，因为物品i必定比i+1靠前。代码</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">BasicZeroOnePackDictionaryOrder</span><span class="params">()</span></span></div><div class="line">&#123;</div><div class="line">	<span class="built_in">memset</span>(F,<span class="number">0</span>,<span class="keyword">sizeof</span>(F));</div><div class="line">	<span class="keyword">for</span>(<span class="keyword">int</span> i=N;i&gt;=<span class="number">1</span>;i--)<span class="comment">//反向定义</span></div><div class="line">	&#123;</div><div class="line">		<span class="keyword">for</span>(<span class="keyword">int</span> v=<span class="number">0</span>;v&lt;=V;v++) 		</div><div class="line">		&#123;</div><div class="line">			F[i][v]=F[i+<span class="number">1</span>][v];  <span class="comment">//默认不加入物品 			</span></div><div class="line">			<span class="keyword">if</span>(v&gt;=C[i])  <span class="comment">//可以加入的时候考虑一下</span></div><div class="line">				F[i][v]=max(F[i+<span class="number">1</span>][v],F[i+<span class="number">1</span>][v-C[i]]+W[i]);</div><div class="line">		&#125;</div><div class="line">	&#125;</div><div class="line">	<span class="keyword">int</span> v=V;</div><div class="line">	<span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>;i&lt;=N;i++)</div><div class="line">	&#123;</div><div class="line">		<span class="keyword">if</span>(F[i][v]==F[i+<span class="number">1</span>][v-C[i]]+W[i])</div><div class="line">		&#123;</div><div class="line">			<span class="built_in">cout</span>&lt;&lt;i&lt;&lt;<span class="string">" "</span>;</div><div class="line">			v=v-C[i];</div><div class="line">		&#125;</div><div class="line">	&#125;</div><div class="line">	<span class="built_in">cout</span>&lt;&lt;<span class="built_in">endl</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="求方案总数">求方案总数</h3>
<p>求方案总数一般只要把状态转移方程中的max改成sum，子问题定义为“可行的方案数”即可，要注意初始条件。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">BasicZeroOnePackSum</span><span class="params">()</span></span></div><div class="line">&#123;</div><div class="line">	<span class="built_in">memset</span>(F,<span class="number">0</span>,<span class="keyword">sizeof</span>(F));</div><div class="line">	F[<span class="number">0</span>][<span class="number">0</span>]=<span class="number">1</span>;</div><div class="line">	<span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>;i&lt;=N;i++)</div><div class="line">	&#123;</div><div class="line">		<span class="keyword">for</span>(<span class="keyword">int</span> v=<span class="number">0</span>;v&lt;=V;v++) 		</div><div class="line">		&#123;</div><div class="line">			F[i][v]=F[i<span class="number">-1</span>][v];  <span class="comment">//默认不加入物品 			</span></div><div class="line">			<span class="keyword">if</span>(v&gt;=C[i])  <span class="comment">//可以加入的时候考虑一下</span></div><div class="line">				F[i][v]=sum(F[i<span class="number">-1</span>][v],F[i<span class="number">-1</span>][v-C[i]]);</div><div class="line">		&#125;</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="求最优方案总数">求最优方案总数</h3>
<p>最优方案就是总价值最大的方案。这个时候只要跟着原01背包的策略走，哪个策略好就用哪个策略的方案数，如果一样好就把2个策略的方案数相加即可。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">BasicZeroOnePackBestSum</span><span class="params">()</span></span></div><div class="line">&#123;</div><div class="line">	<span class="built_in">memset</span>(F,<span class="number">0</span>,<span class="keyword">sizeof</span>(F));</div><div class="line">	<span class="built_in">memset</span>(G,<span class="number">0</span>,<span class="keyword">sizeof</span>(G));</div><div class="line">	G[<span class="number">0</span>][<span class="number">0</span>]=<span class="number">1</span>;</div><div class="line">	<span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>;i&lt;=N;i++)</div><div class="line">	&#123;</div><div class="line">		<span class="keyword">for</span>(<span class="keyword">int</span> v=<span class="number">0</span>;v&lt;=V;v++) 		</div><div class="line">		&#123;</div><div class="line">			F[i][v]=F[i<span class="number">-1</span>][v];  <span class="comment">//默认不加入物品 			</span></div><div class="line">			G[i][v]=G[i<span class="number">-1</span>][v]; 			</div><div class="line">			<span class="keyword">if</span>(v&gt;=C[i])  <span class="comment">//可以加入的时候考虑一下</span></div><div class="line">			&#123;</div><div class="line">				F[i][v]=max(F[i<span class="number">-1</span>][v],F[i<span class="number">-1</span>][v-C[i]]+W[i]);</div><div class="line">				<span class="keyword">if</span>(F[i<span class="number">-1</span>][v-C[i]]+W[i]&gt;F[i<span class="number">-1</span>][v])</div><div class="line">				&#123;</div><div class="line">					G[i][v]=G[i<span class="number">-1</span>][v-C[i]];</div><div class="line">				&#125;</div><div class="line">				<span class="keyword">else</span> <span class="keyword">if</span>(F[i<span class="number">-1</span>][v-C[i]]+W[i]==F[i<span class="number">-1</span>][v])</div><div class="line">				&#123;</div><div class="line">					G[i][v]+=G[i<span class="number">-1</span>][v-C[i]];</div><div class="line">				&#125;</div><div class="line">			&#125;</div><div class="line">		&#125;</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="求第k优解">求第K优解</h3>
<p>基本思想是将每个状态表示成有序队列，将状态转移方程中的max转化成有序队列的合并。<span class="math inline">\(F[i,v]\)</span>这个有序队列是由<span class="math inline">\(F[i-1,v]\)</span>，<span class="math inline">\(F[i-1,v-C_i+W_i\)</span>两个有序队列合并得到的。这个两个有序队列各自有自己的前K个最优解，然后两者合并得到当前的最优的K个解。 代码如下，为了节省空间使用了一维的01背包解法。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div></pre></td><td class="code"><pre><div class="line"><span class="function">priority_queue <span class="title">mergeTopK</span><span class="params">(priority_queue a,priority_queue b,<span class="keyword">int</span> W,<span class="keyword">int</span> K)</span></span></div><div class="line">&#123;</div><div class="line">	priority_queue c;</div><div class="line">	<span class="keyword">while</span> (c.size()&lt;K) 	</div><div class="line">	&#123;</div><div class="line">		<span class="keyword">if</span>(a.top()&gt;b.top()+W)</div><div class="line">		&#123;</div><div class="line">			c.push(a.top());</div><div class="line">			a.pop();</div><div class="line">		&#125;</div><div class="line">		<span class="keyword">else</span></div><div class="line">		&#123;</div><div class="line">			c.push(b.top()+W);</div><div class="line">			b.pop();</div><div class="line">		&#125;</div><div class="line">	&#125;</div><div class="line">	<span class="keyword">return</span> c;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">ZeroOnePackTopK</span><span class="params">(<span class="keyword">int</span> K)</span></span></div><div class="line">&#123;</div><div class="line">	priority_queue FQueue[V+<span class="number">1</span>];</div><div class="line">	<span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;=V;i++)</div><div class="line">	&#123;</div><div class="line">		<span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">1</span>;j&lt;=K;j++)</div><div class="line">		&#123;</div><div class="line">			FQueue[i].push(<span class="number">0</span>);<span class="comment">//数组初始化为K个0</span></div><div class="line">		&#125;</div><div class="line">	&#125;</div><div class="line">	<span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>;i&lt;=N;i++) 	&#123; 		<span class="keyword">for</span>(<span class="keyword">int</span> v=V;v&gt;=C[i];v--)</div><div class="line">		&#123;</div><div class="line">			FQueue[v]=mergeTopK(FQueue[v],FQueue[v-C[i]],W[i],K);</div><div class="line">		&#125;</div><div class="line">	&#125;</div><div class="line">	<span class="keyword">while</span>(!FQueue[V].empty())<span class="comment">//输出最优的K个解</span></div><div class="line">	&#123;</div><div class="line">		<span class="built_in">cout</span>&lt;&lt;FQueue[V].top()&lt;&lt;<span class="string">" "</span>;</div><div class="line">		FQueue[V].pop();</div><div class="line">	&#125;</div><div class="line">	<span class="built_in">cout</span>&lt;&lt;<span class="built_in">endl</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>参考： 1.《背包问题9讲》，https://github.com/tianyicui/pack</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;所有问法均以01背包问题为例，问题如下： 有&lt;span class=&quot;math inline&quot;&gt;\(N\)&lt;/span&gt;种物品和一个容量为&lt;span class=&quot;math inline&quot;&gt;\(V\)&lt;/span&gt;的背包，每种物品都有无限件可用。放入第i种物品的费用是&lt;span class=&quot;math inline&quot;&gt;\(C_i\)&lt;/span&gt;，价值是&lt;span class=&quot;math inline&quot;&gt;\(W_i\)&lt;/span&gt;，求背包所装物体的价值最大。 基本代码：&lt;/p&gt;
&lt;figure class=&quot;highlight cpp&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;8&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;9&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;10&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;11&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;12&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;13&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;14&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;15&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;16&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;17&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;18&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;19&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;20&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;#&lt;span class=&quot;meta-keyword&quot;&gt;include&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;using&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;namespace&lt;/span&gt; &lt;span class=&quot;built_in&quot;&gt;std&lt;/span&gt;;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt; N=&lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;,V=&lt;span class=&quot;number&quot;&gt;10&lt;/span&gt;;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt; F[N+&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;][V+&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;];&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt; C[N+&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;]=&amp;#123;&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;5&lt;/span&gt;&amp;#125;;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt; W[N+&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;]=&amp;#123;&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;6&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;6&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;7&lt;/span&gt;&amp;#125;;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;BasicZeroOnePack&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;()&lt;/span&gt;  &lt;span class=&quot;comment&quot;&gt;//O(NV)&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;/span&gt;&amp;#123;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;	&lt;span class=&quot;built_in&quot;&gt;memset&lt;/span&gt;(F,&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;,&lt;span class=&quot;keyword&quot;&gt;sizeof&lt;/span&gt;(F));&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;	&lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt;(&lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt; i=&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;;i&amp;lt;=N;i++)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;	&amp;#123;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;		&lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt;(&lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt; v=&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;;v&amp;lt;=V;v++) 		&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;		&amp;#123;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;			F[i][v]=F[i&lt;span class=&quot;number&quot;&gt;-1&lt;/span&gt;][v];  &lt;span class=&quot;comment&quot;&gt;//默认不加入物品 			&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;			&lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt;(v&amp;gt;=C[i])  &lt;span class=&quot;comment&quot;&gt;//可以加入的时候考虑一下&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;				F[i][v]=max(F[i&lt;span class=&quot;number&quot;&gt;-1&lt;/span&gt;][v],F[i&lt;span class=&quot;number&quot;&gt;-1&lt;/span&gt;][v-C[i]]+W[i]);&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;		&amp;#125;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;	&amp;#125;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&amp;#125;&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;改动都是基于这个代码。&lt;/p&gt;
&lt;h3 id=&quot;恰好装满的要求&quot;&gt;恰好装满的要求&lt;/h3&gt;
&lt;p&gt;如果要求恰好装满的话,初始化时除了&lt;span class=&quot;math inline&quot;&gt;\(F[0]=0\)&lt;/span&gt;以外，其余的F[1,…V]均设为负无穷。因为如果要求恰好装满，初始化的时候只有容量为0的背包装容量为0的物体才是合法状态，其余的状态全都不合法，用负无穷表示。
    
    </summary>
    
      <category term="算法" scheme="http://crescentluna.github.io/categories/%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="DP" scheme="http://crescentluna.github.io/tags/DP/"/>
    
      <category term="背包问题" scheme="http://crescentluna.github.io/tags/%E8%83%8C%E5%8C%85%E9%97%AE%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>背包问题学习笔记(1)-基本的背包问题介绍</title>
    <link href="http://crescentluna.github.io/2013/08/05/bag-problem-1/"/>
    <id>http://crescentluna.github.io/2013/08/05/bag-problem-1/</id>
    <published>2013-08-05T03:30:28.000Z</published>
    <updated>2017-02-21T18:24:27.407Z</updated>
    
    <content type="html"><![CDATA[<p>背包问题是动态规划中非常经典的问题。这里仅仅是一份学习笔记和简单的代码实现，更详细的介绍请见参考1和2。</p>
<h2 id="背包问题">01背包问题</h2>
<h3 id="题目">题目</h3>
<p>有<span class="math inline">\(N\)</span>种物品和一个容量为<span class="math inline">\(V\)</span>的背包，每种物品都有无限件可用。放入第i种物品的费用是<span class="math inline">\(C_i\)</span>，价值是<span class="math inline">\(W_i\)</span>。求解：将哪些物品装入背包，可使这些物品的耗费的费用总和不超过背包容量，且价值总和最大。</p>
<h3 id="基本思路">基本思路</h3>
<p><span class="math inline">\(F[i,v]\)</span>代表前<span class="math inline">\(i\)</span>件物品放入容量为<span class="math inline">\(v\)</span>的背包可以获得的最大价值。 01背包的状态转移方程为: <span class="math display">\[F[i,v]=\max\{F[i-1][v],F[i-1][v-C_i]+W_i\}\]</span> 将“前<span class="math inline">\(i\)</span>件物品放入容量为<span class="math inline">\(v\)</span>的背包”中这个子问题，只考虑地<span class="math inline">\(i\)</span>件物品的策略(放或不放）。如果放入<span class="math inline">\(i\)</span>物品，问题转为“前i-1件物品放入容量为<span class="math inline">\(v-C_i\)</span> 的背包中“这个子问题的最大价值<span class="math inline">\(F[i-1,v-C_i]\)</span>加上<span class="math inline">\(W_i\)</span>。如果不放，问题转化为”前i-1件物品放入容量为<span class="math inline">\(v-C_i\)</span> 的背包中“,价值为<span class="math inline">\(F[i-1,v]\)</span>。</p>
<p>代码是 <a id="more"></a> <figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#<span class="meta-keyword">include</span></span></div><div class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</div><div class="line"><span class="keyword">const</span> <span class="keyword">int</span> N=<span class="number">3</span>,V=<span class="number">10</span>;<span class="comment">//物品数目N，背包容量V</span></div><div class="line"><span class="keyword">int</span> F[N+<span class="number">1</span>][V+<span class="number">1</span>];<span class="comment">//二维数组</span></div><div class="line"><span class="keyword">int</span> FOne[V+<span class="number">1</span>];<span class="comment">//一维数组</span></div><div class="line"><span class="keyword">int</span> C[N+<span class="number">1</span>]=&#123;<span class="number">0</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>&#125;;<span class="comment">//物品容量C</span></div><div class="line"><span class="keyword">int</span> W[N+<span class="number">1</span>]=&#123;<span class="number">0</span>,<span class="number">4</span>,<span class="number">6</span>,<span class="number">7</span>&#125;;<span class="comment">//物品价值W</span></div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">max</span><span class="params">(<span class="keyword">int</span> a,<span class="keyword">int</span> b)</span></span></div><div class="line">&#123;</div><div class="line">	<span class="keyword">if</span>(a&gt;=b)</div><div class="line">		<span class="keyword">return</span> a;</div><div class="line">	<span class="keyword">else</span></div><div class="line">		<span class="keyword">return</span> b;</div><div class="line">&#125;</div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">BasicZeroOnePack</span><span class="params">()</span><span class="comment">//O(NV)</span></span></div><div class="line">&#123;</div><div class="line">	<span class="built_in">memset</span>(F,<span class="number">0</span>,<span class="keyword">sizeof</span>(F));</div><div class="line">	<span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>;i&lt;=N;i++)</div><div class="line">	&#123;</div><div class="line"></div><div class="line">		<span class="keyword">for</span>(<span class="keyword">int</span> v=<span class="number">0</span>;v&lt;=V;v++)</div><div class="line">		&#123;</div><div class="line">			F[i][v]=F[i<span class="number">-1</span>][v];<span class="comment">//默认不加入物品，有可能物品空间太大背包放不下</span></div><div class="line">			<span class="keyword">if</span>(v&gt;=C[i])<span class="comment">//可以加入的时候考虑一下</span></div><div class="line">				F[i][v]=max(F[i<span class="number">-1</span>][v],F[i<span class="number">-1</span>][v-C[i]]+W[i]);</div><div class="line">		&#125;</div><div class="line">	&#125;</div></pre></td></tr></table></figure></p>
<p>注：由于后面代码有很多一样的地方，所以后面的就只列函数和不同的地方，数组定义和max函数定义什么就省略了。</p>
<h3 id="空间复杂度优化一维数组">空间复杂度优化———一维数组</h3>
<p>因为状态转移方程只用到了i-1的状态，i-1之前的状态都用不上。所以只要保证我们在算<span class="math inline">\(F[i,v]\)</span>时，能取得<span class="math inline">\(F[i-1,v]\)</span>和<span class="math inline">\(F[i-1,v-C_i]\)</span>的值即可。在只使用一维数组的情况下，只要保证在覆盖当前位置前，<span class="math inline">\(F[v]\)</span>保存的是i-1时刻的<span class="math inline">\(F[v]\)</span>的值就行，<span class="math inline">\(F[v-C_i]\)</span>也是类似。所以只要每次循环中以<span class="math inline">\(v \leftarrow V,\ldots,0\)</span> 的递减顺序计算更新<span class="math inline">\(F[v]\)</span>就可以了。 关键代码如下</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">ZeroOnePack</span><span class="params">()</span><span class="comment">//一维数组搞定，O(N)的空间</span></span></div><div class="line">&#123;</div><div class="line">	<span class="built_in">memset</span>(FOne,<span class="number">0</span>,<span class="keyword">sizeof</span>(FOne));</div><div class="line">	<span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>;i&lt;=N;i++)</div><div class="line">	&#123;</div><div class="line">		<span class="keyword">for</span>(<span class="keyword">int</span> v=V;v&gt;=C[i];v--)</div><div class="line">		&#123;</div><div class="line">			FOne[v]=max(FOne[v],FOne[v-C[i]]+W[i]);</div><div class="line">			<span class="comment">//cout&lt;&lt;i&lt;&lt;" "&lt;&lt;v&lt;&lt;" "&lt;&lt;F[0][v]&lt;&lt;endl;</span></div><div class="line">		&#125;</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>事实上，由于后面的问题可以分解为一维数组解01背包问题，所以将处理01背包问题中的一个物品的过程抽象出来，定义如下。</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">def ZeroOnePack(F,C,W)</div><div class="line">   for(v = V to C)</div><div class="line">      F[v]=max(F[v],F[v-C]+W)</div></pre></td></tr></table></figure>
<h2 id="完全背包问题">完全背包问题</h2>
<h3 id="题目-1">题目</h3>
<p>有N种物品和一个容量为V的背包，每种物品都有无限件可用。放入第i种物品的费用是<span class="math inline">\(C_i\)</span>，价值是<span class="math inline">\(W_i\)</span>。求解：将哪些物品装入背包，可使这些物品的耗费的费用总和不超过背包容量，且价值总和最大。</p>
<h3 id="基本思路-1">基本思路</h3>
<p>如果按01背包的思路，我们只要有状态转移矩阵 <span class="math display">\[F[i,v]=\max\{F[i-1][v-k C_i]+k W_i|0 \leq k \leq [v/C_i]\}\]</span> 这个问题有<span class="math inline">\(O(VN)\)</span>个状态要解，而且求每个状态的时间是<span class="math inline">\(O(\frac{v}{C_i})\)</span>，总的复杂度上界是<span class="math inline">\(O(NV\sum \frac{V}{C_i})\)</span>，还是比较大的。 代码如下:</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">BasicCompletePack</span><span class="params">()</span>  <span class="comment">//O(NVsum(V/C_i))的算法</span></span></div><div class="line">&#123;</div><div class="line">	<span class="built_in">memset</span>(F,<span class="number">0</span>,<span class="keyword">sizeof</span>(F));</div><div class="line">	<span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>;i&lt;=N;i++)</div><div class="line">	&#123;</div><div class="line">		<span class="keyword">for</span>(<span class="keyword">int</span> v=<span class="number">1</span>;v&lt;=V;v++)</div><div class="line">		&#123;</div><div class="line">			<span class="keyword">int</span> max=F[i<span class="number">-1</span>][v];</div><div class="line">			<span class="keyword">int</span> kmax=v/C[i];</div><div class="line">			F[i][v]=F[i<span class="number">-1</span>][v];  <span class="comment">//还是默认不加入物品,这是k=0的情况</span></div><div class="line">			<span class="keyword">if</span>(kmax&gt;=<span class="number">1</span>)<span class="comment">//v&gt;=c[i];</span></div><div class="line">			&#123;</div><div class="line">				<span class="keyword">for</span>(<span class="keyword">int</span> k=<span class="number">1</span>;k&lt;=kmax;k++)</div><div class="line">				&#123;</div><div class="line">					<span class="keyword">if</span>(F[i<span class="number">-1</span>][v-k*C[i]]+k*W[i]&gt;max)</div><div class="line">					&#123;</div><div class="line">						max=F[i<span class="number">-1</span>][v-k*C[i]]+k*W[i];</div><div class="line">					&#125;</div><div class="line">				&#125;</div><div class="line">				F[i][v]=max;</div><div class="line">			&#125;</div><div class="line">		&#125;</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="优化思路反向考虑">优化思路——————反向考虑</h3>
<p>我们的问题是求”前<span class="math inline">\(i\)</span>件物品放入容量为<span class="math inline">\(v\)</span>的背包“的最大价值，原先的循环是先确定前i-1个物品的最优策略，然后考虑加不加物品<span class="math inline">\(i\)</span>，来比较不同容量<span class="math inline">\(v\)</span>下背包的价值是否有变化，但是这里物品<span class="math inline">\(i\)</span>变成多个可选了，所以复杂度提高了。我们反过来想，先确定背包<span class="math inline">\(v-1\)</span>容量时的物品的最优策略，然后考虑<span class="math inline">\(v\)</span>容量下，对于随着可选物品<span class="math inline">\(i\)</span>的增多，考虑背包的最大价值是否有变化。 状态转移方程为 <span class="math display">\[F[i,v]=\max\{F[i-1][v],F[i][v-C_i]+W_i\}\]</span> 这里<span class="math inline">\(F[i-1][v]\)</span>是不选第<span class="math inline">\(i\)</span>个物品时，背包的最大价值，<span class="math inline">\(F[i][v-C_i]+W_i\)</span>是选了物品<span class="math inline">\(i\)</span>时，背包的最大价值。需要注意的是，在<span class="math inline">\(v\)</span>增长的过程中，物品<span class="math inline">\(i\)</span>有可能被选择多次，因为每轮<span class="math inline">\(v\)</span>的循环都要考虑一次物品<span class="math inline">\(i\)</span>。 代码如下：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">RerVerseCompletePack</span><span class="params">()</span></span></div><div class="line">&#123;</div><div class="line">	<span class="built_in">memset</span>(F,<span class="number">0</span>,<span class="keyword">sizeof</span>(F));</div><div class="line">	<span class="keyword">for</span>(<span class="keyword">int</span> v=<span class="number">1</span>;v&lt;=V;v++)</div><div class="line">	&#123;</div><div class="line">		<span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>;i&lt;=N;i++)</div><div class="line">		&#123;</div><div class="line">			<span class="keyword">if</span>(v&lt;C[i])</div><div class="line">			&#123;</div><div class="line">				F[i][v]=F[i<span class="number">-1</span>][v];<span class="comment">//如果物品i放不进去，选择前i-1中最好的结果</span></div><div class="line">			&#125;</div><div class="line">			<span class="keyword">else</span></div><div class="line">			&#123;</div><div class="line">				F[i][v]=max(F[i<span class="number">-1</span>][v],F[i][v-C[i]]+W[i]);</div><div class="line">			&#125;</div><div class="line">		&#125;</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="进一步优化一维数组">进一步优化——————一维数组</h3>
<p>直接先给出代码:</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">CompletePack</span><span class="params">()</span>  <span class="comment">//一维数组搞定，O(N)的空间</span></span></div><div class="line">&#123;</div><div class="line">	<span class="built_in">memset</span>(FOne,<span class="number">0</span>,<span class="keyword">sizeof</span>(FOne));</div><div class="line">	<span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>;i&lt;=N;i++)</div><div class="line">	&#123;</div><div class="line">		<span class="keyword">for</span>(<span class="keyword">int</span> v=C[i];v&lt;=V;v++)</div><div class="line">		&#123;</div><div class="line">			FOne[v]=max(FOne[v],FOne[v-C[i]]+W[i]);</div><div class="line">		&#125;</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这个算法的时间复杂度也是<span class="math inline">\(O(VN)\)</span>，但空间复杂度只有<span class="math inline">\(O(N)\)</span>。你会注意到，这个代码与01背包的优化算法只有<span class="math inline">\(v\)</span>的循环顺序不同而已。这个不同之处就是问题的关键。在<span class="math inline">\(v \leftarrow V,\ldots,0\)</span> 的递减顺序计算的时候，背包内除了当前加或不加的物品<span class="math inline">\(i\)</span>以外，其余的物品一定是前<span class="math inline">\(i-1\)</span>个之中的，这样保证了物品<span class="math inline">\(i\)</span>只会被处理一次。而完全背包问题中，物品<span class="math inline">\(i\)</span>是可以加无数次的，我们在对当前容量<span class="math inline">\(v\)</span>的背包做加不加物品<span class="math inline">\(i\)</span>的策略时，是根据有可能在之前较小的容量<span class="math inline">\(v\)</span>中已经选入第<span class="math inline">\(i\)</span>种物品的子结果<span class="math inline">\(F[i,v-C_i]\)</span>中得到的（而不是<span class="math inline">\(F[i-1,C_i]\)</span>)。 所以，写出来的状态转移方程是这 <span class="math display">\[F[v]=\max\{F[v],F[v-C_i]+W_i\}\]</span> 我们可以理解为 <span class="math display">\[F[i,v]=\max\{F[i-1][v],F[i][v-C_i]+W_i\}\]</span> 和优化思路1的状态转移方程一摸一样。 同样，我们将完全背包的过程抽象出来:</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">def CompletePack(F,C,W)</div><div class="line">   for(v = C to V)</div><div class="line">      F[v]=max(F[v],F[v-C]+W)</div></pre></td></tr></table></figure>
<h2 id="多重背包问题">多重背包问题</h2>
<h3 id="题目-2">题目</h3>
<p>有N种物品和一个容量为V的背包。第i种物品最多有<span class="math inline">\(M_i\)</span>件可用，每件耗费的空间是<span class="math inline">\(C_i\)</span>，价值是<span class="math inline">\(W_i\)</span>。求解将哪些物品装入背包可使这些物品的耗费的空间总和不超过背包容量，且价值总和最大。</p>
<h3 id="基本思路-2">基本思路</h3>
<p>基本思路和完全背包类似，就是将<span class="math inline">\(k\)</span>的上限改为<span class="math inline">\(M_i\)</span>即可，这里就不写了。</p>
<h3 id="优化算法转为01背包问题">优化算法———转为01背包问题</h3>
<p>如果我们直接把第<span class="math inline">\(i\)</span>种物品的看做对<span class="math inline">\(M_i\)</span>个01背包物品做选择的话，其复杂度其实并没有下降。</p>
<p>换一种拆分方式，我们希望把第<span class="math inline">\(i\)</span>中物品换成一些小的物品，不管第<span class="math inline">\(i\)</span> 种物品有几件，都可以由这些小物品的相互组合得到。现在考虑二进制的思想，把物品拆成<span class="math inline">\(2^k*C_i\)</span>,<span class="math inline">\(1,2,2^{k-1}\)</span>,价值为<span class="math inline">\(2^k*W_i\)</span>的物品。假设最大物品数量是<span class="math inline">\(M_i\)</span>，必定存在一个k,使得<span class="math inline">\(1+2+2^2+\ldots+2^{k-1} \leq M_i \leq 1+2+2^2+\ldots+2^k\)</span>。用<span class="math inline">\(k\)</span>位二进制最多能表示到<span class="math inline">\(\sum_{i=1}^k 2^{i-1}=2^k+1\)</span>这么大的数，而由上面的式子，肯定有<span class="math inline">\(M_i-2^k+1&lt;2^k\)</span>，所以用<span class="math inline">\(k\)</span>位二进制数就可以表示系数中的最大值<span class="math inline">\(M_i-2^k+1\)</span>。令系数为<span class="math inline">\(1+2+2^2+\ldots+2^{k-1},M_i-2^k+1\)</span>，<span class="math inline">\(k\)</span>是满足<span class="math inline">\(M_i-2^k+1&gt;0\)</span>的最小整数。</p>
<p>同样地，完全背包也可以使用这个方法，只不过上限变成了<span class="math inline">\([v/C_i]\)</span>而已。</p>
<p>给出代码如下：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">int</span> M[N+<span class="number">1</span>]=&#123;<span class="number">0</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">1</span>&#125;;<span class="comment">//多了一个物品数目M的限制</span></div><div class="line"><span class="keyword">int</span> CBinary[<span class="number">100</span>];<span class="comment">//二进制拆分完的新物品容量</span></div><div class="line"><span class="keyword">int</span> WBinary[<span class="number">100</span>];<span class="comment">//二进制拆分完的新物品价值</span></div><div class="line"><span class="keyword">int</span> FOneBinary[<span class="number">100</span>];<span class="comment">//二进制拆分完的一维数组</span></div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">MultiplePack</span><span class="params">()</span></span></div><div class="line">&#123;<span class="built_in">memset</span>(FOneBinary,<span class="number">0</span>,<span class="keyword">sizeof</span>(FOneBinary));</div><div class="line">	<span class="keyword">int</span> num=<span class="number">1</span>;<span class="comment">//记录新物品总数目</span></div><div class="line">	<span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>;i&lt;=N;i++)</div><div class="line">	&#123;</div><div class="line">		<span class="keyword">int</span> thisM=M[i];</div><div class="line">		<span class="keyword">if</span>(thisM&gt;V/C[i])<span class="comment">//如果太大的话</span></div><div class="line">		&#123;</div><div class="line">			thisM=V/C[i];</div><div class="line">		&#125;</div><div class="line">		<span class="keyword">for</span>(<span class="keyword">int</span> k=<span class="number">1</span>;k&lt;thisM;k=k*<span class="number">2</span>)<span class="comment">//0&lt;M-2^k+1&lt;2^(k+1)</span></div><div class="line">		&#123;</div><div class="line">			CBinary[num]=k*C[i];</div><div class="line">			WBinary[num]=k*W[i];</div><div class="line">			num++;</div><div class="line">			thisM=thisM-k;</div><div class="line">		&#125;</div><div class="line">		<span class="comment">//再加一遍</span></div><div class="line">		CBinary[num]=thisM*C[i];</div><div class="line">		WBinary[num]=thisM*W[i];</div><div class="line">		num++;</div><div class="line">	&#125;</div><div class="line">	<span class="comment">//拆分完毕，开始01背包</span></div><div class="line">	<span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>;i&lt;=num;i++)</div><div class="line">	&#123;</div><div class="line">		<span class="keyword">for</span>(<span class="keyword">int</span> v=V;v&gt;=CBinary[i];v--)</div><div class="line">		&#123;</div><div class="line">			FOneBinary[v]=max(FOneBinary[v],FOneBinary[v-CBinary[i]]+WBinary[i]);</div><div class="line">		&#125;</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>将整个过程抽象出来,得到</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">def MultiplePack(F,C,W,M)</div><div class="line">if M&gt;=[V/C]//如果自定义的上限超过了背包可以装的量，就变成完全背包问题了，可以做二进制拆分，也可以直接用完全背包的方法做。</div><div class="line">   CompletePack(F,C,W);</div><div class="line">   return;</div><div class="line">k=1;</div><div class="line">while(k&lt;M)</div><div class="line">   ZeroOnePack(KC,kW);</div><div class="line">   M=M-k;</div><div class="line">   k=k*2;</div><div class="line">ZeroOnePack(M*C,M*W）；//处理最后一个</div></pre></td></tr></table></figure>
<h3 id="多重背包衍生问题只求可行性的算法">多重背包衍生问题——————只求可行性的算法</h3>
<h4 id="问题">问题</h4>
<p>有N件物品和一个容量为V的背包。放入第i件物品最多有<span class="math inline">\(M_i\)</span>件可用，每件占用的容量是<span class="math inline">\(C_i\)</span>。求是否能刚好填满给定容量的背包。这个问题只求可行性，不考虑价值，有复杂度为<span class="math inline">\(O(VN)\)</span>的算法。</p>
<h4 id="基本思路-3">基本思路</h4>
<p>这个问题是有<span class="math inline">\(O(VN)\)</span>的解法的，它的思想是这样的，令<span class="math inline">\(F[i][j]\)</span>代表“用了前<span class="math inline">\(i\)</span>中物品刚好填满容量为<span class="math inline">\(j\)</span>的背包后，还留下的可用物品<span class="math inline">\(i\)</span>的个数”。如果<span class="math inline">\(F[i][j]=-1\)</span>说明这个状态不行，如果状态可行的话应该有<span class="math inline">\(0 \leq F[i][j]\leq M_i\)</span>。</p>
<p>首先，在循环到第<span class="math inline">\(i\)</span>个物品的时候，用前<span class="math inline">\(i-1\)</span>个物品能够填满容量为<span class="math inline">\(j\)</span>的背包的情况必须被记录下来，这时候只要我们不选<span class="math inline">\(i\)</span>，就可以填满容量为<span class="math inline">\(j\)</span>的背包了。（注意，不选<span class="math inline">\(i\)</span>的时候<span class="math inline">\(F[i][j]=M_i\)</span>）。当然，如果用前<span class="math inline">\(i-1\)</span>个物品能够填满容量为<span class="math inline">\(j\)</span>的背包，那么我们只要加上一个物品<span class="math inline">\(i\)</span>，就可以恰好填满容量为<span class="math inline">\(j+C[i]\)</span>的背包啦。</p>
<p>所以状态转移矩阵是这样的， <span class="math display">\[F[i,j]=\max{F[i][j],F[i][j-C[i]]-1}\]</span> 两种情况，一种情况是<span class="math inline">\(F[i,j]\)</span>是由<span class="math inline">\(F[i][j-C[i]]\)</span>得来，因为用了一个物品<span class="math inline">\(i\)</span>，所以值<span class="math inline">\(为F[i][j-C[i]]-1\)</span>,另一种情况是不使用物品<span class="math inline">\(i\)</span>，由<span class="math inline">\(F[i-1,j]\)</span> 直接得到，这时候值为<span class="math inline">\(M_i\)</span>。 给出代码： <figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">PossbleMutiplePack</span><span class="params">()</span></span></div><div class="line">&#123;</div><div class="line">	<span class="built_in">memset</span>(F,<span class="number">-1</span>,<span class="keyword">sizeof</span>(F));<span class="comment">//可行状态全设为-1</span></div><div class="line">	F[<span class="number">0</span>][<span class="number">0</span>]=<span class="number">0</span>;</div><div class="line">	<span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>;i&lt;=N;i++)</div><div class="line">	&#123;</div><div class="line">		<span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">0</span>;j&lt;=V;j++)</div><div class="line">		&#123;</div><div class="line">			<span class="keyword">if</span>(F[i<span class="number">-1</span>][j]&gt;=<span class="number">0</span>)</div><div class="line">			&#123;</div><div class="line">				F[i][j]=M[i];</div><div class="line">			&#125;</div><div class="line">			<span class="keyword">else</span></div><div class="line">				F[i][j]=<span class="number">-1</span>;</div><div class="line">		&#125;</div><div class="line">		<span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">0</span>;j&lt;=V-C[i];j++)</div><div class="line">		&#123;</div><div class="line">			<span class="keyword">if</span>(F[i][j]&gt;<span class="number">0</span>)</div><div class="line">			&#123;</div><div class="line">				F[i][j+C[i]]=max(F[i][j+C[i]],F[i][j]<span class="number">-1</span>);</div><div class="line">			&#125;</div><div class="line">			</div><div class="line">		&#125;</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>最后<span class="math inline">\(F[N][0,\ldots,V]\)</span>就是多重背包在各个容量上是否可行的答案。</p>
<h2 id="混合三种背包问题">混合三种背包问题</h2>
<h3 id="题目-3">题目</h3>
<p>如果将前面1、2、3中的三种背包问题混合起来。也就是说，有的物品只可以取一 次（01 背包），有的物品可以取无限次（完全背包），有的物品可以取的次数有一个上限 （多重背包）。应该怎么求解呢？</p>
<h3 id="解法">解法</h3>
<p>直接调用之前的三个过程</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> i=<span class="number">1</span> to N</div><div class="line">    <span class="keyword">if</span>(第i件物品属于<span class="number">01</span>背包)</div><div class="line">        ZeroOnePack(F,C[i],W[i]);</div><div class="line">    <span class="keyword">else</span> <span class="keyword">if</span>(第i件物品属于完全背包)</div><div class="line">        CompletePack(F,C[i],W[i]);</div><div class="line">    <span class="keyword">else</span> <span class="keyword">if</span>(第i件物品属于多重背包)</div><div class="line">        MutliplePack(F,C[i],W[i],M[i]);</div></pre></td></tr></table></figure>
<p>参考： 1.《背包问题9讲》，https://github.com/tianyicui/pack 2.师兄的博客，http://www.ahathinking.com/archives/118.html</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;背包问题是动态规划中非常经典的问题。这里仅仅是一份学习笔记和简单的代码实现，更详细的介绍请见参考1和2。&lt;/p&gt;
&lt;h2 id=&quot;背包问题&quot;&gt;01背包问题&lt;/h2&gt;
&lt;h3 id=&quot;题目&quot;&gt;题目&lt;/h3&gt;
&lt;p&gt;有&lt;span class=&quot;math inline&quot;&gt;\(N\)&lt;/span&gt;种物品和一个容量为&lt;span class=&quot;math inline&quot;&gt;\(V\)&lt;/span&gt;的背包，每种物品都有无限件可用。放入第i种物品的费用是&lt;span class=&quot;math inline&quot;&gt;\(C_i\)&lt;/span&gt;，价值是&lt;span class=&quot;math inline&quot;&gt;\(W_i\)&lt;/span&gt;。求解：将哪些物品装入背包，可使这些物品的耗费的费用总和不超过背包容量，且价值总和最大。&lt;/p&gt;
&lt;h3 id=&quot;基本思路&quot;&gt;基本思路&lt;/h3&gt;
&lt;p&gt;&lt;span class=&quot;math inline&quot;&gt;\(F[i,v]\)&lt;/span&gt;代表前&lt;span class=&quot;math inline&quot;&gt;\(i\)&lt;/span&gt;件物品放入容量为&lt;span class=&quot;math inline&quot;&gt;\(v\)&lt;/span&gt;的背包可以获得的最大价值。 01背包的状态转移方程为: &lt;span class=&quot;math display&quot;&gt;\[F[i,v]=\max\{F[i-1][v],F[i-1][v-C_i]+W_i\}\]&lt;/span&gt; 将“前&lt;span class=&quot;math inline&quot;&gt;\(i\)&lt;/span&gt;件物品放入容量为&lt;span class=&quot;math inline&quot;&gt;\(v\)&lt;/span&gt;的背包”中这个子问题，只考虑地&lt;span class=&quot;math inline&quot;&gt;\(i\)&lt;/span&gt;件物品的策略(放或不放）。如果放入&lt;span class=&quot;math inline&quot;&gt;\(i\)&lt;/span&gt;物品，问题转为“前i-1件物品放入容量为&lt;span class=&quot;math inline&quot;&gt;\(v-C_i\)&lt;/span&gt; 的背包中“这个子问题的最大价值&lt;span class=&quot;math inline&quot;&gt;\(F[i-1,v-C_i]\)&lt;/span&gt;加上&lt;span class=&quot;math inline&quot;&gt;\(W_i\)&lt;/span&gt;。如果不放，问题转化为”前i-1件物品放入容量为&lt;span class=&quot;math inline&quot;&gt;\(v-C_i\)&lt;/span&gt; 的背包中“,价值为&lt;span class=&quot;math inline&quot;&gt;\(F[i-1,v]\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;代码是
    
    </summary>
    
      <category term="算法" scheme="http://crescentluna.github.io/categories/%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="DP" scheme="http://crescentluna.github.io/tags/DP/"/>
    
      <category term="背包问题" scheme="http://crescentluna.github.io/tags/%E8%83%8C%E5%8C%85%E9%97%AE%E9%A2%98/"/>
    
      <category term="动态规划" scheme="http://crescentluna.github.io/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"/>
    
  </entry>
  
  <entry>
    <title>我是否吃遍了食堂的盘子？</title>
    <link href="http://crescentluna.github.io/2013/07/14/%E6%88%91%E6%98%AF%E5%90%A6%E5%90%83%E9%81%8D%E4%BA%86%E9%A3%9F%E5%A0%82%E7%9A%84%E7%9B%98%E5%AD%90%EF%BC%9F/"/>
    <id>http://crescentluna.github.io/2013/07/14/我是否吃遍了食堂的盘子？/</id>
    <published>2013-07-14T05:53:34.000Z</published>
    <updated>2017-02-21T18:24:27.407Z</updated>
    
    <content type="html"><![CDATA[<p>前一阵子突发奇想，想看看吃饭吃了这么久，是不是把食堂里的盘子都吃过一遍了。提出问题如下： 现存在1，2，3号个盘子，每次吃饭拿一个盘子，用完之后放回去，请问吃了5次饭之后三个盘子都被用过的概率是多少？如果是m个盘子，吃了n次饭所有的盘子都被用过的概率呢？</p>
<h2 id="求概率">求概率</h2>
<p>虽然我猜出了公式，但是还是不怎么懂。。。讲一下猜的过程：</p>
<ol style="list-style-type: decimal">
<li>3个盘子吃了5次饭的情况。 既然我要算所有盘子都被吃过的情况，那么只要排除掉盘子没吃过的情况就好了。首先考虑只用了2个盘子的情况，总够有<span class="math inline">\(C^2_3 \cdot 2^5\)</span>种。但是要注意到这里对2个盘子的穷举已经包括了只用了1个盘子的情形而且还冗余了，所以要减掉冗余的3种情况。 所以三个盘子都被用过的概率为：
<div>
<span class="math display">\[1-\frac{C^2_3 \cdot 2^5 -3}{3^5}=\frac{150}{243}\]</span>
</div></li>
<li>4个盘子吃了5次饭的情况。 这里我们首先考虑只用了3个盘子的情况，共有<span class="math inline">\(C^3_4 \cdot 3^5\)</span>种情况。但是对这3个盘子的情况的列举已经包括了只包含2个盘子的情况，所以要减去<span class="math inline">\(C^2_4 \cdot 2^5\)</span>种2个盘子的情况，但是要注意的是对这2个盘子的列举又已经包括了1个盘子的情况，所以要加上种只含一个盘子的情况<span class="math inline">\(C^1_4 \cdot 1^5\)</span>补回来。 所以三个盘子都被用过的概率为：
<div>
<span class="math display">\[ 1-\frac{C^3_4 \cdot 3^5-C^2_4 \cdot 2^5+C^1_4 \cdot 1^5}{4^5}=\frac{240}{1024} \]</span>
</div></li>
<li>m个盘子吃了n次饭的情况。 根据上面的减减加加的想法推断（完全是瞎猜好么），就可以得到m个盘子吃了n次饭的概率公式：
<div>
<span class="math display">\[ 1-\frac{C^{m-1}_n \cdot (m-1)^n-C^{m-2}_n \cdot (m-2)^n+... \pm C^1_n \cdot 1^n}{m^n} \]</span>
</div>
m个盘子都被用过的概率为： 随便代入一个例子。m=20,n=50得到的概率为：0.16417976964878256。用模拟的方法发现这个数是对的- -。</li>
</ol>
<h2 id="求期望">求期望</h2>
<p>虽然我们知道了具体的概率，还是希望有个期望值来告诉我们大概在吃几个盘子的时候你能够舔遍全食堂的盘子。根据《算法导论》5.4.2的说法，我们将吃到以前没吃过的盘子的时候定义为“命中”，然后我们要求为了吃完m个盘子需要吃的次数n。</p>
<p>命中次数可以用来讲n次吃饭划分为多个阶段。第i个阶段包括从第i-1次命中到第i次命中所吃的次数。第1阶段因为所以盘子都没被吃过，所以是必中的。第i阶段的每一次吃饭，都有i-1个盘子被吃过，m-i+1个盘子没被吃过，这样第i次命中的概率就是(m-i+1)/m。</p>
用<span class="math inline">\(n_i\)</span>表示第i阶段的吃饭次数。每个随机变量<span class="math inline">\(n_i\)</span>都服从概率为(m-i+1)/m的几何分布。根据几何分布的期望可以算得
<div>
<span class="math display">\[ E[n_i]=\frac{m}{m-i+1}\]</span>
</div>
由期望的线性性质，可以得到
<div>
<span class="math display">\[E[n]=E[\sum^m_{i=1}n_i]=\sum^m_{i=1}E[n_i]=\sum^m_{i=1}\frac{m}{m-i+1}=m \sum \frac{1}{m}=m(\ln m+O(1))\]</span>
</div>
<p>最后的式子是调和级数的上界，所以我们希望吃完所有的盘子大概需要<span class="math inline">\(m\ln m\)</span>次，这个问题也被称作赠卷收集者问题（coupon collector’s problem)。 如果食堂有1000 个盘子，我们大学4年估计是吃不完的。。。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;前一阵子突发奇想，想看看吃饭吃了这么久，是不是把食堂里的盘子都吃过一遍了。提出问题如下： 现存在1，2，3号个盘子，每次吃饭拿一个盘子，用完之后放回去，请问吃了5次饭之后三个盘子都被用过的概率是多少？如果是m个盘子，吃了n次饭所有的盘子都被用过的概率呢？&lt;/p&gt;
&lt;h2 i
    
    </summary>
    
      <category term="杂项" scheme="http://crescentluna.github.io/categories/%E6%9D%82%E9%A1%B9/"/>
    
    
      <category term="盘子" scheme="http://crescentluna.github.io/tags/%E7%9B%98%E5%AD%90/"/>
    
  </entry>
  
  <entry>
    <title>三硬币问题-一个EM算法和Gibbs Sampling的例子</title>
    <link href="http://crescentluna.github.io/2013/07/03/%E4%B8%89%E7%A1%AC%E5%B8%81%E9%97%AE%E9%A2%98-%E4%B8%80%E4%B8%AAEM%E7%AE%97%E6%B3%95%E5%92%8CGibbs%20Sampling%E7%9A%84%E4%BE%8B%E5%AD%90/"/>
    <id>http://crescentluna.github.io/2013/07/03/三硬币问题-一个EM算法和Gibbs Sampling的例子/</id>
    <published>2013-07-02T17:41:26.000Z</published>
    <updated>2017-02-21T18:24:27.407Z</updated>
    
    <content type="html"><![CDATA[<p>讲一个EM算法和Gibbs 抽样的小例子，用于加深理解（变分推断版本请见<a href="http://crescentluna.github.io/2014/12/12/variational-inference-3/">变分推断学习笔记(3)——三硬币问题的变分推断解法</a>）。</p>
<p>题目(引用自参考1）：假设有3枚硬币，分别记做A，B，C。这些硬币正面出现的概率分别是<span class="math inline">\(\pi\)</span>,<span class="math inline">\(p\)</span>和<span class="math inline">\(q\)</span>。进行如下掷硬币实验：先掷硬币A，根据其结果选出硬币B或C，正面选B，反面选硬币C；然后投掷选重中的硬币，出现正面记作1，反面记作0；独立地重复<span class="math inline">\(n\)</span>次（n=10)，结果为 <span class="math display">\[1111110000\]</span> 我们只能观察投掷硬币的结果，而不知其过程，估计这三个参数<span class="math inline">\(\pi\)</span>,<span class="math inline">\(p\)</span>和<span class="math inline">\(q\)</span>。</p>
<h1 id="em算法">EM算法</h1>
<p>可以看到投掷硬币时到底选择了B或者C是未知的。我们设隐藏变量Z 来指示来自于哪个硬币，<span class="math inline">\(Z=\{z_1,z_2,\ldots,z_n \}\)</span>，令<span class="math inline">\(\theta=\{\pi,p,q\}\)</span>，观察数据<span class="math inline">\(X=\{x_1,x_2,\ldots,x_n \}\)</span>。</p>
<p>写出生成一个硬币时的概率： <span class="math display">\[\begin{split}P(x|\theta) &amp; =\sum_z P(x,z|\theta)=\sum_z P(z|\pi)P(x|z,\theta) \\&amp; =\pi p^x (1-p)^{1-x}+(1-\pi)q^x(1-q)^{1-x} \\\end{split}\]</span> 有了一个硬币的概率，我们就可以写出所有观察数据的log似然函数： <span class="math display">\[L(\theta|X)=\log P(X|\theta)=\sum^n_{j=1}\log[\pi p^{x_j} (1-p)^{1-{x_j}}+(1-\pi)q^{x_j}(1-q)^{1-{x_j}}]\]</span> 然后求极大似然 <span class="math display">\[\hat{\theta}=\arg \max L(\theta|X)\]</span> 其中<span class="math inline">\(L(\theta|X)=\log P(X|\theta)=\log \sum_Z P(X,Z|\theta)\)</span>。因为log里面带着加和所以这个极大似然是求不出解析解的。 <a id="more"></a> 如果我们知道隐藏变量Z的话，求以下的似然会容易很多: <span class="math display">\[L(\theta|X,Z)=\log P(X,Z|\theta)\]</span> 但是隐藏变量Z的值谁也不知道，所以我们转用EM算法求它的后验概率期望<span class="math inline">\(E_{Z|X,\theta}(L(\theta|X,Z))\)</span>的最大值。</p>
<ul>
<li><p>E步：假设当前模型的参数为<span class="math inline">\(\pi,p,q\)</span>时，隐含变量来自于硬币B的后验概率 <span class="math display">\[\mu=P(Z|X,\theta)=\frac{\pi p^{x_i}(1-p)^{1-x_i}}{ \pi p^{x_i}(1-p)^{1-x_i}+(1-\pi)q^{x_i}(1-q)^{1-x_i}}\]</span> 那么隐含变量来自于硬币C的后验概率自然为<span class="math inline">\(1-\mu\)</span>。</p></li>
<li><p>M步： 先写出似然关于后验概率的期望，它是似然期望下界函数的最大值， <span class="math display">\[\begin{split}&amp;E_{Z|X,\theta}(L(\theta|X,Z))=\\&amp;\sum^n_{j=1}[ \mu \log{\frac{\pi p^{x_i}(1-p)^{1-x_i}}{\mu}}+(1-\mu) \log{\frac{(1-\pi)q^{x_i}(1-q)^{1-x_i}}{1-\mu}} ]\\\end{split}\]</span> 要注意这里把<span class="math inline">\(\mu\)</span>看做固定值。然后我们分别求偏导，获得参数<span class="math inline">\(\pi,p,q\)</span>的新估计值 <span class="math display">\[\begin{split}&amp; \pi=\frac{1}{n}\sum^n_{j=1}\mu_j \\&amp; p=\frac{\sum^n_{j=1}\mu_j x_j}{\sum^n_{j=1}\mu_j} \\&amp; q=\frac{\sum^n_{j=1}(1-\mu_j) x_j}{\sum^n_{j=1}(1-\mu_j)} \\\end{split}\]</span></p></li>
</ul>
<p>算法首先选取参数初始值<span class="math inline">\(\theta^{(0)}=\pi^{(0)},p^{(0)},q^{(0)}\)</span>，然后迭代到收敛为止。</p>
<h1 id="gibbs-sampling">Gibbs Sampling</h1>
<p>其实这道题用EM做就可以了，因为最近看了Gibbs Sampling，所以套一下看看，如果有错敬请指出~</p>
<p>先给出Gibbs sampling的抽象的应用步骤，由于目标是对latent varibles逐一sample得到complete data再算充分统计量。所以关键是求 <span class="math inline">\(P(Z_i|Z_{-i},O,\alpha)\)</span>：</p>
<ol style="list-style-type: decimal">
<li>根据Markov bucket性质写出latent variable与对应observed variable的joint分布: <span class="math inline">\(P(Z,O|\alpha)=f(Z,O,\theta,\alpha)\)</span></li>
<li>根据样本对立性和参数条件独立性对joint distribution进行分解</li>
<li>积分参数<span class="math inline">\(\theta\)</span>，得到仅包含超参和充分统计量的joint分布表达式:<span class="math inline">\(P(Z,O|\alpha)=f(Z,O,\alpha)\)</span></li>
<li>对单个latent variable, 计算full conditional概率，用来采样: <span class="math inline">\(P(Z_{i}|Z_{-i},O,\alpha)\)</span></li>
</ol>
<h2 id="建模过程">建模过程</h2>
<p>首先我们确定建模过程，画出概率图： <a href="http://7sbo5n.com1.z0.glb.clouddn.com/概率图.jpg" target="_blank" rel="external"><img src="http://7sbo5n.com1.z0.glb.clouddn.com/概率图.jpg" alt="概率图"></a> 可以看到硬币B还是硬币C的类标签服从贝努利分布 <span class="math display">\[Z_j \sim Bernoulli(\pi)\]</span> 然后硬币正反面的出现也服从贝努利分布 <span class="math display">\[W_{z_{j}k} \sim Bernoulli(\theta_{Z_j})\]</span> 其中<span class="math inline">\(\theta=\{p,q\}\)</span>。需要注意这里参数都变成了随机变量，都有先验分布。为了简单和无偏倚起见，Beta分布的先验参数均设为1, <span class="math inline">\(\alpha=&lt;1,1&gt;\)</span>，<span class="math inline">\(\beta=&lt;1,1&gt;\)</span>，这样<span class="math inline">\(p(\alpha)=1,p(\beta)=1\)</span>，在实际运算中就不考虑了。</p>
<h2 id="确定联合分布">确定联合分布</h2>
<p>然后写出联合分布来应该是（要注意这里的联合分布应该是似然与先验<span class="math inline">\(p(\theta)\)</span>的乘积，因为先验<span class="math inline">\(p(\alpha)=1,p(\beta)=1\)</span>所以省略了后项)</p>
<p><span class="math display">\[
\begin{split}P(C,Z,\pi,p,q)&amp;=p(W|Z,\Theta)p(\Theta|\gamma_{\theta})p(Z|\pi)p(\pi|\gamma_{\pi})\\
&amp;= \prod^N_{i=1} \pi^{z_{i}} (1-\pi)^{1-z_{i}} p^{W_{z_{i0}}} (1-p)^{W_{z_{i1}}} q^{W_{(1-z_{i})0}} (1-q)^{W_{(1-z_{i})1}} \\
&amp; =\pi^{C_0} (1-\pi)^{C_1} p^{N_{C_0}(0)} (1-p)^{N_{C_0}(1)} q^{N_{C_1}(0)} (1-q)^{N_{C_1}(1)} \\ 
\end{split}
\]</span></p>
<p>其中<span class="math inline">\(z_i=0\)</span>时选中硬币B，<span class="math inline">\(z_i=1\)</span>时选中硬币C，<span class="math inline">\(W_{xj}\)</span>是观察到的硬币B或硬币C的正反结果，正面为<span class="math inline">\(W_{x0}=1,W_{x1}=0\)</span>，反面为<span class="math inline">\(W_{x1}=1,W_{x0}=0\)</span> ，<span class="math inline">\(C_0\)</span>是所有结果中选中硬币B的次数，<span class="math inline">\(C_1\)</span>是选中硬币C的次数，<span class="math inline">\(N_{C_0}(0)\)</span>是选B 的硬币中正面的硬币个数，<span class="math inline">\(N_{C_0}(1)\)</span> 是选B的硬币中反面的硬币个数，<span class="math inline">\(N_{C_1}(0)\)</span>是选C的硬币中正面的硬币个数，<span class="math inline">\(N_{C_1}(1)\)</span>是选C的硬币中反面的硬币个数。</p>
<h2 id="standard-gibbs-sampling">Standard Gibbs Sampling</h2>
<h3 id="对类标签z采样">对类标签Z采样</h3>
<p>然后根据Gibbs Sampling对隐藏变量Z采样，首先，我们设<span class="math inline">\({Z}^{-j}\)</span>是除了<span class="math inline">\(Z_j\)</span>外所有的硬币标签，然后<span class="math inline">\(C^{(-j)}\)</span>是除了<span class="math inline">\(W_j\)</span>外所有的硬币结果集合。硬币A的投掷有2个结果，<span class="math inline">\(Z_j=0\)</span>（硬币B）或<span class="math inline">\(Z_j=1\)</span>（硬币C）。如果<span class="math inline">\(Z_j=0\)</span>，那么<span class="math inline">\(C_0^{-j}=C_0-1,C_1^{-j}=C_1\)</span>。反之如果文档<span class="math inline">\(Z_j=1\)</span>，那么<span class="math inline">\(C_1^{-j}=C_1-1,C_0^{-j}=C_0\)</span>。</p>
<p>我们构建Gibbs Sampler，采样公式为： <span class="math display">\[\begin{split}P(Z_j=0|Z^{-j},C^{-j},\pi,p,q) &amp;=\frac{P(Z,C,\pi,p,q)}{P(Z^{-j},C^{-j},\pi,p,q)} \\&amp;=\pi p^{W_{j0}} (1-p)^{W_{j1}}\\P(Z_j=1|Z^{-j},C^{-j},\pi,p,q) &amp;=\frac{P(Z,C,\pi,p,q)}{P(Z^{-j},C^{-j},\pi,p,q)} \\&amp;=(1-\pi) q^{W_{j0}} (1-q)^{W_{j1}}\end{split}\]</span></p>
<p>然后根据这两个概率对<span class="math inline">\(Z_j^{(t+1)}\)</span>做贝努利实验，得到新采样的结果。</p>
<h3 id="对剩下参数的采样">对剩下参数的采样</h3>
<p>在对<span class="math inline">\(Z\)</span>采样获得了所有硬币的正反面之后，我们接下来要估计<span class="math inline">\(\pi,p,q\)</span>。</p>
<p>Gibbs Sampler的公式分别为: <span class="math display">\[\begin{split}&amp;P(\pi|Z,C,p,q)=\pi^{C_0} (1-\pi)^{C_1}\\&amp;P(p|Z,C,\pi,q)=p^{N_{C_0}(0)} (1-p)^{N_{C_0}(1)}\\&amp;P(q|Z,C,\pi,p)=q^{N_{C_1}(0)} (1-q)^{N_{C_1}(1)}\\ \end{split}\]</span> 这里<span class="math inline">\(\pi,p,q\)</span>均服从贝努利分布，根据Beta-Bernoulli共轭(之前先验分布为Beta(1,1))，我们可以把他们看成后验的Beta分布 <span class="math display">\[\begin{split}&amp;\pi \sim Beta(1+C_0,1+C_1)\\&amp;p \sim Beta(1+N_{C_0}(0),1+N_{C_0}(1))\\&amp;q \sim Beta(1+N_{C_1}(0),1+N_{C_1}(1))\\\end{split}\]</span> 然后从各自的Beta分布中采样出<span class="math inline">\(\pi,p,q\)</span>的值，详见<a href="http://www.crescentmoon.info/?p=525" target="_blank" rel="external">这里</a>的求<span class="math inline">\(\theta\)</span>部分。</p>
<h2 id="collapsed-gibbs-sampling">Collapsed Gibbs Sampling</h2>
<h3 id="对变量积分">对变量积分</h3>
<p>根据《Gibbs Sampling for the UniniTiated》上所说,<span class="math inline">\(\pi\)</span>可以被积分掉来简化联合分布，这种方法被称作“Collapsed Gibbs Sampling”,指通过积分避开了实际待估计的参数，转而对每个硬币的正反面<span class="math inline">\(z\)</span>进行采样，一旦每个硬币的z确定下来，被积分的值可以在统计频次后计算出来。正巧这里<span class="math inline">\(\theta=\{p,q\}\)</span>也只生成一个样本，可以被积分，就统统给积分好了。 <span class="math display">\[\begin{split}&amp;P(C,Z)= \int\int\int P(C,Z,\pi,p,q) d \pi d p d q \\&amp;=\int\int\int \pi^{C_0} (1-\pi)^{C_1} p^{N_{C_0}(0)} (1-p)^{N_{C_0}(1)} q^{N_{C_1}(0)} (1-q)^{N_{C_1}(1)} d \pi d p d q \\&amp;=\int \pi^{C_0} (1-\pi)^{C_1}d \pi \int p^{N_{C_0}(0)} (1-p)^{N_{C_0}(1)} dp \int q^{N_{C_1}(0)} (1-q)^{N_{C_1}(1)} \\&amp;=\frac{\Gamma(C_0+1)\Gamma(C_1+1)}{\Gamma(C_0+C_1+2)}\frac{\Gamma(N_{C_0}(0)+1)\Gamma(N_{C_0}(1)+1)}{\Gamma(C_0+2)}\frac{\Gamma(N_{C_1}(0)+1)\Gamma(N_{C_1}(1)+1)}{\Gamma(C_1+2)}\end{split}\]</span> 最后一步是根据Beta分布的积分公式得出的。要注意这个式子里还是存在未知数的<span class="math inline">\(Z\)</span>的，只不过被加和掩盖了而已。</p>
<h3 id="对类标签z采样-1">对类标签Z采样</h3>
<p>在这里采样公式就变成了 <span class="math display">\[\begin{split}&amp;P(Z_j=0|Z^{-j},C^{-j})=\frac{C_0}{C_0+C_1+1}(\frac{N_{C_0}(0)}{C_0+1})^{W_{j0}}(\frac{N_{C_1}(0)}{C_1+1})^{W_{j1}}\\&amp;P(Z_j=1|Z^{-j},C^{-j})=\frac{C_1}{C_0+C_1+1}(\frac{N_{C_0}(1)}{C_0+1})^{W_{j0}}(\frac{N_{C_1}(1)}{C_1+1})^{W_{j1}}\\ \end{split}\]</span> 根据这两个概率对<span class="math inline">\(Z_j^{(t+1)}\)</span>做贝努利实验，得到新采样的结果。</p>
<p>在<span class="math inline">\(Z\)</span>的一轮迭代结束后我们还要估计被积分了的变量的值，已知后验概率为 <span class="math display">\[\begin{split}&amp;p(\pi|Z)=\prod^n_{i=1}p(z|\pi)p(\pi|r_\pi)=Beta(\pi|1+C_0,1+C_1)\\&amp;p(p|Z)=\prod^n_{i=1}p(z|p)p(p|r_\theta)=Beta(\pi|1+N_{C_0}(0),1+N_{C_0}(1))\\&amp;p(q|Z)=\prod^n_{i=1}p(z|q)p(q|r_\theta)=Beta(\pi|1+N_{C_1}(0),1+N_{C_1}(1))\\ \end{split}\]</span> 根据贝叶斯推断的话，我们应该求整个参数分布上的期望，所以有 <span class="math display">\[E(\pi)=\int \pi \cdot \pi^{C_0+1-1}(1-\pi)^{C_1+1-1} d \pi=\frac{C_0+1}{C_0+C_1+2}\]</span> 这里还是利用Beta分布的积分公式，同理剩下的俩为 <span class="math display">\[\begin{split}&amp;E(p)=\frac{N_{C_0}(0)+1}{C_0+2}\\&amp;E(q)=\frac{N_{C_1}(0)+1}{C_1+2}\\ \end{split}\]</span> 这里可以看到先验知识（分子式上面的1和分母的2）和观察到的数据很好的结合了起来。</p>
<h1 id="em算法和sampling算法见prml的11.1.6和参考3">EM算法和Sampling算法（见PRML的11.1.6和参考3)</h1>
<h2 id="两者的联系">两者的联系</h2>
<p>两者都常用来估计latent variable分布的参数。主要思想一致：先估计生成一些latent variable，然后看成complete data，算参数。 区别：</p>
<ul>
<li><p>EM直接估计所有latent varibles的联合概率;</p></li>
<li><p>Gibbs估计单个单个latent variable的条件概率;</p></li>
</ul>
<p>所以，EM类似梯度下降，而Gibbs类似于coordinate gradient decent(不是conjugate)。</p>
<h2 id="monte-carlo-em算法">Monte Carlo EM算法</h2>
<p>EM算法中的E步估计所有latent varible的联合概率分布，可能很复杂。所以可以用抽样的方法来估计，以下是似然函数的期望表达式： <span class="math display">\[Q(\theta,\theta^{old})=\int p(Z|X,\theta^{old})\ln p(Z,X|\theta)d Z\]</span> 我们可以在后验概率<span class="math inline">\(p(Z|X,\theta^{old})\)</span>的分布上进行有限次数的抽样来接近这个期望： <span class="math display">\[Q(\theta,\theta^{old}) \simeq \frac{1}{L}\sum^L_{l=1}\ln p(Z^{(l)},X|\theta)\]</span> 然后像往常一样在M步优化Q函数，这个方法叫做Monte Carlo EM算法。</p>
<p>而每次在E步只生成一个样本的方法叫做stochastic EM，是Monte Carlo EM 算法的一个特例。在E步，我们从后验概率<span class="math inline">\(p(Z|X,\theta^{old})\)</span>中抽取一个样本，用来近似计算似然的期望，然后在M步更新模型参数的值。</p>
<p>刚才介绍的Collapsed Gibbs Sampling的方法就是stochastic EM的一个例子，只不过，M步骤的参数估计结果在E步骤中没有用到，所以不需要重复多余的M步骤，只需在最后进行一次M步骤，得到所需要的参数即可。</p>
<h2 id="参考文献">参考文献：</h2>
<ol style="list-style-type: decimal">
<li>《统计学习方法》</li>
<li>《Pattern Recognition and Machine Learning》</li>
<li>http://xhyan.wordpress.com/2012/04/30/%E3%80%90todo%E3%80%91quick-derivation-in-gibbs-sampling/</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;讲一个EM算法和Gibbs 抽样的小例子，用于加深理解（变分推断版本请见&lt;a href=&quot;http://crescentluna.github.io/2014/12/12/variational-inference-3/&quot;&gt;变分推断学习笔记(3)——三硬币问题的变分推断解法&lt;/a&gt;）。&lt;/p&gt;
&lt;p&gt;题目(引用自参考1）：假设有3枚硬币，分别记做A，B，C。这些硬币正面出现的概率分别是&lt;span class=&quot;math inline&quot;&gt;\(\pi\)&lt;/span&gt;,&lt;span class=&quot;math inline&quot;&gt;\(p\)&lt;/span&gt;和&lt;span class=&quot;math inline&quot;&gt;\(q\)&lt;/span&gt;。进行如下掷硬币实验：先掷硬币A，根据其结果选出硬币B或C，正面选B，反面选硬币C；然后投掷选重中的硬币，出现正面记作1，反面记作0；独立地重复&lt;span class=&quot;math inline&quot;&gt;\(n\)&lt;/span&gt;次（n=10)，结果为 &lt;span class=&quot;math display&quot;&gt;\[1111110000\]&lt;/span&gt; 我们只能观察投掷硬币的结果，而不知其过程，估计这三个参数&lt;span class=&quot;math inline&quot;&gt;\(\pi\)&lt;/span&gt;,&lt;span class=&quot;math inline&quot;&gt;\(p\)&lt;/span&gt;和&lt;span class=&quot;math inline&quot;&gt;\(q\)&lt;/span&gt;。&lt;/p&gt;
&lt;h1 id=&quot;em算法&quot;&gt;EM算法&lt;/h1&gt;
&lt;p&gt;可以看到投掷硬币时到底选择了B或者C是未知的。我们设隐藏变量Z 来指示来自于哪个硬币，&lt;span class=&quot;math inline&quot;&gt;\(Z=\{z_1,z_2,\ldots,z_n \}\)&lt;/span&gt;，令&lt;span class=&quot;math inline&quot;&gt;\(\theta=\{\pi,p,q\}\)&lt;/span&gt;，观察数据&lt;span class=&quot;math inline&quot;&gt;\(X=\{x_1,x_2,\ldots,x_n \}\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;写出生成一个硬币时的概率： &lt;span class=&quot;math display&quot;&gt;\[\begin{split}P(x|\theta) &amp;amp; =\sum_z P(x,z|\theta)=\sum_z P(z|\pi)P(x|z,\theta) \\&amp;amp; =\pi p^x (1-p)^{1-x}+(1-\pi)q^x(1-q)^{1-x} \\\end{split}\]&lt;/span&gt; 有了一个硬币的概率，我们就可以写出所有观察数据的log似然函数： &lt;span class=&quot;math display&quot;&gt;\[L(\theta|X)=\log P(X|\theta)=\sum^n_{j=1}\log[\pi p^{x_j} (1-p)^{1-{x_j}}+(1-\pi)q^{x_j}(1-q)^{1-{x_j}}]\]&lt;/span&gt; 然后求极大似然 &lt;span class=&quot;math display&quot;&gt;\[\hat{\theta}=\arg \max L(\theta|X)\]&lt;/span&gt; 其中&lt;span class=&quot;math inline&quot;&gt;\(L(\theta|X)=\log P(X|\theta)=\log \sum_Z P(X,Z|\theta)\)&lt;/span&gt;。因为log里面带着加和所以这个极大似然是求不出解析解的。
    
    </summary>
    
      <category term="学术" scheme="http://crescentluna.github.io/categories/%E5%AD%A6%E6%9C%AF/"/>
    
    
      <category term="EM" scheme="http://crescentluna.github.io/tags/EM/"/>
    
      <category term="Gibbs Sampling" scheme="http://crescentluna.github.io/tags/Gibbs-Sampling/"/>
    
  </entry>
  
  <entry>
    <title>《Gibbs Sampling for the UniniTiated》阅读笔记(下)---连续型参数求积分的思考</title>
    <link href="http://crescentluna.github.io/2013/06/29/Gibbs%20Sampling%20for%20the%20UniniTiated-3/"/>
    <id>http://crescentluna.github.io/2013/06/29/Gibbs Sampling for the UniniTiated-3/</id>
    <published>2013-06-29T04:59:37.000Z</published>
    <updated>2017-02-21T18:24:27.407Z</updated>
    
    <content type="html"><![CDATA[<p>《Gibbs Sampling for the UniniTiated》阅读笔记结构：</p>
<ol style="list-style-type: decimal">
<li> <a href="http://www.crescentmoon.info/?p=504" target="_blank" rel="external">参数估计方法及Gibbs Sampling简介</a></li>
<li><a href="http://www.crescentmoon.info/?p=525" target="_blank" rel="external">一个朴素贝叶斯文档模型例子</a><br>
</li>
<li><a href="http://www.crescentmoon.info/?p=548" target="_blank" rel="external">连续型参数求积分的思考</a>
<hr>
</li>
</ol>
<p>这篇是下篇，讨论中篇联合分布中对参数求积分来简化的问题。</p>
<p>之前存在的一个问题就是为啥我们可以对连续参数<span class="math inline">\(\pi\)</span>求积分消去它，而不能对词分布<span class="math inline">\(\theta_0\)</span>和<span class="math inline">\(\theta_1\)</span>求积分。这个主意看上去很美，但是实际做的时候，你会碰到一大把无法约掉的伽马函数。让我们看看具体的过程。</p>
<a id="more"></a>
<p>首先明确的是，我们的目标是求得从这个文档所在的类<span class="math inline">\(C_x\)</span>（除去该文档）中生成的<span class="math inline">\(\theta\)</span>分布中生成这篇文档的所有词的概率（真拗口啊- -）。我们先做个简单的版本，先求生成这篇文档中的一个词的概率，我们令<span class="math inline">\(w_k\)</span> 为文档<span class="math inline">\(W_j\)</span>中的<span class="math inline">\(k\)</span>位置的词，然后为了简单起见，省略以下公式所有的统计量上都存在的<span class="math inline">\((-j)\)</span>上标： <span class="math display">\[\begin{split}&amp; P(w_k=y|\mathbb{C}_x^{(-j)}; \gamma_\theta) \\&amp; =\int_\Delta P(w_k=y|\theta)P(\theta|\mathbb{C}_x^{(-j)}; \gamma_\theta) d\theta \\&amp;=\int_\Delta\theta_{x,y}\frac{\Gamma{(\sum^V_{i=1}\mathcal{N}_{\mathbb{C}_x}(i)+\gamma_{\theta_i})}}{\prod^V_{i=1}\Gamma{(\mathcal{N}_{\mathbb{C}_x}(i)+\gamma_{\theta_i})}}\prod^V_{i=1}\theta_{x,i}^{\mathcal{N}_{\mathbb{C}_x}(i)+\gamma_{\theta_i}-1}d\theta\\&amp;= \frac{\Gamma{(\sum^V_{i=1}\mathcal{N}_{\mathbb{C}_x}(i)+\gamma_{\theta_i})}}{\prod^V_{i=1}\Gamma{(\mathcal{N}_{\mathbb{C}_x}(i)+\gamma_{\theta_i})}} \int_\Delta\theta_{x,y} \prod^V_{i=1}\theta_{x,i}^{\mathcal{N}_{\mathbb{C}_x}(i)+\gamma_{\theta_i}-1}d\theta\\&amp;= \frac{\Gamma{(\sum^V_{i=1}\mathcal{N}_{\mathbb{C}_x}(i)+\gamma_{\theta_i})}}{\prod^V_{i=1}\Gamma{(\mathcal{N}_{\mathbb{C}_x}(i)+\gamma_{\theta_i})}} \int_\Delta\theta_{x,y}^{\mathcal{N}_{\mathbb{C}_x}(y)+\gamma_{\theta_y}}\prod^V_{i=1\wedge i\neq y}\theta_{x,i}^{\mathcal{N}_{\mathbb{C}_x}(i)+\gamma_{\theta_i}-1}d\theta\\&amp;= \frac{\Gamma{(\sum^V_{i=1}\mathcal{N}_{\mathbb{C}_x}(i)+\gamma_{\theta_i})}}{\prod^V_{i=1}\Gamma{(\mathcal{N}_{\mathbb{C}_x}(i)+\gamma_{\theta_i})}} \frac{\Gamma{(\mathcal{N}_{\mathbb{C}_y}(i)+\gamma_{\theta_y}+1)\prod^V_{i=1\wedge i\neq y}\Gamma{(\mathcal{N}_{\mathbb{C}_x}(i)+\gamma_{\theta_i})}} }{\Gamma(\sum^V_{i=1}\mathcal{N}_{\mathbb{C}_x}(i)+\gamma_{\theta_i}+1)}\\&amp;=\frac{\mathcal{N}_{\mathbb{C}_x}(y)+\gamma_{\theta_y}}{\prod^V_{i=1}\mathcal{N}_{\mathbb{C}_x}(i)+\gamma_{\theta_i}}\end{split}\]</span> 这个过程是和之前求<span class="math inline">\(\pi\)</span>的积分是一样的，只不过由Beta分布换成了Dirichlet 分布的情形。在此时，我们获得了一个简单清晰的生成单个词的概率，介于我们实际上需要生成文档中的所有词，为什么能不能用这种方法生成文档<span class="math inline">\(W_j\)</span>中每一个词，然后把所有生成词的概率相乘呢？</p>
<p>答案是不行。因为每个单词中<span class="math inline">\(\theta_{x,y}\)</span>它并不是一个固定的值，每一次我们对单个词的抽样将对<span class="math inline">\(\theta\)</span>的值产生影响！如果我们尝试着一次性生成2个词，就会发现在倒数第二步无法消去大量的伽马函数。从概率图上看，每个<span class="math inline">\(\theta\)</span>的箭头指向<span class="math inline">\(R_j\)</span>个相互的独立的词，所以这些词处于一个贝叶斯网络中，如果<span class="math inline">\(\theta\)</span>未知的话，我们不能假设这些词都是独立的。</p>
<p>在积分变量的时候要小心，如果是多项分布抽一个样本，对其积分就可以让Gibbs Sampler更简单。反之，如果一次从同一个多项分布中抽取多个样本，即使可以积分，形式也会非常复杂。</p>
<p>PS：</p>
<p><span style="color: #ff0000;">看到这里我有疑问。在LDA中，doc-topic分布<span class="math inline">\(\theta\)</span>和topic-word分布<span class="math inline">\(\phi\)</span>恰恰是通过积分求出来的，难道是因为它们假设主题和词的样本是一次性全部生成的？LDA的采样方法被称为”Collapsed Gibbs Sampling“，即通过积分避开了实际待估计的参数,转而对每个单词w的主题z采样，然后通过确定所有w的z然后积分获得<span class="math inline">\(\theta\)</span>和<span class="math inline">\(\phi\)</span>的值，和这里的区别究竟在哪？</span></p>
<p><span style="color: #ff0000;">2013/7/2日更新：</span></p>
<p>看了维基百科对Dirichlet-multinomial distribution的介绍，终于搞明白了。</p>
<p>首先这个跟是不是用“Collapsed”方法没啥关系，而是这两个模型有本质上的不同。</p>
<p><a href="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130702144912.png" target="_blank" rel="external"><img src="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130702144912.png" alt="QQ截图20130702144912"></a></p>
<p>在LDA模型中，每个词<span class="math inline">\(w_{dn}\)</span>都是由唯一的<span class="math inline">\(z_{dn}\)</span>生成的，两者是一一对应的关系。我们写出<span class="math inline">\(z_{dn}\)</span>生成文档<span class="math inline">\(W_j\)</span>的联合概率 <span class="math display">\[\begin{split}P(W_j|Z_n)&amp; =p(w_{dn}|W_j^{-dn},z_{dn})p(W_j^{-dn}|z_{dn}) \\&amp; =p(w_{dn}|W_j^{-dn},z_{dn})P(W_j^-{dn}) \\&amp; \sim p(w_{dn}|W_j^{-dn},z_{dn}) \\\end{split}\]</span> 可以看到，我们注意到<span class="math inline">\(W_j^{-dn}\)</span>（除去了<span class="math inline">\(w_{dn}\)</span>的词集合）中没有任何一个点与<span class="math inline">\(z_{dn}\)</span>有关。</p>
<p><a href="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130702144726.png" target="_blank" rel="external"><img src="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130702144726.png" alt="QQ截图20130702144726"></a></p>
<p>而朴素贝叶斯模型，它和LDA非常类似，唯一的区别就在于它是一篇文档一个主题，而不是一个词一个主题。这样的话，一个分布topic-word分布就会生成多个词，而不是生成一个。在计算隐藏变量<span class="math inline">\(Z\)</span>的条件分布时，就需要一次性包含<span class="math inline">\(Z_d\)</span>所产生的所有词，从而像上面所说的那样坑爹地无法化简。</p>
<h2 id="参考文献">参考文献</h2>
<ol style="list-style-type: decimal">
<li>《Pattern Recognition and Machine Learning》</li>
<li>《LDA数学八卦》</li>
<li><a href="http://www.xperseverance.net/blogs/2013/03/1682/" target="_blank" rel="external">Reading Note : Gibbs Sampling for the Uninitiated</a></li>
<li>《Gibbs Sampling for the UniniTiated》</li>
<li><a href="http://en.wikipedia.org/wiki/Dirichlet-multinomial_distribution#A_combined_example:_LDA_topic_models" target="_blank" rel="external">Dirichlet-multinomial_distribution</a></li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;《Gibbs Sampling for the UniniTiated》阅读笔记结构：&lt;/p&gt;
&lt;ol style=&quot;list-style-type: decimal&quot;&gt;
&lt;li&gt; &lt;a href=&quot;http://www.crescentmoon.info/?p=504&quot;&gt;参数估计方法及Gibbs Sampling简介&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.crescentmoon.info/?p=525&quot;&gt;一个朴素贝叶斯文档模型例子&lt;/a&gt;&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.crescentmoon.info/?p=548&quot;&gt;连续型参数求积分的思考&lt;/a&gt;
&lt;hr&gt;
&lt;/hr&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;这篇是下篇，讨论中篇联合分布中对参数求积分来简化的问题。&lt;/p&gt;
&lt;p&gt;之前存在的一个问题就是为啥我们可以对连续参数&lt;span class=&quot;math inline&quot;&gt;\(\pi\)&lt;/span&gt;求积分消去它，而不能对词分布&lt;span class=&quot;math inline&quot;&gt;\(\theta_0\)&lt;/span&gt;和&lt;span class=&quot;math inline&quot;&gt;\(\theta_1\)&lt;/span&gt;求积分。这个主意看上去很美，但是实际做的时候，你会碰到一大把无法约掉的伽马函数。让我们看看具体的过程。&lt;/p&gt;
    
    </summary>
    
      <category term="学术" scheme="http://crescentluna.github.io/categories/%E5%AD%A6%E6%9C%AF/"/>
    
      <category term="Gibbs Sampling笔记" scheme="http://crescentluna.github.io/categories/%E5%AD%A6%E6%9C%AF/Gibbs-Sampling%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Gibbs Sampling" scheme="http://crescentluna.github.io/tags/Gibbs-Sampling/"/>
    
  </entry>
  
  <entry>
    <title>《Gibbs Sampling for the UniniTiated》阅读笔记(中)---一个朴素贝叶斯文档模型例子</title>
    <link href="http://crescentluna.github.io/2013/06/29/Gibbs%20Sampling%20for%20the%20UniniTiated-2/"/>
    <id>http://crescentluna.github.io/2013/06/29/Gibbs Sampling for the UniniTiated-2/</id>
    <published>2013-06-29T04:49:12.000Z</published>
    <updated>2017-02-21T18:24:27.407Z</updated>
    
    <content type="html"><![CDATA[<p>《Gibbs Sampling for the UniniTiated》阅读笔记结构：</p>
<ol style="list-style-type: decimal">
<li> <a href="http://www.crescentmoon.info/?p=504" target="_blank" rel="external">参数估计方法及Gibbs Sampling简介</a></li>
<li><a href="http://www.crescentmoon.info/?p=525" target="_blank" rel="external">一个朴素贝叶斯文档模型例子</a></li>
<li><a href="http://www.crescentmoon.info/?p=548" target="_blank" rel="external">连续型参数求积分的思考</a></li>
</ol>
<hr>

<p>这篇是中篇，介绍一个非常简单的朴素贝叶斯文档模型生成的例子，用来说明Gibbs Sampler具体是如何构造的。</p>
<h2 id="文档生成的建模过程">文档生成的建模过程</h2>
<p>首先我们有一批文档，文档里面有很多单词，这些单词都是无顺序可交换的（词袋模型），这些文档分成两类，类标签为0或者1。给予一篇未标记的文档<span class="math inline">\(W_j\)</span>，我们要做的工作就是预测文档的类标签是<span class="math inline">\(L_j=0\)</span>还是<span class="math inline">\(L_j=1\)</span>。为了方便起见，我们定了类标签所表示的类<span class="math inline">\(\mathbb{C}_0={W_j|L_j=0}\)</span>和<span class="math inline">\(\mathbb{C}_1={W_j|L_j=1}\)</span>。一般来说预测这种事都是选择最有可能发生的，即找到<span class="math inline">\(W_j\)</span>的后验概率<span class="math inline">\(P(L_j|W_j)\)</span>最大的标签<span class="math inline">\(L_j\)</span>。使用贝叶斯公式 <span class="math display">\[\begin{equation}
\begin{split}
L_j=\arg \max \limits_{L}P(L|W_j)&amp; =\arg \max \limits_{L}\frac{P(W_j|L)P(L)}{P(W_j)}\\&amp; =\arg \max \limits_{L} P(W_j|L)P(L) \\\end{split}
\end{equation}\]</span> 因为分母<span class="math inline">\(P(W_j)\)</span>与<span class="math inline">\(L\)</span>无关所以删去了。 通过贝叶斯公式的转换，我们可以想象这些文档的生成过程。首先，我们选择文档的类标签<span class="math inline">\(L_j\)</span>;假设这个过程是通过投硬币完成的（正面概率为<span class="math inline">\(\pi=P(L_j=1)\)</span> )，正式地来说，就是服从贝努利分布 <span class="math display">\[\begin{equation}L_j \sim Bernoulli(\pi)\end{equation}\]</span> 然后，对于文档上<span class="math inline">\(R_j\)</span>个“词位”中的每一个，我们根据一个概率分布<span class="math inline">\(\theta\)</span>，随机独立地抽样一个词<span class="math inline">\(w_i\)</span>。因为每个类生成词的<span class="math inline">\(\theta\)</span>分布都不同，所以应该有<span class="math inline">\(\theta_1\)</span>和<span class="math inline">\(\theta_2\)</span>，具体地生成词的时候，我们根据文档的标签<span class="math inline">\(L_j\)</span>来决定由哪个类来生成 <span class="math display">\[\begin{equation}
W_j \sim Multinomial(R_j,\theta_{L_j})
\end{equation}\]</span> <a id="more"></a></p>
<h2 id="先验">先验</h2>
<p>上面提到的参数<span class="math inline">\(\pi\)</span>和<span class="math inline">\(\theta\)</span>还有各自的先验知识。我们假设参数<span class="math inline">\(\pi\)</span>是从一个参数为<span class="math inline">\(\gamma_{\pi_1}\)</span>和<span class="math inline">\(\gamma_{\pi_0}\)</span>的Beta分布中采样出来的。这里<span class="math inline">\(\gamma_\pi=&lt;\gamma_{\pi_1},\gamma_{\pi_0}&gt;\)</span> 被称作超参数，因为它们是先验模型的参数，是为了确定模型的参数而存在的。类似地，就像Beta分布是贝努利分布的共轭先验一样，<span class="math inline">\(\theta\)</span> 参数的确定来自于参数为<span class="math inline">\(\gamma_\theta\)</span>的Dirichlet分布。公式如下： <span class="math display">\[\begin{equation}\pi \sim Beta(\gamma_\pi)\end{equation}\]</span><span class="math display">\[\begin{equation}\theta \sim Dirichlet(\gamma_\theta)\end{equation}\]</span> 选择Beta分布和Dirichlet分布作为先验分布，是为了数学上的方便。所以的参数定义见图1和图2(这个概率图非常好看，是从别人的博客对这篇文章的介绍中借来的，来自于[3]）.</p>
<p><a href="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130629204136.png" target="_blank" rel="external"><img src="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130629204136.png" alt="QQ截图20130629204136"></a></p>
<p><a href="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130629204149.png" target="_blank" rel="external"><img src="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130629204149.png" alt="QQ截图20130629204149"></a></p>
<h2 id="状态空间以及初始化">状态空间以及初始化</h2>
<p>状态空间。在前面提到过一样，Gibbs 抽样器要做的是遍历模型中<span class="math inline">\(k\)</span>维状态空间，对里面所有的变量采样。在当前模型的状态空间中存在以下变量：</p>
<ul>
<li><p>1个标量变量<span class="math inline">\(\pi\)</span></p></li>
<li><p>2个向量变量<span class="math inline">\(\theta_0\)</span>和<span class="math inline">\(\theta_1\)</span>。</p></li>
<li><p>N篇文档所对应的二元标签向量变量<span class="math inline">\(\mathbf{L}\)</span>。</p></li>
</ul>
<p>还有每篇文档里的词向量<span class="math inline">\(W_j\)</span>，但是他们是被观察到的数据，值已经知道了（这就是为啥图2中的概率图<span class="math inline">\(W_{jk}\)</span>是实心的）。</p>
<p>初始化。初始化就是一篇文档的生成过程，不细讲了。</p>
<h2 id="产生联合分布">产生联合分布</h2>
<p>对于之前提到的每次循环<span class="math inline">\(t=1\ldots T\)</span>，我们都像上篇的式子(11)中提到的那样，利用其他变量的条件分布对状态空间中的每个变量更新。具体过程如下：</p>
<ol style="list-style-type: decimal">
<li><p>写出所有变量的联合分布</p></li>
<li><p>简化联合分布的公式</p></li>
<li><p>确定上篇的式（11）中条件分布的公式</p></li>
<li><p>给出采样器伪码的最终形式</p></li>
</ol>
<h2 id="写出联合分布">写出联合分布</h2>
<p>根据模型，我们可以得到整个文档的联合分布<span class="math inline">\(P(\mathbb{C},\mathbf{L},\pi,\theta_0,\theta_1;\gamma_{\pi_1},\gamma_{\pi_0},\gamma_{\theta})\)</span>。 根据模型的生成过程和概率图，我们可以将联合分布分解: <span class="math display">\[\begin{equation}
P(\pi|\gamma_{\pi_1},\gamma_{\pi_0})P(\mathbf{L}|\pi)P(\theta_0|\gamma_{\theta})P(\theta_1|\gamma_{\theta})P(\mathbb{C}_0|\theta_0,\mathbf{L})P(\mathbb{C}_1|\theta_1,\mathbf{L})
\end{equation}\]</span> 然后将一个一个地来解释这些式子(从引用了一部分）：</p>
<ul>
<li><p><span class="math inline">\(P(\pi|\gamma_{\pi_1},\gamma_{\pi_0})\)</span>。这个式子是根据超参数为<span class="math inline">\(\gamma_{\pi_1}\)</span>和<span class="math inline">\(\gamma_{\pi_0}\)</span>的Beta分布中采样出<span class="math inline">\(\pi\)</span>的公式，根据Beta 分布的定义，概率为 <span class="math display">\[\begin{equation}P(\pi|\gamma_{\pi_1},\gamma_{\pi_0})=\frac{\Gamma(\gamma_{\pi_1}+\gamma_{\pi_0})}{\Gamma(\gamma_{\pi_1})\Gamma(\gamma_{\pi_0})}\pi^{\gamma_{\pi_1}-1}(1-\pi)^{\gamma_{\pi_0}-1} \end{equation}\]</span> 因为右边式子的<span class="math inline">\(\frac{\Gamma(\gamma_{\pi_1}+\gamma_{\pi_0})}{\Gamma(\gamma_{\pi_1})\Gamma(\gamma_{\pi_0})}\)</span> 是个常量(设为<span class="math inline">\(c\)</span>)，而整个联合分布最终要把所有的常量归一化，所以在这里先不管它，把式子写作 <span class="math display">\[\begin{equation}
P(\pi|\gamma_{\pi_1},\gamma_{\pi_0}) \propto \pi^{\gamma_{\pi_1}-1}(1-\pi)^{\gamma_{\pi_0}-1}
\end{equation}\]</span></p></li>
<li><p><span class="math inline">\(P(\mathbf{L}|\pi)\)</span>。第二个式子是由参数<span class="math inline">\(\pi\)</span>或<span class="math inline">\(1-\pi\)</span>生成标签变量1或0的过程，要注意一共有<span class="math inline">\(N\)</span>篇文档，每篇文档的标签生成都是相互独立的： <span class="math display">\[\begin{equation}
\begin{split}P(\mathbf{L}|\pi)&amp; =\prod^N_{n=1}\pi^{L_n}(1-\pi)^{N-L_n} \\ &amp;=\pi^{C_1}(1-\pi)^{C_0} \\ \end{split} \end{equation}\]</span> 其中<span class="math inline">\(C_0\)</span>是和<span class="math inline">\(C_1\)</span>分别是标签为0和1的文档数目。</p></li>
<li><p><span class="math inline">\(P(\theta_0|\gamma_{\theta})\)</span>和<span class="math inline">\(P(\theta_1|\gamma_{\theta})\)</span>。这个式子是根据超参数为<span class="math inline">\(\gamma_{\theta}\)</span>的Dirichlet分布中采样出<span class="math inline">\(\theta_0\)</span>和<span class="math inline">\(\theta_1\)</span>的词分布的过程。因为<span class="math inline">\(\theta_0\)</span>和<span class="math inline">\(\theta_1\)</span>相互独立，生成的过程是一样的，所以为了简单起见，我们暂时舍去类的下标，公式如下： <span class="math display">\[\begin{equation}
\begin{split}
P(\theta|\gamma_{\theta})&amp;=\frac{\Gamma{(\sum^V_{i=1}\gamma_{\theta_i})}}{\prod^V_{i=1}\Gamma{(\gamma_{\theta_i})}}\prod^V_{i=1}\theta_i^{\gamma_{\theta_i}-1}\\&amp; = c&#39;\prod^V_{i=1}\theta_i^{\gamma_{\theta_i}-1} \\&amp; \propto \prod^V_{i=1}\theta_i^{\gamma_{\theta_i}-1} \\\end{split} \end{equation}\]</span> <span class="math inline">\(\gamma_{\theta_i}\)</span>代表了向量<span class="math inline">\(\gamma_{\theta}\)</span>第<span class="math inline">\(i\)</span>维的值。类似地，<span class="math inline">\(\theta_i\)</span>代表向量<span class="math inline">\(\theta\)</span>地<span class="math inline">\(i\)</span>维的值，其物理意义就是该分布上第<span class="math inline">\(i\)</span>个词被生成的概率。<span class="math inline">\(c&#39;\)</span>是另一个归一化因子。</p></li>
<li><p><span class="math inline">\(P(\mathbb{C}_0|\theta_0,\mathbf{L})\)</span>和<span class="math inline">\(P(\mathbb{C}_1|\theta_1,\mathbf{L})\)</span>。这俩式子是具体地生成该类的所有文档中的所有词的过程。首先要求来看对于单独一个文档<span class="math inline">\(n\)</span>，产生所有word也就是<span class="math inline">\(W_n\)</span>的概率。假设对于某个文档，<span class="math inline">\(\theta=(0.2,0.5,0.3)\)</span>，意思就是word1产生概率为0.2，word2产生概率为0.5，word3的概率为0.3。假如这个文档里word1有2个，word2有3个，word3有2个，则这个文档的产生概率就是(0.2<em>0.2)</em>(0.5<em>0.5</em>0.5)<em>(0.3</em>0.3)。所以按照这个道理，一个文档生成的联合概率如下： <span class="math display">\[\begin{equation}P(W_n|\mathbf{L},\theta_{L_n})=\prod^V_{i=1}\theta_i^{W_{ni}} \end{equation}\]</span> 这里<span class="math inline">\(\theta_{L_n}\)</span>代表了文档<span class="math inline">\(n\)</span>所在的类（<span class="math inline">\(\theta_0\)</span>或<span class="math inline">\(\theta_1\)</span>），<span class="math inline">\(\theta_i\)</span>像之前提过的那样，代表产生第<span class="math inline">\(i\)</span>个词的概率，而指数<span class="math inline">\(W_{ni}\)</span>为该词出现的频率。现在我们有一篇文档的生成概率了，然后我们把一个类别下面的所有文档都合并起来(因为都是相互独立的么) <span class="math display">\[\begin{equation}\begin{split} P(\mathbb{C}_x|\mathbf{L},\theta_x)&amp; =\prod_{n\in \mathbb{C}_x} \prod^V_{i=1}\theta_i^{W_{ni}} \\ &amp;=\prod^V_{i=1}\theta_{x,i}^{\mathcal{N}_{\mathbb{C}_x(i)}}\end{split}\end{equation}\]</span> 其中<span class="math inline">\(x\)</span>代表了类的标号（这里是0或1），<span class="math inline">\(\mathcal{N}_{\mathbb{C}_x}\)</span>代表了类标号为<span class="math inline">\(x\)</span>的所有文档生成词<span class="math inline">\(i\)</span>的数目。</p></li>
</ul>
<h3 id="先验选择和简化联合分布">先验选择和简化联合分布</h3>
<p>然后我们将先验知识 <span class="math inline">\(P(\mathbf{L}|\pi)\)</span>与观察到的evidence<span class="math inline">\(P(\pi|\gamma_{\pi_1},\gamma_{\pi_0})\)</span> 相乘，我们可以看到估计的参数<span class="math inline">\(\pi\)</span> 是如何被观察到的数据所影响的。利用式子(7) 和(9) 我们得到 <span class="math display">\[\begin{equation}
\begin{split}
P(\pi|\mathbf{L};\gamma_{\pi_1},\gamma_{\pi_0}) &amp;=P(\mathbf{L}|\pi)P(\pi|\gamma_{\pi_1},\gamma_{\pi_0}) \\&amp; \propto [\pi^{C_1}(1-\pi)^{C_0}][\pi^{\gamma_{\pi_1}-1}(1-\pi)^{\gamma_{\pi_0}-1}] \\&amp; \propto \pi^{C_1+\gamma_{\pi_1}-1}(1-\pi)^{C_0+\gamma_{\pi_0}-1} \\
\end{split}
\end{equation}\]</span> 对于<span class="math inline">\(\theta\)</span>,类似地我们结合(10）和（12）,有 <span class="math display">\[\begin{equation}
\begin{split}P(\theta|W_n;\gamma_\theta)&amp; =P(W_n|\theta)P(\theta|\gamma_\theta) \\&amp; \propto \prod^V_{i=1}\theta_i^{W_{ni}} \prod^V_{i=1}\theta_i^{\gamma_{\theta_i}-1} \\&amp; \propto \prod^V_{i=1}\theta_i^{W_{ni}+\gamma_{\theta_i}-1} \\\end{split}\end{equation}\]</span> 然后从一篇文章推广到某类下的所有文章，我们有 <span class="math display">\[\begin{equation}
P(\theta_x|\mathbb{C}_x;\gamma_\theta) \propto \prod^V_{i=1}\theta_{x,i}^{\mathcal{N}_{\mathbb{C}_x}(i)+\gamma_{\theta_i}-1}
\end{equation}\]</span> 最后，我们把所有的因式都乘起来，再令<span class="math inline">\(\mu=&lt;\gamma_{\pi_1},\gamma_{\pi_0},\gamma_{\theta}&gt;\)</span>,写出简化后的全联合概率: <span class="math display">\[\begin{equation}
P(\mathbb{C},\mathbf{L},\pi,\theta_0,\theta_1;\mu) \propto \pi^{C_1+\gamma_{\pi_1}-1}(1-\pi)^{C_0+\gamma_{\pi_0}-1} \prod^V_{i=1}\theta_{0,i}^{\mathcal{N}_{\mathbb{C}_0}+\gamma_{\theta_i}-1} \theta_{1,i}^{\mathcal{N}_{\mathbb{C}_1}+\gamma_{\theta_i}-1}
\end{equation}\]</span></p>
<h3 id="将隐含变量pi积出">将隐含变量<span class="math inline">\(\pi\)</span>积出</h3>
<p>为了进一步地简化模型，我们可以对参数<span class="math inline">\(\pi\)</span>求积分，从而消去这个参数。 <span class="math display">\[\begin{equation}
\begin{split}&amp; P(\mathbb{C},\mathbf{L},\theta_0,\theta_1;\mu)\\ =&amp;\int_\pi P(\mathbb{C},\mathbf{L},\pi,\theta_0,\theta_1;\mu) d\pi \\=&amp;\int_\pi P(\pi|\gamma_{\pi_1},\gamma_{\pi_0})P(\mathbf{L}|\pi)P(\theta_0|\gamma_{\theta})P(\theta_1|\gamma_{\theta})P(\mathbb{C}_0|\theta_0,\mathbf{L})P(\mathbb{C}_1|\theta_1,\mathbf{L}) d\pi \\ =&amp;P(\theta_0|\gamma_{\theta})P(\theta_1|\gamma_{\theta})P(\mathbb{C}_0|\theta_0,\mathbf{L})P(\mathbb{C}_1|\theta_1,\mathbf{L}) \int_\pi P(\pi|\gamma_{\pi_1},\gamma_{\pi_0})P(\mathbf{L}|\pi) d\pi \\\end{split}\end{equation}\]</span> 这里我们对积分式子内的<span class="math inline">\(\pi\)</span>求积分 <span class="math display">\[\begin{equation}
\begin{split}&amp;\int_\pi P(\pi|\gamma_{\pi_1},\gamma_{\pi_0})P(\mathbf{L}|\pi) d\pi \\ =&amp;\frac{\Gamma(\gamma_{\pi_1}+\gamma_{\pi_0})}{\Gamma(\gamma_{\pi_1})\Gamma(\gamma_{\pi_0})}\pi^{\gamma_{\pi_1}-1}(1-\pi)^{\gamma_{\pi_0}-1} \pi^{C_1}(1-\pi)^{C_0} d\pi \\=&amp; \frac{\Gamma(\gamma_{\pi_1}+\gamma_{\pi_0})}{\Gamma(\gamma_{\pi_1})\Gamma(\gamma_{\pi_0})} \int_\pi \pi^{C_1+\gamma_{\pi_1}-1}(1-\pi)^{C_0+\gamma_{\pi_0}-1} d\pi \\\end{split}\end{equation}\]</span> 要注意这里式子右边的积分部分其实是尚未归一化的参数为<span class="math inline">\(C_1+\gamma_{\pi_1}\)</span>和<span class="math inline">\(C_0+\gamma_{\pi_0}\)</span> 的Beta分布求积分，所以我们只需要先补上Beta(<span class="math inline">\(C_1+\gamma_{\pi_1}\)</span>,<span class="math inline">\(C_0+\gamma_{\pi_0}\)</span>)归一化的常数项即可: <span class="math display">\[\begin{equation}\frac{\Gamma(C_0+C_1+\gamma_{\pi_1}+\gamma_{\pi_0})}{\Gamma(C_1+\gamma_{\pi_1})\Gamma(C_0+\gamma_{\pi_0})}\end{equation}\]</span> 令<span class="math inline">\(N=C_0+C_1\)</span>，我们得到（<strong>注意这里应为Beta归一化的常数的倒数！！！原文中写反了</strong>） <span class="math display">\[\begin{equation}
\int_\pi P(\pi|\gamma_{\pi_1},\gamma_{\pi_0})P(\mathbf{L}|\pi) d\pi=\frac{\Gamma(\gamma_{\pi_1}+\gamma_{\pi_0})}{\Gamma(\gamma_{\pi_1})\Gamma(\gamma_{\pi_0})} \frac{\Gamma(C_1+\gamma_{\pi_1})\Gamma(C_0+\gamma_{\pi_0})}{\Gamma(C_0+C_1+\gamma_{\pi_1}+\gamma_{\pi_0})}
\end{equation}\]</span> 最后得出 <span class="math display">\[\begin{equation}P(\mathbb{C},\mathbf{L},\theta_0,\theta_1;\mu) \propto \frac{\Gamma(\gamma_{\pi_1}+\gamma_{\pi_0})}{\Gamma(\gamma_{\pi_1})\Gamma(\gamma_{\pi_0})} \frac{\Gamma(C_1+\gamma_{\pi_1})\Gamma(C_0+\gamma_{\pi_0})}{\Gamma(C_0+C_1+\gamma_{\pi_1}+\gamma_{\pi_0})}\prod^V_{i=1}\theta_{0,i}^{\mathcal{N}_{\mathbb{C}_0}(i)+\gamma_{\theta_i}-1} \theta_{1,i}^{\mathcal{N}_{\mathbb{C}_1}(i)+\gamma_{\theta_i}-1}\end{equation}\]</span> 这里很容易想到的是，既然我们可以通过对<span class="math inline">\(\pi\)</span>求积分来简化联合分布，为什么不用这种方法把里面的参数全积分了呢？详细请见下篇的解释。</p>
<h2 id="构建gibbs-sampler">构建Gibbs Sampler</h2>
<p>根据Gibbs 抽样的定义，我们每次都从条件分布中抽样出<span class="math inline">\(Z_i\)</span>的新值： <span class="math display">\[\begin{equation}
P(Z_i|z_1^{(t+1)},\ldots,z^{(t+1)}_{i-1},z^{(t)}_{i+1},\ldots,z^{t}_k)
\end{equation}\]</span> 替换成具体的例子，在分配<span class="math inline">\(L_1^{(t+1)}\)</span>的值时，我们需要计算条件分布 <span class="math display">\[\begin{equation}P(L_1|L_2^{(t)},\ldots,L_N^{(t)},\mathbb{C},\theta_0^{(t)},\theta_1^{(t)};\mu)\end{equation}\]</span> 然后我们算<span class="math inline">\(L_2{(t+1)}\)</span>的值， <span class="math display">\[\begin{equation}P(L_2|L_1^{(t+1)},L_3^{(t)},\ldots,L_N^{(t)},\mathbb{C},\theta_0^{(t)},\theta_1^{(t)};\mu)\end{equation}\]</span> 以此类推直到计算完<span class="math inline">\(L_N^{(t+1)}\)</span>为止。然后我们计算<span class="math inline">\(\theta_0\)</span>的值, <span class="math display">\[\begin{equation}P(\theta_0^{(t+1)}|L_1^{(t+1)},L_2^{(t+1)},\ldots,L_N^{(t+1)},\mathbb{C},\theta_1^{(t)};\mu)\end{equation}\]</span> 最后是<span class="math inline">\(\theta_1\)</span> <span class="math display">\[\begin{equation}P(\theta_1^{(t+1)}|L_1^{(t+1)},L_2^{(t+1)},\ldots,L_N^{(t+1)},\mathbb{C},\theta_0^{(t)};\mu)\end{equation}\]</span> 在循环<span class="math inline">\(t\)</span>开始的时候，我们拥有一些信息，这些信息包括：每个文档中词的数目，标签为0的文档数，标签为1的文档数，所有标签为0的文档的词数目，所有标签为1的文档的词数目，每个文档的当前标签，当前的<span class="math inline">\(\theta_1\)</span>和<span class="math inline">\(\theta_0\)</span>值，等等。当我们想得到文档<span class="math inline">\(j\)</span>的新标签时，我们暂时地移除所有当前文档的信息（包括词数目和标签信息），然后通过余下的信息得出<span class="math inline">\(L_j=0\)</span> 的条件概率和<span class="math inline">\(L_j=1\)</span>的条件概率，最后根据这俩概率的相对比例采样得到新的<span class="math inline">\(L_j{(t+1)}\)</span>。对<span class="math inline">\(\theta\)</span>采样时也是如此。</p>
<h3 id="对文档标签mathbfl采样">对文档标签<span class="math inline">\(\mathbf{L}\)</span>采样</h3>
<p>接下来我们具体地看看那如何对文档标签采样。根据之前的条件概率公式，我们得到 <span class="math display">\[\begin{equation}
\begin{split}
P(L_j|\mathbf{L}^{(-j)},\mathbb{C}^{(-j)},\theta_0,\theta_1;\mu)&amp; =\frac{P(L_j,W_j,\mathbf{L}^{(-j)},\mathbb{C}^{(-j)},\theta_0,\theta_1;\mu)}{P(\mathbf{L}^{(-j)},\mathbb{C}^{(-j)},\theta_0,\theta_1;\mu)} \\
&amp;=\frac{P(\mathbf{L},\mathbb{C},\theta_0,\theta_1;\mu)}{P(\mathbf{L}^{(-j)},\mathbb{C}^{(-j)},\theta_0,\theta_1;\mu)}
\end{split}
\end{equation}\]</span> <span class="math inline">\(\mathbf{L}^{(-j)}\)</span>是除了<span class="math inline">\(L_j\)</span>外所有的文档标签，然后<span class="math inline">\(\mathbb{C}^{(-j)}\)</span>是除了<span class="math inline">\(W_j\)</span>外所有的文档集合。这个分布有2个结果，<span class="math inline">\(L_j=0\)</span>或<span class="math inline">\(L_j=1\)</span>。 要注意这里分子就是式(32)中的全联合概率分布。而分母仅仅除去了文档<span class="math inline">\(W_j\)</span> 的信息。然后我们来看看除去了该文档对整个式子造成了什么影响。</p>
<p>式（32）中第一个因式<span class="math inline">\(\frac{\Gamma(\gamma_{\pi_1}+\gamma_{\pi_0})}{\Gamma(\gamma_{\pi_1})\Gamma(\gamma_{\pi_0})}\)</span>是个常数，与<span class="math inline">\(W_j\)</span>无关，分子分母都有一个，所以计算的时候被约掉了。第二个式子是 <span class="math display">\[\begin{equation}
\frac{\Gamma(N+\gamma_{\pi_1}+\gamma_{\pi_0})}{\Gamma(C_1+\gamma_{\pi_1})\Gamma(C_0+\gamma_{\pi_0})}
\end{equation}\]</span> 现在我们来看看去掉了一个文档<span class="math inline">\(W_j\)</span>，发生了什么变化。首先，语料的大小从<span class="math inline">\(N\)</span>减小到<span class="math inline">\(N-1\)</span>。 如果文档<span class="math inline">\(L_j=0\)</span>，那么<span class="math inline">\(C_0^{(-j)}=C_0-1,C_1^{(-j)}=C_1\)</span>。反之如果文档<span class="math inline">\(L_j=1\)</span>，那么<span class="math inline">\(C_1^{(-j)}=C_1-1,C_0^{(-j)}=C_0\)</span>。我们令<span class="math inline">\(x \in {0,1}\)</span>这样就可以统一以上的情况，<span class="math inline">\(C_x^{(-j)}=C_x-1\)</span>。然后我们重写分子和分母的这两个式子，从 <span class="math display">\[\begin{equation}
\frac{\frac{\Gamma(C_1+\gamma_{\pi_1}}{\Gamma(N+\gamma_{\pi_1}+\gamma_{\pi_0}))\Gamma(C_0+\gamma_{\pi_0})}}{\frac{\Gamma(C_1^{(-j)}+\gamma_{\pi_1})\Gamma(C_0^{(-j)}+\gamma_{\pi_0})}{\Gamma(N+\gamma_{\pi_1}+\gamma_{\pi_0}-1)}}
\end{equation}\]</span> 到 <span class="math display">\[\begin{equation}
\frac{\Gamma(C_x+\gamma_{\pi_x})\Gamma(N+\gamma_{\pi_1}+\gamma_{\pi_0}-1)}{\Gamma(N+\gamma_{\pi_1}+\gamma_{\pi_0})\Gamma(C_x+\gamma_{\pi_x}-1)}
\end{equation}\]</span> 利用伽马函数<span class="math inline">\(\Gamma(a+1)=a\Gamma(a)\)</span>的性质，我们简化上面的式子最终得到 <span class="math display">\[\begin{equation}
\frac{C_x+\gamma_{\pi_x}-1}{N+\gamma_{\pi_1}+\gamma_{\pi_0}-1}
\end{equation}\]</span> <strong>（注意！！！这里原文又写错了，分子项少减了一个1</strong>）这样我们就把伽马函数消去了。再来看剩下的式子 <span class="math display">\[\begin{equation}
\prod^V_{i=1}\theta_{0,i}^{\mathcal{N}_{\mathbb{C}_0}(i)+\gamma_{\theta_i}-1} \theta_{1,i}^{\mathcal{N}_{\mathbb{C}_1}(i)+\gamma_{\theta_i}-1}
\end{equation}\]</span> 当我们去掉了文档<span class="math inline">\(W_j\)</span>之后，可以发现文档不属于的另一类是不会有变化的，会上下约掉。而文档属于的那一类，<span class="math inline">\(C_x^{(-j)}=C_x-1\)</span>，与文档<span class="math inline">\(W_j\)</span>无关的都被約掉，只剩下<span class="math inline">\(W_j\)</span>中的词，最后 <span class="math display">\[\begin{equation}
\prod^V_{i=1} \frac{\theta_{x,i}^{\mathcal{N}_{\mathbb{C}_x}(i)+\gamma_{\theta_i}-1}}{\theta_{x,i}^{\mathcal{N}_{\mathbb{C}_x^{(-j)}}(i)+\gamma_{\theta_i}-1}}=\prod^V_{i=1} \theta_{x,i}^{W_{ji}}
\end{equation}\]</span> 然后把（42）式和（44）式相乘，就得到了最后的公式，对于<span class="math inline">\(x\in{0,1}\)</span> <span class="math display">\[\begin{equation}
P(L_j=x|\mathbf{L})^{(-j)},\mathbb{C}^{(-j)},\theta_0,\theta_1;\mu)=\frac{C_x+\gamma_{\pi_x}-1}{N+\gamma_{\pi_1}+\gamma_{\pi_0}-1}\prod^V_{i=1} \theta_{x,i}^{W_{ji}}
\end{equation}\]</span> 这个式子清楚地表现了标签是如何被选择的。这个式子中，前半部分其实只有<span class="math inline">\(C_x\)</span>是变量，所以如果<span class="math inline">\(C_0\)</span>大，则算出来的<span class="math inline">\(P(L_j=0)\)</span>的概率就会大一点，所以下一次Lj的值就会倾向于C0，反之就会倾向于C1。 而后半部分，是在判断当前<span class="math inline">\(\theta\)</span>参数的情况下，这些词<span class="math inline">\(W_j\)</span>是否符合该参数所对应的分布（表现为似然值更大），然后确定整个文档是更倾向于C0还是C1。</p>
<p>最后（终于结束了！），我们从得到的条件概率中抽样：</p>
<ul>
<li><p>令<span class="math inline">\(value0\)</span>为式(34)<span class="math inline">\(x=0\)</span>时的值。</p></li>
<li><p>令<span class="math inline">\(value1\)</span>为式(34)<span class="math inline">\(x=1\)</span>时的值。</p></li>
<li><p>令概率分布为<span class="math inline">\(&lt;\frac{value0}{value0+value1},\frac{value1}{value0+value1}&gt;\)</span>。</p></li>
<li><p>根据这个分布进行贝努利实验（投两边概率不同的硬币）得到新的<span class="math inline">\(L_j^{(t+1)}\)</span>的值。</p></li>
</ul>
<h3 id="对theta采样">对<span class="math inline">\(\theta\)</span>采样</h3>
<p>接下來看看如何对<span class="math inline">\(\theta_0\)</span>和<span class="math inline">\(\theta_1\)</span>采样。因为这俩参数都是相互独立的，我们再一次删去他们的下标，统一地看做<span class="math inline">\(\theta\)</span>。将式(25)中与当前<span class="math inline">\(\theta\)</span>无关的参数删去，我们可以观察到 <span class="math display">\[\begin{equation}P(\theta|\mathbb{C},\mathbf{L});\mu) \propto P(\mathbb{C},\mathbf{L}|\theta)P(\theta|\mu)\end{equation}\]</span> 其实这时候式子里只剩下<span class="math inline">\(\theta_{x,i}^{\mathcal{N}_{\mathbb{C}_x}(i)+\gamma_{\theta_i}-1}\)</span>而已。可以看到，因为先验分布<span class="math inline">\(P(\theta|\mu)\)</span>是共轭的Dirichlet分布，所以后验分布也是Dirichlet分布，只是每个词项<span class="math inline">\(i\)</span>的参数变成了<span class="math inline">\(\mathcal{N}_{\mathbb{C}_x}(i)+\gamma_{\theta_i}\)</span>而已，我们定义<span class="math inline">\(V\)</span> 维的向量<span class="math inline">\(\mathbf{t}\)</span>,令<span class="math inline">\(t_i=\mathcal{N}_{\mathbb{C}_x}(i)+\gamma_{\theta_i}\)</span>，就有 <span class="math display">\[\begin{equation}\theta \sim Dirichlet(\mathbf{t})\end{equation}\]</span> 那么如何从一个Dirichlet分布采样出<span class="math inline">\(\theta\)</span>呢？（见的11.1.2的Reject Sampling）为了从参数为<span class="math inline">\(&lt;\alpha_1,\ldots,\alpha_V&gt;\)</span>的<span class="math inline">\(V\)</span>维Dirichlet分布中抽样出一个随机向量<span class="math inline">\(\mathbf{a}=&lt;a_1,\ldots,a_V&gt;\)</span>，我们只要从<span class="math inline">\(V\)</span>个伽马分布中各自采样一个独立的样本<span class="math inline">\(y_1,\ldots,y_V\)</span>，其中每个伽马分布的密度函数为 <span class="math display">\[\begin{equation}Gamma(\alpha_i,1)=\frac{y_i^{\alpha_i-1}e^{-y_i}}{\Gamma(\alpha_i)}\end{equation}\]</span> 然后令<span class="math inline">\(a_i=y_i/\sum^V_{j=1}y_j\)</span>即可。</p>
<h3 id="利用已有标签的文档">利用已有标签的文档</h3>
<p>我们可以利用已经被标签过的文档来，只需要不对这些文档采样新标签即可。这些被标注的文档会作为背景信息为算法服务。</p>
<h3 id="整个过程">整个过程</h3>
<p>最终的算法图：<a href="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130629204201.png" target="_blank" rel="external"><img src="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130629204201.png" alt="QQ截图20130629204201"></a></p>
<p>需要注意的是每次新的标签<span class="math inline">\(L_j\)</span>产生的时候，都会对接下来的文档标签数目统计产生影响，这就是Gibbs Sampler的本质。</p>
<h3 id="从gibbs-sampler产生值">从Gibbs sampler产生值</h3>
<p>Gibbs Sampling在每个循环都会产生变量的值，像之前提到过的那样，在理论上，变量<span class="math inline">\(Z_i\)</span>可以通过<span class="math inline">\(T\)</span>次获得的值近似 <span class="math display">\[\begin{equation}\frac{1}{T}\sum^T_{t=1}z^{(t)}_i\end{equation}\]</span> 但是实际中一般不直接这样用。</p>
<h3 id="收敛和burn-in迭代">收敛和burn-in迭代</h3>
<p>根据选择初始值的不同，Gibbs sampler需要一定的迭代次数才能保证点<span class="math inline">\(&lt;z_1^{t},z_2^{t},\ldots,z_k^{t}&gt;\)</span>都是从马尔科夫链的平稳分布中生成的（换句话说就是马尔科夫链需要一定的次数才能收敛）。为了避免在这之前的估计对结果产生的影响，一般都丢弃<span class="math inline">\(t&lt;B\)</span>之前的结果，之前的阶段就被称为“burn-in”阶段，所以取平均值的时候是从<span class="math inline">\(B+1\)</span>次到<span class="math inline">\(T\)</span>次的。</p>
<h3 id="自相关和lag">自相关和lag</h3>
<p>式（38）里的近似假设<span class="math inline">\(Z_i\)</span>的那些样本都是相互独立的，而事实上我们知道他们不是，因为新的点都是从前面的点所给的条件所生成的。这个问题被称作自相关(autocorrelation)。为了避免这个问题，很多Gibbs Sampling在实现的时候取每<span class="math inline">\(L\)</span>个值的平均值，这个<span class="math inline">\(L\)</span>被称作滞后（lag，我也不知道怎么翻译）。具体的探讨请见原文。</p>
<p>还有多链问题和超参数的选择问题，以及原文推荐的一些有用的文章，这里就不详述了。</p>
<h2 id="参考文献">参考文献：</h2>
<ol style="list-style-type: decimal">
<li>《Pattern Recognition and Machine Learning》</li>
<li>《LDA数学八卦》</li>
<li>http://www.xperseverance.net/blogs/2013/03/1682/</li>
<li>《Gibbs Sampling for the UniniTiated》</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;《Gibbs Sampling for the UniniTiated》阅读笔记结构：&lt;/p&gt;
&lt;ol style=&quot;list-style-type: decimal&quot;&gt;
&lt;li&gt; &lt;a href=&quot;http://www.crescentmoon.info/?p=504&quot;&gt;参数估计方法及Gibbs Sampling简介&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.crescentmoon.info/?p=525&quot;&gt;一个朴素贝叶斯文档模型例子&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.crescentmoon.info/?p=548&quot;&gt;连续型参数求积分的思考&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;/hr&gt;
&lt;p&gt;这篇是中篇，介绍一个非常简单的朴素贝叶斯文档模型生成的例子，用来说明Gibbs Sampler具体是如何构造的。&lt;/p&gt;
&lt;h2 id=&quot;文档生成的建模过程&quot;&gt;文档生成的建模过程&lt;/h2&gt;
&lt;p&gt;首先我们有一批文档，文档里面有很多单词，这些单词都是无顺序可交换的（词袋模型），这些文档分成两类，类标签为0或者1。给予一篇未标记的文档&lt;span class=&quot;math inline&quot;&gt;\(W_j\)&lt;/span&gt;，我们要做的工作就是预测文档的类标签是&lt;span class=&quot;math inline&quot;&gt;\(L_j=0\)&lt;/span&gt;还是&lt;span class=&quot;math inline&quot;&gt;\(L_j=1\)&lt;/span&gt;。为了方便起见，我们定了类标签所表示的类&lt;span class=&quot;math inline&quot;&gt;\(\mathbb{C}_0={W_j|L_j=0}\)&lt;/span&gt;和&lt;span class=&quot;math inline&quot;&gt;\(\mathbb{C}_1={W_j|L_j=1}\)&lt;/span&gt;。一般来说预测这种事都是选择最有可能发生的，即找到&lt;span class=&quot;math inline&quot;&gt;\(W_j\)&lt;/span&gt;的后验概率&lt;span class=&quot;math inline&quot;&gt;\(P(L_j|W_j)\)&lt;/span&gt;最大的标签&lt;span class=&quot;math inline&quot;&gt;\(L_j\)&lt;/span&gt;。使用贝叶斯公式 &lt;span class=&quot;math display&quot;&gt;\[\begin{equation}
\begin{split}
L_j=\arg \max \limits_{L}P(L|W_j)&amp;amp; =\arg \max \limits_{L}\frac{P(W_j|L)P(L)}{P(W_j)}\\&amp;amp; =\arg \max \limits_{L} P(W_j|L)P(L) \\\end{split}
\end{equation}\]&lt;/span&gt; 因为分母&lt;span class=&quot;math inline&quot;&gt;\(P(W_j)\)&lt;/span&gt;与&lt;span class=&quot;math inline&quot;&gt;\(L\)&lt;/span&gt;无关所以删去了。 通过贝叶斯公式的转换，我们可以想象这些文档的生成过程。首先，我们选择文档的类标签&lt;span class=&quot;math inline&quot;&gt;\(L_j\)&lt;/span&gt;;假设这个过程是通过投硬币完成的（正面概率为&lt;span class=&quot;math inline&quot;&gt;\(\pi=P(L_j=1)\)&lt;/span&gt; )，正式地来说，就是服从贝努利分布 &lt;span class=&quot;math display&quot;&gt;\[\begin{equation}L_j \sim Bernoulli(\pi)\end{equation}\]&lt;/span&gt; 然后，对于文档上&lt;span class=&quot;math inline&quot;&gt;\(R_j\)&lt;/span&gt;个“词位”中的每一个，我们根据一个概率分布&lt;span class=&quot;math inline&quot;&gt;\(\theta\)&lt;/span&gt;，随机独立地抽样一个词&lt;span class=&quot;math inline&quot;&gt;\(w_i\)&lt;/span&gt;。因为每个类生成词的&lt;span class=&quot;math inline&quot;&gt;\(\theta\)&lt;/span&gt;分布都不同，所以应该有&lt;span class=&quot;math inline&quot;&gt;\(\theta_1\)&lt;/span&gt;和&lt;span class=&quot;math inline&quot;&gt;\(\theta_2\)&lt;/span&gt;，具体地生成词的时候，我们根据文档的标签&lt;span class=&quot;math inline&quot;&gt;\(L_j\)&lt;/span&gt;来决定由哪个类来生成 &lt;span class=&quot;math display&quot;&gt;\[\begin{equation}
W_j \sim Multinomial(R_j,\theta_{L_j})
\end{equation}\]&lt;/span&gt;
    
    </summary>
    
      <category term="学术" scheme="http://crescentluna.github.io/categories/%E5%AD%A6%E6%9C%AF/"/>
    
      <category term="Gibbs Sampling笔记" scheme="http://crescentluna.github.io/categories/%E5%AD%A6%E6%9C%AF/Gibbs-Sampling%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Gibbs Sampling" scheme="http://crescentluna.github.io/tags/Gibbs-Sampling/"/>
    
  </entry>
  
  <entry>
    <title>《Gibbs Sampling for the UniniTiated》阅读笔记(上)---参数估计方法及Gibbs Sampling简介</title>
    <link href="http://crescentluna.github.io/2013/06/29/Gibbs%20Sampling%20for%20the%20UniniTiated-1/"/>
    <id>http://crescentluna.github.io/2013/06/29/Gibbs Sampling for the UniniTiated-1/</id>
    <published>2013-06-29T00:59:53.000Z</published>
    <updated>2017-02-21T18:24:27.407Z</updated>
    
    <content type="html"><![CDATA[<p>前一阵子折腾的事儿太多，写了点东西都没有传上来，是我偷懒了- -，下不为例。</p>
<p>这篇文章基本上是来自于《Gibbs Sampling for the UniniTiated》，说是笔记其实和翻译也差不多了。</p>
<p>整个结构分为上中下三部分：</p>
<ol style="list-style-type: decimal">
<li> <a href="http://www.crescentmoon.info/?p=504" target="_blank" rel="external">参数估计方法及Gibbs Sampling简介</a></li>
<li><a href="http://www.crescentmoon.info/?p=525" target="_blank" rel="external">一个朴素贝叶斯文档模型例子</a></li>
<li><a href="http://www.crescentmoon.info/?p=548" target="_blank" rel="external">连续型参数求积分的思考</a></li>
</ol>
<hr>

<p>这篇是上部分，介绍基础参数估计和Gibbs Sampling概念。</p>
<h2 id="为什么求积分参数估计方法">为什么求积分—参数估计方法</h2>
<p>很多概率模型的算法并不需要使用积分，只要对概率求和就行了（比如隐马尔科夫链的Baum-Welch算法），那么什么时候用到求积分呢？—— 当为了获得概率密度估计的时候，比如说根据一句话前面部分的文本估计下一个词的概率，根据email的内容估计它是否是垃圾邮件的概率等等。为了估计概率密度，一般有MLE（最大似然估计），MAP（最大后验估计），bayesian estimation（贝叶斯估计）三种方法。</p>
<h3 id="最大似然估计">最大似然估计</h3>
<p>这里举一个例子来讲最大似然估计。假设我们有一个硬币，它扔出正面的概率<span class="math inline">\(\pi\)</span>不确定，我们扔了10次，结果为HHHHTTTTTT（H为正面，T为反面）。利用最大似然估计的话，很容易得到下一次为正面的概率为0.4,因为它估计的是使观察数据产生的概率最大的参数。 <a href="http://7sbo5n.com1.z0.glb.clouddn.com/first.png" target="_blank" rel="external"><img src="http://7sbo5n.com1.z0.glb.clouddn.com/first.png" alt="first"></a></p>
<p>令<span class="math inline">\(\chi=\{HHHHTTTTTT\}\)</span>代表观察到的数据,<span class="math inline">\(y\)</span>为下一次抛硬币可能的结果,估计公式如下: <span class="math display">\[\begin{equation}\begin{split}\tilde{\pi}_{MLE} &amp;=\arg \max \limits_{\pi}P(\chi|\pi) \\P(y|\chi) &amp; \approx \int_{\pi} p(y|\tilde{\pi}_{MLE})P(\pi|\chi) d\pi = p(y|\tilde{\pi}_{MLE})\end{split}\end{equation}\]</span></p>
<a id="more"></a>
<h3 id="最大后验估计">最大后验估计</h3>
<p>最大似然估计是直接最大化似然函数对参数进行估计。如果我们有一些关于硬币的先验知识的话（比如我们知道参数<span class="math inline">\(\pi\)</span>服从某个形式的概率分布)，那么根据贝叶斯公式，我们就能求得观察到数据之后<span class="math inline">\(\pi\)</span>的后验概率，令后验概率最大化 <span class="math display">\[\begin{equation}
\begin{split}\tilde{\pi}_{MAP} &amp; =\arg \max \limits_{\pi}P(\pi|\chi) \\ &amp; =\arg \max \limits_{\pi} \frac{P(\chi|\pi)P(\pi)}{P(\chi)} \\ &amp; =\arg \max \limits_{\pi} P(\chi|\pi)P(\pi) \\P(y|\chi) &amp; \approx \int_{\pi} p(y|\tilde{\pi}_{MAP})P(\pi|\chi) d\pi =p(y|\tilde{\pi}_{MAP}) \\\end{split}\end{equation}\]</span> 这里<span class="math inline">\(P(\chi)\)</span>与参数<span class="math inline">\(\pi\)</span>无关就省略了。假设我们取一个令<span class="math inline">\(P(\pi)\)</span>的期望为0.5的先验分布，那么之后观察到的数据将对之前假设硬币正反概率一样的bias 产生影响。</p>
<h3 id="贝叶斯估计">贝叶斯估计</h3>
<p>首先我们可以看到,最大似然估计和最大后验估计都是基于一个假设，即把待估计的参数<span class="math inline">\(\pi\)</span>看做是一个固定的值，只是其取值未知。而最大似然是最简单的形式，其假定参数虽然未知，但是是确定值，就是找到使得样本对数似然分布最大的参数。而最大后验，只是优化函数为后验概率形式，多了一个先验概率项。 而贝叶斯估计和二者最大的不同在于，它假定参数是一个随机的变量，不是确定值。在样本分布<span class="math inline">\(P(\pi|\chi)\)</span> 上，<span class="math inline">\(\pi\)</span>是有可能取从0到1的任意一个值的，只是取到的概率不同。而MAP和MLE只取了整个概率分布<span class="math inline">\(P(\pi|\chi)\)</span> 上的一个点，丢失了一些观察到的数据<span class="math inline">\(\chi\)</span>给予的信息（这也就是经典统计学派和贝叶斯学派最大的分歧所在。）</p>
<p>为了利用所有的信息，我们可以对参数的概率分布求期望值。对于一个离散型变量<span class="math inline">\(z\)</span>的函数<span class="math inline">\(f(z)\)</span>的期望一般是这样求得 <span class="math display">\[\begin{equation}
E[f(z)]=\sum_{z\in\mathcal{Z}}f(z)p(z)
\end{equation}\]</span> 这里<span class="math inline">\(\mathcal{Z}\)</span>是所有<span class="math inline">\(z\)</span>可能取的值，<span class="math inline">\(p(z)\)</span>是取这个值的概率。如果<span class="math inline">\(z\)</span> 是个连续型的变量，期望就是求积分而不是求和了: <span class="math display">\[\begin{equation}
E[f(z)]=\int f(z)p(z) dz
\end{equation}\]</span> 对于这里的例子来说,<span class="math inline">\(z=\pi\)</span>,函数<span class="math inline">\(f(z)=P(y|\pi)\)</span>（这里文章讲的不清楚，这个<span class="math inline">\(P(y|\pi)\)</span>是模型生成该结果的概率，而<span class="math inline">\(P(\pi|\chi)\)</span>是使用这个模型的概率，比如说有3个模型，第一个模型生成该数据的概率为0.5，第二个为0.4，第三个为0.3，需要考虑模型的结果），需要取期望的概率分布为<span class="math inline">\(P(\pi|\chi)\)</span>，所以整个期望是模型的所有分布上生成该数据的概率 <span class="math display">\[\begin{equation}
P(y|\chi)= \int P(y|\pi)P(\pi|\chi)d \pi
\end{equation}\]</span> 对<span class="math inline">\(P(\pi|\chi)\)</span>我们使用贝叶斯公式 <span class="math display">\[\begin{equation}
P(\pi|\chi)=\frac{P(\chi|\pi)P(\pi)}{P(\chi)}=\frac{P(\chi|\pi)P(\pi)}{\int_\pi P(\chi|\pi)P(\pi)d \pi}
\end{equation}\]</span> 要注意这里的后验概率是个函数，而不是像之前的MAP一样是个值，它充分考虑了<span class="math inline">\(\pi\)</span>的先验知识和根据观察数据<span class="math inline">\(\chi\)</span>获得的信息，并将它们联系起来。</p>
<h2 id="为什么抽样gibbs-sampling">为什么抽样–Gibbs-Sampling</h2>
<p>积分存在的一个问题就是他们很难算。事实上，之前的后验概率里面的分母很可能没有解析解，这个时候就要用抽样的方法获得这个概率分布的具体形式了。</p>
<h3 id="monte-carlo算法">Monte Carlo算法</h3>
<p>Gibbs 抽样是Markov Chain Monte Carlo(蒙特卡洛方法）的一种。所谓的蒙特卡洛方法就是模拟统计的方法，举一个例子：假设有一个正方形和它的内接圆，然后我们随机地往正方形上撒很多很多的米粒，最后统计那些在圆里面的米粒数据（记作C），然后那些在正方形里的米粒数目（记作S)。可以看到两者之比近似于圆和正方形的面积之比： <span class="math display">\[\begin{equation}
\frac{C}{S} \approx \frac{\pi(\frac{d}{2})^2}{d^2}
\end{equation}\]</span> 这样我们就可以得到<span class="math inline">\(\pi \approx \frac{4C}{S}\)</span>，这是一个典型的通过模拟来积分的例子，这里的圆面积是通过无数个点加和来逼近真实面积的。</p>
<p>在这个例子里，我们是对一个均匀分布采用来求值。回到刚才我们的问题，是要计算期望值<span class="math inline">\(E_{p(x)}[f(x)]\)</span>，这里我们未知的是概率分布<span class="math inline">\(p(x)\)</span>，假设它不是一个均匀分布，而且很难获得解析解。 <a href="http://7sbo5n.com1.z0.glb.clouddn.com/second.png" target="_blank" rel="external"><img src="http://7sbo5n.com1.z0.glb.clouddn.com/second.png" alt="second"></a> 图2给出了<span class="math inline">\(f(z)\)</span>和<span class="math inline">\(p(z)\)</span>的例子。从概念上来说，之前的积分应该是在<span class="math inline">\(z\)</span>的所有空间上对<span class="math inline">\(f(z)p(z)\)</span>加和所得到的结果。换一种角度来看，如果我们从<span class="math inline">\(p(z)\)</span>随机依次抽取<span class="math inline">\(N\)</span> 个点<span class="math inline">\(z^{(0)},z^{(1)},z^{(2)},\ldots,z^{(N)}\)</span>,当<span class="math inline">\(N\)</span>趋向于无穷的时候，我们可以通过无数个采样获得的点<span class="math inline">\(z\)</span> 来逼近它真实的概率分布，就有 <span class="math display">\[\begin{equation}
E_{p(x)}[f(x)]=\lim_{N\rightarrow \infty}\frac{1}{N}\sum^{N}_{t=1}f(z^{(t)})
\end{equation}\]</span> 当<span class="math inline">\(z\)</span>是离散变量的时候，<span class="math inline">\(f(z)\)</span>的期望就是加权平均，每个<span class="math inline">\(z\)</span>的权值就是它的概率。当<span class="math inline">\(z\)</span>是连续变量是，我们也可以用类似的想法，对于所有采样得到<span class="math inline">\(z\)</span>，我们都用观察到的频率<span class="math inline">\(\frac{1}{N}count(z)(N \rightarrow \infty)\)</span>代替真实的概率<span class="math inline">\(p(z)\)</span>，所以<span class="math inline">\(p(z)\)</span>就隐含在采样得到的样本之中（这个想法是直观上的，因为实际上连续变量统计样本<span class="math inline">\(z\)</span>出现的次数<span class="math inline">\(count(z)\)</span>是没有意义的）。很容易可以发现，在整个<span class="math inline">\(p(z)\)</span> 的分布上，概率大的地方被采样的次数也多。</p>
<p>上面的式子<span class="math inline">\(N\)</span>是趋向于无穷大的，我们也可以使用有限数目<span class="math inline">\(T\)</span>的点来获得一个比较近似的值。 <span class="math display">\[\begin{equation}
E_{p(x)}[f(x)]\approx \frac{1}{T}\sum^{T}_{t=1}f(z^{(t)})
\end{equation}\]</span> 好，现在我们有近似获得积分值的方法了。剩下的问题就是，如何根据<span class="math inline">\(p(z)\)</span>采样出样本<span class="math inline">\(z^{(0)},z^{(1)},z^{(2)},\ldots,z^{(T)}\)</span>。这正好是模拟统计所研究的问题，所以有很多很多的采样方法，比如rejection sampling ,adaptive sampling, important sampling等等(见PRML），而我们采样的方法把<span class="math inline">\(z\)</span> 看做状态空间中的点，用从<span class="math inline">\(z^{(0)}\)</span> 转移到<span class="math inline">\(z^{(1)}\)</span> 再转移到<span class="math inline">\(z^{(2)}\)</span> 这样的方式遍历<span class="math inline">\(z\)</span> 的状态空间，见图2。</p>
<p>可以看到<span class="math inline">\(Z\)</span>的状态转移是一条马尔科夫链，这里<span class="math inline">\(g\)</span>是一个根据转移概率<span class="math inline">\(P_{trans}(z^{(t+1)}|z^{(0)},z^{(1)},\ldots,z^{(t)})\)</span>来决定下一个状态是什么的函数。由Markov Chain 的性质，我们可以知道下一个状态<span class="math inline">\(z^{(t+1)}\)</span> 仅仅取决于当前状态<span class="math inline">\(z^{(t)}\)</span>。 <span class="math display">\[\begin{equation}
P_{trans}(z^{(t+1)}|z^{(0)},z^{(1)},\ldots,z^{(t)})=P_{trans}(z^{(t+1)}|z^{(t)})
\end{equation}\]</span> 而Markov Chain Monte Carlo方法的核心就在于如何设计这个函数<span class="math inline">\(g\)</span>使得访问状态<span class="math inline">\(z\)</span>的概率刚好是<span class="math inline">\(p(z)\)</span>，这就需要转移概率<span class="math inline">\(P_{trans}\)</span>满足一定的条件（平稳细致条件，详见PRML的第四章)，而Gibbs sampling就是满足该条件的抽样方法之一。</p>
<h3 id="gibbs-sampling算法">Gibbs sampling算法</h3>
<p>假设每个点 <span class="math inline">\(z=&lt;z_1^{0},\ldots,z_k^{0}&gt;(k&gt;1)\)</span>。Gibbs Sampling 每次在确定下一个状态的时候，并不是一次性地确定所有维度上的值，而是选取一个维度，通过剩下的<span class="math inline">\(k-1\)</span>个维度来确定这个维度的值，见图2。</p>
<p>通过条件概率的定义我们可以得到 <span class="math display">\[\begin{equation}
\begin{split}
&amp; P(Z_i|z_1^{(t+1)},\ldots,z^{(t+1)}_{i-1},z^{(t)}_{i+1},\ldots,z^{t}_k) \\=&amp;\frac{P(z_1^{(t+1)},\ldots,z^{(t+1)}_{i-1},z^{(t)}_{i},z^{(t)}_{i+1},\ldots,z^{(t)}_k)}{P(z_1^{(t+1)},\ldots,z^{(t+1)}_{i-1},z^{(t)}_{i+1},\ldots,z^{(t)}_k)}\\\end{split}
\end{equation}\]</span> 注意右式的分子部分是全联合概率，而分母部分少了<span class="math inline">\(z^{(t)}_i\)</span>这个维度，也就是我们要估计的这个维度。把这个操作对每个维度都采样一遍我们就得到了下一个新的点<span class="math inline">\(z^{(t+1)}=g(z^{(t)})=&lt;z_1^{(t+1)},\ldots,z_k^{(t+1)}&gt;\)</span>。要注意的是每次新采样得到的值都可以直接用于下一次采样，所以公式里<span class="math inline">\(z^{(t)}_{i+1}\)</span>前面的维度上标都是<span class="math inline">\(t+1\)</span>，因为他们都刚刚被采样过。</p>
<h2 id="吐槽的部分">吐槽的部分</h2>
<p>一般文章对Gibbs Sampling的介绍也就到此为止了，这样的介绍对新手来说（比如我- -)太难了,讲了也不会用。比如以下问题：</p>
<ul>
<li><p>对特定的模型Gibbs Sampling条件概率的分布采样到底是咋做的？</p></li>
<li><p>连续型的参数变量如何处理？</p></li>
<li><p>只做<span class="math inline">\(T\)</span>次循环怎么确保获得你想要的期望值？</p></li>
</ul>
<p>于是这篇文章又生动活泼地举了一个从朴素贝叶斯模型生成Gibbs 采样器的例子。（见中篇）</p>
<p>参考文献：</p>
<ol style="list-style-type: decimal">
<li><p>《Pattern Recognition and Machine Learning》</p></li>
<li><p>《LDA数学八卦》</p></li>
<li><p>http://www.xperseverance.net/blogs/2013/03/1682/</p></li>
<li><p>《Gibbs Sampling for the UniniTiated》</p></li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;前一阵子折腾的事儿太多，写了点东西都没有传上来，是我偷懒了- -，下不为例。&lt;/p&gt;
&lt;p&gt;这篇文章基本上是来自于《Gibbs Sampling for the UniniTiated》，说是笔记其实和翻译也差不多了。&lt;/p&gt;
&lt;p&gt;整个结构分为上中下三部分：&lt;/p&gt;
&lt;ol style=&quot;list-style-type: decimal&quot;&gt;
&lt;li&gt; &lt;a href=&quot;http://www.crescentmoon.info/?p=504&quot;&gt;参数估计方法及Gibbs Sampling简介&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.crescentmoon.info/?p=525&quot;&gt;一个朴素贝叶斯文档模型例子&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.crescentmoon.info/?p=548&quot;&gt;连续型参数求积分的思考&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;/hr&gt;
&lt;p&gt;这篇是上部分，介绍基础参数估计和Gibbs Sampling概念。&lt;/p&gt;
&lt;h2 id=&quot;为什么求积分参数估计方法&quot;&gt;为什么求积分—参数估计方法&lt;/h2&gt;
&lt;p&gt;很多概率模型的算法并不需要使用积分，只要对概率求和就行了（比如隐马尔科夫链的Baum-Welch算法），那么什么时候用到求积分呢？—— 当为了获得概率密度估计的时候，比如说根据一句话前面部分的文本估计下一个词的概率，根据email的内容估计它是否是垃圾邮件的概率等等。为了估计概率密度，一般有MLE（最大似然估计），MAP（最大后验估计），bayesian estimation（贝叶斯估计）三种方法。&lt;/p&gt;
&lt;h3 id=&quot;最大似然估计&quot;&gt;最大似然估计&lt;/h3&gt;
&lt;p&gt;这里举一个例子来讲最大似然估计。假设我们有一个硬币，它扔出正面的概率&lt;span class=&quot;math inline&quot;&gt;\(\pi\)&lt;/span&gt;不确定，我们扔了10次，结果为HHHHTTTTTT（H为正面，T为反面）。利用最大似然估计的话，很容易得到下一次为正面的概率为0.4,因为它估计的是使观察数据产生的概率最大的参数。 &lt;a href=&quot;http://7sbo5n.com1.z0.glb.clouddn.com/first.png&quot;&gt;&lt;img src=&quot;http://7sbo5n.com1.z0.glb.clouddn.com/first.png&quot; alt=&quot;first&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;令&lt;span class=&quot;math inline&quot;&gt;\(\chi=\{HHHHTTTTTT\}\)&lt;/span&gt;代表观察到的数据,&lt;span class=&quot;math inline&quot;&gt;\(y\)&lt;/span&gt;为下一次抛硬币可能的结果,估计公式如下: &lt;span class=&quot;math display&quot;&gt;\[\begin{equation}\begin{split}\tilde{\pi}_{MLE} &amp;amp;=\arg \max \limits_{\pi}P(\chi|\pi) \\P(y|\chi) &amp;amp; \approx \int_{\pi} p(y|\tilde{\pi}_{MLE})P(\pi|\chi) d\pi = p(y|\tilde{\pi}_{MLE})\end{split}\end{equation}\]&lt;/span&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="学术" scheme="http://crescentluna.github.io/categories/%E5%AD%A6%E6%9C%AF/"/>
    
      <category term="Gibbs Sampling笔记" scheme="http://crescentluna.github.io/categories/%E5%AD%A6%E6%9C%AF/Gibbs-Sampling%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Gibbs Sampling" scheme="http://crescentluna.github.io/tags/Gibbs-Sampling/"/>
    
      <category term="参数估计" scheme="http://crescentluna.github.io/tags/%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1/"/>
    
  </entry>
  
  <entry>
    <title>SyntaxHighlighter支持高亮的语言列表</title>
    <link href="http://crescentluna.github.io/2013/04/11/SyntaxHighlighter%E6%94%AF%E6%8C%81%E9%AB%98%E4%BA%AE%E7%9A%84%E8%AF%AD%E8%A8%80%E5%88%97%E8%A1%A8/"/>
    <id>http://crescentluna.github.io/2013/04/11/SyntaxHighlighter支持高亮的语言列表/</id>
    <published>2013-04-10T23:48:31.000Z</published>
    <updated>2017-02-21T18:24:27.407Z</updated>
    
    <content type="html"><![CDATA[<p>SyntaxHighlighter是个WordPress（也支持别的博客）上非常好用的语法高亮插件，但是在该插件的设置里没给出默认支持的高亮语言列表<del>（让你自己去官网上找</del>）。查了一下插件的文件，发现除去官网上所列的以外，还支持latex和matlab：</p>
<a id="more"></a>
<h2 id="语言-缩写">语言 缩写</h2>
<ul>
<li>‘as3’ =&gt; ‘as3’,</li>
<li>‘actionscript3’ =&gt; ‘as3’,</li>
<li>‘bash’ =&gt; ‘bash’,</li>
<li>‘shell’ =&gt; ‘bash’,</li>
<li>‘coldfusion’ =&gt; ‘coldfusion’,</li>
<li>‘cf’ =&gt; ‘coldfusion’,</li>
<li>‘clojure’ =&gt; ‘clojure’,</li>
<li>‘clj’ =&gt; ‘clojure’,</li>
<li>‘cpp’ =&gt; ‘cpp’,</li>
<li>‘c’ =&gt; ‘cpp’,</li>
<li>‘c-sharp’ =&gt; ‘csharp’,</li>
<li>‘csharp’ =&gt; ‘csharp’,</li>
<li>‘css’ =&gt; ‘css’,</li>
<li>‘delphi’ =&gt; ‘delphi’,</li>
<li>‘pas’ =&gt; ‘delphi’,</li>
<li>‘pascal’ =&gt; ‘delphi’,</li>
<li>‘diff’ =&gt; ‘diff’,</li>
<li>‘patch’ =&gt; ‘diff’,</li>
<li>‘erl’ =&gt; ‘erlang’,</li>
<li>‘erlang’ =&gt; ‘erlang’,</li>
<li>‘fsharp’ =&gt; ‘fsharp’,</li>
<li>‘groovy’ =&gt; ‘groovy’,</li>
<li>‘java’ =&gt; ‘java’,</li>
<li>‘jfx’ =&gt; ‘javafx’,</li>
<li>‘javafx’ =&gt; ‘javafx’,</li>
<li>‘js’ =&gt; ‘jscript’,</li>
<li>‘jscript’ =&gt; ‘jscript’,</li>
<li>‘javascript’ =&gt; ‘jscript’,</li>
<li>‘latex’ =&gt; ‘latex’, // Not used as a shortcode</li>
<li>‘tex’ =&gt; ‘latex’,</li>
<li>‘matlab’ =&gt; ‘matlabkey’,</li>
<li>‘objc’ =&gt; ‘objc’,</li>
<li>‘obj-c’ =&gt; ‘objc’,</li>
<li>‘perl’ =&gt; ‘perl’,</li>
<li>‘pl’ =&gt; ‘perl’,</li>
<li>‘php’ =&gt; ‘php’,</li>
<li>‘plain’ =&gt; ‘plain’,</li>
<li>‘text’ =&gt; ‘plain’,</li>
<li>‘ps’ =&gt; ‘powershell’,</li>
<li>‘powershell’ =&gt; ‘powershell’,</li>
<li>‘py’ =&gt; ‘python’,</li>
<li>‘python’ =&gt; ‘python’,</li>
<li>‘r’ =&gt; ‘r’, // Not used as a shortcode</li>
<li>‘splus’ =&gt; ‘r’,</li>
<li>‘rails’ =&gt; ‘ruby’,</li>
<li>‘rb’ =&gt; ‘ruby’,</li>
<li>‘ror’ =&gt; ‘ruby’,</li>
<li>‘ruby’ =&gt; ‘ruby’,</li>
<li>‘scala’ =&gt; ‘scala’,</li>
<li>‘sql’ =&gt; ‘sql’,</li>
<li>‘vb’ =&gt; ‘vb’,</li>
<li>‘vbnet’ =&gt; ‘vb’,</li>
<li>‘xml’ =&gt; ‘xml’,</li>
<li>‘xhtml’ =&gt; ‘xml’,</li>
<li>‘xslt’ =&gt; ‘xml’,</li>
<li>‘html’ =&gt; ‘xml’,</li>
<li>‘xhtml’ =&gt; ‘xml’,</li>
</ul>
<p>当然只仅仅是默认支持的语言，还有更多的语言需要自己改代码添加（<a href="http://www.undermyhat.org/blog/2009/09/list-of-brushes-syntaxhighligher/" target="_blank" rel="external">支持语言列表</a>）。 添加的方法请见<a href="http://www.viper007bond.com/wordpress-plugins/syntaxhighlighter/adding-a-new-brush-language/" target="_blank" rel="external">Adding A New Brush (Language)</a>。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;SyntaxHighlighter是个WordPress（也支持别的博客）上非常好用的语法高亮插件，但是在该插件的设置里没给出默认支持的高亮语言列表&lt;del&gt;（让你自己去官网上找&lt;/del&gt;）。查了一下插件的文件，发现除去官网上所列的以外，还支持latex和matlab：&lt;/p&gt;
    
    </summary>
    
      <category term="杂项" scheme="http://crescentluna.github.io/categories/%E6%9D%82%E9%A1%B9/"/>
    
    
      <category term="wordpress" scheme="http://crescentluna.github.io/tags/wordpress/"/>
    
  </entry>
  
  <entry>
    <title>高斯混合模型的matlab实现（转）</title>
    <link href="http://crescentluna.github.io/2013/04/11/%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B%E7%9A%84matlab%E5%AE%9E%E7%8E%B0%EF%BC%88%E8%BD%AC%EF%BC%89/"/>
    <id>http://crescentluna.github.io/2013/04/11/高斯混合模型的matlab实现（转）/</id>
    <published>2013-04-10T23:20:16.000Z</published>
    <updated>2017-02-21T18:24:27.407Z</updated>
    
    <content type="html"><![CDATA[<p>高斯混合函数实现部分是基本上是转载的的pluskid大神<a href="http://blog.pluskid.org/?p=39" target="_blank" rel="external">文章里</a>的里的代码，加了一点注释，并根据他给的<a href="http://freemind.pluskid.org/machine-learning/regularized-gaussian-covariance-estimation/#7de08bf962fca45b9699432818b939067d7c7327" target="_blank" rel="external">方法二</a>解决 covariance 矩阵 singular 的问题。</p>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">function</span> <span class="title">varargout</span> = <span class="title">gmm</span><span class="params">(X, K_or_centroids)</span></span></div><div class="line"><span class="comment">% ============================================================</span></div><div class="line"><span class="comment">%转载自http://blog.pluskid.org/?p=39</span></div><div class="line"><span class="comment">% Expectation-Maximization iteration implementation of</span></div><div class="line"><span class="comment">% Gaussian Mixture Model.</span></div><div class="line"><span class="comment">%</span></div><div class="line"><span class="comment">% PX = GMM(X, K_OR_CENTROIDS)</span></div><div class="line"><span class="comment">% [PX MODEL] = GMM(X, K_OR_CENTROIDS)</span></div><div class="line"><span class="comment">%</span></div><div class="line"><span class="comment">%  - X: N-by-D data matrix.%需要注意的是这里的X包括了全部</span></div><div class="line"><span class="comment">%  - K_OR_CENTROIDS: either K indicating the number of</span></div><div class="line"><span class="comment">%       components or a K-by-D matrix indicating the</span></div><div class="line"><span class="comment">%       choosing of the initial K centroids.</span></div><div class="line"><span class="comment">%</span></div><div class="line"><span class="comment">%  - PX: N-by-K matrix indicating the probability of each</span></div><div class="line"><span class="comment">%       component generating each point.</span></div><div class="line"><span class="comment">%  - MODEL: a structure containing the parameters for a GMM:</span></div><div class="line"><span class="comment">%       MODEL.Miu: a K-by-D matrix.</span></div><div class="line"><span class="comment">%       MODEL.Sigma: a D-by-D-by-K matrix.</span></div><div class="line"><span class="comment">%       MODEL.Pi: a 1-by-K vector.</span></div><div class="line"><span class="comment">% ============================================================</span></div><div class="line">    threshold = <span class="number">1e-15</span>;</div><div class="line">    [N, D] = <span class="built_in">size</span>(X);</div><div class="line">    </div><div class="line">    <span class="keyword">if</span> <span class="built_in">isscalar</span>(K_or_centroids)</div><div class="line">        K = K_or_centroids;</div><div class="line">        <span class="comment">% randomly pick centroids</span></div><div class="line">        rndp = randperm(N);</div><div class="line">        centroids = X(rndp(<span class="number">1</span>:K),:);</div><div class="line">    <span class="keyword">else</span></div><div class="line">        K = <span class="built_in">size</span>(K_or_centroids, <span class="number">1</span>);</div><div class="line">        centroids = K_or_centroids;</div><div class="line">    <span class="keyword">end</span></div><div class="line">    </div><div class="line">    <span class="comment">% initial values</span></div><div class="line">    [pMiu pPi pSigma] = init_params();</div><div class="line">        </div><div class="line">    Lprev = -<span class="built_in">inf</span>;</div><div class="line">    <span class="keyword">while</span> true</div><div class="line">        Px = calc_prob();<span class="comment">%计算N(x|mu,sigma)</span></div><div class="line">        </div><div class="line">        <span class="comment">% new value for pGamma</span></div><div class="line">        pGamma = Px .* <span class="built_in">repmat</span>(pPi, N, <span class="number">1</span>);<span class="comment">%估计 gamma 是个N*K的矩阵</span></div><div class="line">        pGamma = pGamma ./ <span class="built_in">repmat</span>(sum(pGamma, <span class="number">2</span>), <span class="number">1</span>, K);<span class="comment">%对矩阵的理解真是出神入化,</span></div><div class="line">   </div><div class="line">        <span class="comment">% new value for parameters of each Component</span></div><div class="line">        Nk = sum(pGamma, <span class="number">1</span>);<span class="comment">%N_K</span></div><div class="line">        pMiu = <span class="built_in">diag</span>(<span class="number">1.</span>/Nk) * pGamma' * X;          <span class="comment">%数字 *( K-by-N * N-by-D)加个括号有助理解</span></div><div class="line">        pPi = Nk/N;</div><div class="line">        <span class="keyword">for</span> kk = <span class="number">1</span>:K</div><div class="line">            Xshift = X-<span class="built_in">repmat</span>(pMiu(kk, : ), N, <span class="number">1</span>);<span class="comment">%x-u</span></div><div class="line">            pSigma(:, :, kk) = (Xshift' * ...</div><div class="line">                (<span class="built_in">diag</span>(pGamma(:, kk)) * Xshift)) / Nk(kk);<span class="comment">%更新sigma</span></div><div class="line">   </div><div class="line">             <span class="keyword">end</span></div><div class="line">        <span class="comment">% check for convergence</span></div><div class="line">        L = sum(<span class="built_in">log</span>(Px*pPi'));</div><div class="line">        <span class="keyword">if</span> L-Lprev &lt; threshold</div><div class="line">            <span class="keyword">break</span>;</div><div class="line">        <span class="keyword">end</span></div><div class="line">        Lprev = L;</div><div class="line">    <span class="keyword">end</span></div><div class="line">    </div><div class="line">    <span class="keyword">if</span> nargout == <span class="number">1</span></div><div class="line">        varargout = &#123;Px&#125;;</div><div class="line">    <span class="keyword">else</span></div><div class="line">        model = [];</div><div class="line">        model.Miu = pMiu;</div><div class="line">        model.Sigma = pSigma;</div><div class="line">        model.Pi = pPi;</div><div class="line">        varargout = &#123;pGamma, model&#125;;<span class="comment">%注意！！！！！这里和大神代码不同，他返回的是px，而我是 pGamma</span></div><div class="line">    <span class="keyword">end</span></div><div class="line">        </div><div class="line">    <span class="function"><span class="keyword">function</span> <span class="params">[pMiu pPi pSigma]</span> = <span class="title">init_params</span><span class="params">()</span>%初始化参数</span></div><div class="line">        pMiu = centroids;<span class="comment">% K-by-D matrix</span></div><div class="line">        pPi = <span class="built_in">zeros</span>(<span class="number">1</span>, K);<span class="comment">%1-by-K matrix</span></div><div class="line">        pSigma = <span class="built_in">zeros</span>(D, D, K);<span class="comment">%</span></div><div class="line">        </div><div class="line">        <span class="comment">% hard assign x to each centroids</span></div><div class="line">        distmat = <span class="built_in">repmat</span>(sum(X.*X, <span class="number">2</span>), <span class="number">1</span>, K) + ... <span class="comment">% X is a N-by-D data matrix.</span></div><div class="line">            <span class="built_in">repmat</span>(sum(pMiu.*pMiu, <span class="number">2</span>)', N, <span class="number">1</span>) - ...<span class="comment">% X-&gt;K列 U-&gt;N行 XU^T is N-by-K</span></div><div class="line">            <span class="number">2</span>*X*pMiu';<span class="comment">%计算每个点到K个中心的距离</span></div><div class="line">        [~, labels] = min(distmat, [], <span class="number">2</span>);<span class="comment">%找到离X最近的pMiu，[C,I] labels代表这个最小值是从那列选出来的</span></div><div class="line">    </div><div class="line">        <span class="keyword">for</span> k=<span class="number">1</span>:K</div><div class="line">            Xk = X(labels == k, : );<span class="comment">% Xk是所有被归到K类的X向量构成的矩阵</span></div><div class="line">            pPi(k) = <span class="built_in">size</span>(Xk, <span class="number">1</span>)/N;<span class="comment">% 数一数几个归到K类的</span></div><div class="line">            pSigma(:, :, k) = cov(Xk); <span class="comment">%计算协方差矩阵，D-by-D matrix,最小方差无偏估计</span></div><div class="line">        <span class="keyword">end</span></div><div class="line">    <span class="keyword">end</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">function</span> <span class="title">Px</span> = <span class="title">calc_prob</span><span class="params">()</span></span></div><div class="line">        Px = <span class="built_in">zeros</span>(N, K);</div><div class="line">        <span class="keyword">for</span> k = <span class="number">1</span>:K</div><div class="line">            Xshift = X-<span class="built_in">repmat</span>(pMiu(k, : ), N, <span class="number">1</span>);<span class="comment">%x-u</span></div><div class="line">            lemda=<span class="number">1e-5</span>;</div><div class="line">            conv=pSigma(:, :, k)+lemda*<span class="built_in">diag</span>(<span class="built_in">diag</span>(<span class="built_in">ones</span>(D)));<span class="comment">%这里处理singular问题，为协方差矩阵加上一个很小lemda*I</span></div><div class="line">            inv_pSigma = inv(conv);<span class="comment">%协方差的逆</span></div><div class="line">            tmp = sum((Xshift*inv_pSigma) .* Xshift, <span class="number">2</span>);<span class="comment">%(X-U_k)sigma.*(X-U_k),tmp是个N*1的向量</span></div><div class="line">            coef = (<span class="number">2</span>*<span class="built_in">pi</span>)^(-D/<span class="number">2</span>) * <span class="built_in">sqrt</span>(det(inv_pSigma));<span class="comment">%前面的参数</span></div><div class="line">            Px(:, k) = coef * <span class="built_in">exp</span>(<span class="number">-0.5</span>*tmp);<span class="comment">%把数据点 x 带入到 Gaussian model 里得到的值</span></div><div class="line">        <span class="keyword">end</span></div><div class="line">    <span class="keyword">end</span></div><div class="line"><span class="keyword">end</span></div><div class="line"><span class="comment">%repmat 通过拓展向量到矩阵</span></div><div class="line"><span class="comment">%inv 求逆</span></div><div class="line"><span class="comment">%min 求矩阵最小值，可以返回标签</span></div><div class="line"><span class="comment">%X(labels == k, : ) 对行做筛选</span></div><div class="line"><span class="comment">% size(Xk, 1) 求矩阵的长或宽</span></div><div class="line"><span class="comment">%scatter 对二维向量绘图</span></div></pre></td></tr></table></figure>
<p><span style="color: #ff0000;">注意：</span></p>
<p>pluskid大神这里最后返回的是px，我觉得非常奇怪，因为PRML里对点做hard assignment时是根据后验概率来判别的。于是我在大神博客上问了一下，他的解释是最大似然和最大后验的区别，前者是挑x被各个模型产生的概率最大的那个，而后者加上了先验知识，各有道理。一句话就茅塞顿开，真大神也~ <a id="more"></a> 然后调用该函数对数据进行聚类，要是数据是二维的或三维的，顺便画个图。</p>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line">X=[...</div><div class="line"><span class="number">1</span> <span class="number">1</span> <span class="number">1.1</span>;</div><div class="line"><span class="number">1</span> <span class="number">2</span> <span class="number">1</span>;</div><div class="line"><span class="number">2</span> <span class="number">1</span> <span class="number">1</span>;</div><div class="line"><span class="number">2</span> <span class="number">2</span> <span class="number">1</span>;</div><div class="line"><span class="number">5</span> <span class="number">5</span> <span class="number">2</span>;</div><div class="line"><span class="number">5</span> <span class="number">6</span> <span class="number">2</span>;</div><div class="line"><span class="number">6</span> <span class="number">5</span> <span class="number">2</span>;</div><div class="line"><span class="number">6</span> <span class="number">6</span> <span class="number">2.1</span>]<span class="comment">%数据X自己改</span></div><div class="line">K=<span class="number">2</span></div><div class="line"><span class="comment">%以上设置数据点</span></div><div class="line">[Px,model]=gmm(X,K);<span class="comment">%这里得到结果</span></div><div class="line">[~,belong]=max(Px,[],<span class="number">2</span>)<span class="comment">%选概率最大的那个数，输出聚类结果</span></div><div class="line"><span class="comment">%以下绘图</span></div><div class="line">z=<span class="built_in">zeros</span>(<span class="built_in">size</span>(X,<span class="number">1</span>),<span class="number">3</span>);</div><div class="line"><span class="keyword">if</span> <span class="built_in">isscalar</span>(K)<span class="comment">%获得类别个数</span></div><div class="line">      K_number=K;</div><div class="line"><span class="keyword">else</span></div><div class="line">      K_number = <span class="built_in">size</span>(K, <span class="number">1</span>);</div><div class="line"><span class="keyword">end</span></div><div class="line"></div><div class="line"><span class="keyword">for</span> k=<span class="number">1</span>:K_number</div><div class="line">    color=[rand rand rand];<span class="comment">%建立颜色矩阵，随机给个颜色</span></div><div class="line">    csize=<span class="built_in">size</span>(z(belong==k,:),<span class="number">1</span>);<span class="comment">%数一数有几行</span></div><div class="line">    z(belong==k,:)=<span class="built_in">repmat</span>(color,csize,<span class="number">1</span>);<span class="comment">%对属于某一类下的点染色</span></div><div class="line"><span class="keyword">end</span></div><div class="line"><span class="keyword">if</span> <span class="built_in">size</span>(X,<span class="number">2</span>)==<span class="number">2</span> <span class="comment">%二维或三维的可以画一画</span></div><div class="line">figure(<span class="string">'color'</span>,<span class="string">'w'</span>);<span class="comment">%把背景改成白的</span></div><div class="line">scatter(X(:,<span class="number">1</span>),X(:,<span class="number">2</span>),<span class="number">30</span>,z)</div><div class="line">axis off;<span class="comment">%关掉坐标系显示</span></div><div class="line"><span class="keyword">else</span></div><div class="line"> <span class="keyword">if</span> <span class="built_in">size</span>(X,<span class="number">2</span>)==<span class="number">3</span> <span class="comment">%3维的情况</span></div><div class="line">    figure(<span class="string">'color'</span>,<span class="string">'w'</span>);<span class="comment">%把背景改成白的</span></div><div class="line">    scatter3(X(:,<span class="number">1</span>),X(:,<span class="number">2</span>),X(:,<span class="number">3</span>),<span class="number">30</span>,z,<span class="string">'filled'</span>)</div><div class="line"> <span class="keyword">end</span></div><div class="line"><span class="keyword">end</span></div></pre></td></tr></table></figure>
<p>下面是对<a href="http://archive.ics.uci.edu/ml/datasets/Iris" target="_blank" rel="external">鸢尾花数据集</a>聚类的结果。选了部分维度画图 二维图： <a href="http://7sbo5n.com1.z0.glb.clouddn.com/uu.jpg" target="_blank" rel="external"><img src="http://7sbo5n.com1.z0.glb.clouddn.com/uu.jpg" alt="uu"></a> 三维图： <a href="http://7sbo5n.com1.z0.glb.clouddn.com/untitled.jpg" target="_blank" rel="external"><img src="http://7sbo5n.com1.z0.glb.clouddn.com/untitled.jpg" alt="untitled"></a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;高斯混合函数实现部分是基本上是转载的的pluskid大神&lt;a href=&quot;http://blog.pluskid.org/?p=39&quot;&gt;文章里&lt;/a&gt;的里的代码，加了一点注释，并根据他给的&lt;a href=&quot;http://freemind.pluskid.org/machine-learning/regularized-gaussian-covariance-estimation/#7de08bf962fca45b9699432818b939067d7c7327&quot;&gt;方法二&lt;/a&gt;解决 covariance 矩阵 singular 的问题。&lt;/p&gt;
&lt;figure class=&quot;highlight matlab&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;8&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;9&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;10&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;11&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;12&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;13&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;14&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;15&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;16&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;17&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;18&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;19&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;20&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;21&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;22&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;23&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;24&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;25&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;26&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;27&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;28&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;29&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;30&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;31&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;32&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;33&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;34&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;35&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;36&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;37&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;38&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;39&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;40&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;41&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;42&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;43&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;44&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;45&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;46&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;47&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;48&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;49&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;50&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;51&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;52&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;53&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;54&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;55&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;56&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;57&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;58&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;59&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;60&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;61&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;62&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;63&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;64&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;65&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;66&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;67&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;68&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;69&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;70&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;71&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;72&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;73&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;74&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;75&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;76&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;77&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;78&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;79&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;80&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;81&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;82&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;83&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;84&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;85&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;86&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;87&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;88&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;89&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;90&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;91&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;92&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;93&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;94&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;95&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;96&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;97&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;98&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;99&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;100&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;101&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;102&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;103&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;104&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;105&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;106&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;107&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;108&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;109&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;110&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;function&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;varargout&lt;/span&gt; = &lt;span class=&quot;title&quot;&gt;gmm&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(X, K_or_centroids)&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;% ============================================================&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;%转载自http://blog.pluskid.org/?p=39&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;% Expectation-Maximization iteration implementation of&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;% Gaussian Mixture Model.&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;%&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;% PX = GMM(X, K_OR_CENTROIDS)&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;% [PX MODEL] = GMM(X, K_OR_CENTROIDS)&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;%&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;%  - X: N-by-D data matrix.%需要注意的是这里的X包括了全部&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;%  - K_OR_CENTROIDS: either K indicating the number of&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;%       components or a K-by-D matrix indicating the&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;%       choosing of the initial K centroids.&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;%&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;%  - PX: N-by-K matrix indicating the probability of each&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;%       component generating each point.&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;%  - MODEL: a structure containing the parameters for a GMM:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;%       MODEL.Miu: a K-by-D matrix.&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;%       MODEL.Sigma: a D-by-D-by-K matrix.&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;%       MODEL.Pi: a 1-by-K vector.&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;% ============================================================&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    threshold = &lt;span class=&quot;number&quot;&gt;1e-15&lt;/span&gt;;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    [N, D] = &lt;span class=&quot;built_in&quot;&gt;size&lt;/span&gt;(X);&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;built_in&quot;&gt;isscalar&lt;/span&gt;(K_or_centroids)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        K = K_or_centroids;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        &lt;span class=&quot;comment&quot;&gt;% randomly pick centroids&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        rndp = randperm(N);&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        centroids = X(rndp(&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;:K),:);&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;else&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        K = &lt;span class=&quot;built_in&quot;&gt;size&lt;/span&gt;(K_or_centroids, &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;);&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        centroids = K_or_centroids;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;end&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;span class=&quot;comment&quot;&gt;% initial values&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    [pMiu pPi pSigma] = init_params();&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        &lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    Lprev = -&lt;span class=&quot;built_in&quot;&gt;inf&lt;/span&gt;;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;while&lt;/span&gt; true&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        Px = calc_prob();&lt;span class=&quot;comment&quot;&gt;%计算N(x|mu,sigma)&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        &lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        &lt;span class=&quot;comment&quot;&gt;% new value for pGamma&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        pGamma = Px .* &lt;span class=&quot;built_in&quot;&gt;repmat&lt;/span&gt;(pPi, N, &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;);&lt;span class=&quot;comment&quot;&gt;%估计 gamma 是个N*K的矩阵&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        pGamma = pGamma ./ &lt;span class=&quot;built_in&quot;&gt;repmat&lt;/span&gt;(sum(pGamma, &lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;), &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;, K);&lt;span class=&quot;comment&quot;&gt;%对矩阵的理解真是出神入化,&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;   &lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        &lt;span class=&quot;comment&quot;&gt;% new value for parameters of each Component&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        Nk = sum(pGamma, &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;);&lt;span class=&quot;comment&quot;&gt;%N_K&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        pMiu = &lt;span class=&quot;built_in&quot;&gt;diag&lt;/span&gt;(&lt;span class=&quot;number&quot;&gt;1.&lt;/span&gt;/Nk) * pGamma&#39; * X;          &lt;span class=&quot;comment&quot;&gt;%数字 *( K-by-N * N-by-D)加个括号有助理解&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        pPi = Nk/N;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; kk = &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;:K&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;            Xshift = X-&lt;span class=&quot;built_in&quot;&gt;repmat&lt;/span&gt;(pMiu(kk, : ), N, &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;);&lt;span class=&quot;comment&quot;&gt;%x-u&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;            pSigma(:, :, kk) = (Xshift&#39; * ...&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;                (&lt;span class=&quot;built_in&quot;&gt;diag&lt;/span&gt;(pGamma(:, kk)) * Xshift)) / Nk(kk);&lt;span class=&quot;comment&quot;&gt;%更新sigma&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;   &lt;/div&gt;&lt;div class=&quot;line&quot;&gt;             &lt;span class=&quot;keyword&quot;&gt;end&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        &lt;span class=&quot;comment&quot;&gt;% check for convergence&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        L = sum(&lt;span class=&quot;built_in&quot;&gt;log&lt;/span&gt;(Px*pPi&#39;));&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; L-Lprev &amp;lt; threshold&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;            &lt;span class=&quot;keyword&quot;&gt;break&lt;/span&gt;;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;end&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        Lprev = L;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;end&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; nargout == &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        varargout = &amp;#123;Px&amp;#125;;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;else&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        model = [];&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        model.Miu = pMiu;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        model.Sigma = pSigma;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        model.Pi = pPi;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        varargout = &amp;#123;pGamma, model&amp;#125;;&lt;span class=&quot;comment&quot;&gt;%注意！！！！！这里和大神代码不同，他返回的是px，而我是 pGamma&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;end&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        &lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;function&lt;/span&gt; &lt;span class=&quot;params&quot;&gt;[pMiu pPi pSigma]&lt;/span&gt; = &lt;span class=&quot;title&quot;&gt;init_params&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;()&lt;/span&gt;%初始化参数&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        pMiu = centroids;&lt;span class=&quot;comment&quot;&gt;% K-by-D matrix&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        pPi = &lt;span class=&quot;built_in&quot;&gt;zeros&lt;/span&gt;(&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;, K);&lt;span class=&quot;comment&quot;&gt;%1-by-K matrix&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        pSigma = &lt;span class=&quot;built_in&quot;&gt;zeros&lt;/span&gt;(D, D, K);&lt;span class=&quot;comment&quot;&gt;%&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        &lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        &lt;span class=&quot;comment&quot;&gt;% hard assign x to each centroids&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        distmat = &lt;span class=&quot;built_in&quot;&gt;repmat&lt;/span&gt;(sum(X.*X, &lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;), &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;, K) + ... &lt;span class=&quot;comment&quot;&gt;% X is a N-by-D data matrix.&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;            &lt;span class=&quot;built_in&quot;&gt;repmat&lt;/span&gt;(sum(pMiu.*pMiu, &lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;)&#39;, N, &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;) - ...&lt;span class=&quot;comment&quot;&gt;% X-&amp;gt;K列 U-&amp;gt;N行 XU^T is N-by-K&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;            &lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;*X*pMiu&#39;;&lt;span class=&quot;comment&quot;&gt;%计算每个点到K个中心的距离&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        [~, labels] = min(distmat, [], &lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;);&lt;span class=&quot;comment&quot;&gt;%找到离X最近的pMiu，[C,I] labels代表这个最小值是从那列选出来的&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; k=&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;:K&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;            Xk = X(labels == k, : );&lt;span class=&quot;comment&quot;&gt;% Xk是所有被归到K类的X向量构成的矩阵&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;            pPi(k) = &lt;span class=&quot;built_in&quot;&gt;size&lt;/span&gt;(Xk, &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;)/N;&lt;span class=&quot;comment&quot;&gt;% 数一数几个归到K类的&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;            pSigma(:, :, k) = cov(Xk); &lt;span class=&quot;comment&quot;&gt;%计算协方差矩阵，D-by-D matrix,最小方差无偏估计&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;end&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;end&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;function&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;Px&lt;/span&gt; = &lt;span class=&quot;title&quot;&gt;calc_prob&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;()&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        Px = &lt;span class=&quot;built_in&quot;&gt;zeros&lt;/span&gt;(N, K);&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; k = &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;:K&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;            Xshift = X-&lt;span class=&quot;built_in&quot;&gt;repmat&lt;/span&gt;(pMiu(k, : ), N, &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;);&lt;span class=&quot;comment&quot;&gt;%x-u&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;            lemda=&lt;span class=&quot;number&quot;&gt;1e-5&lt;/span&gt;;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;            conv=pSigma(:, :, k)+lemda*&lt;span class=&quot;built_in&quot;&gt;diag&lt;/span&gt;(&lt;span class=&quot;built_in&quot;&gt;diag&lt;/span&gt;(&lt;span class=&quot;built_in&quot;&gt;ones&lt;/span&gt;(D)));&lt;span class=&quot;comment&quot;&gt;%这里处理singular问题，为协方差矩阵加上一个很小lemda*I&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;            inv_pSigma = inv(conv);&lt;span class=&quot;comment&quot;&gt;%协方差的逆&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;            tmp = sum((Xshift*inv_pSigma) .* Xshift, &lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;);&lt;span class=&quot;comment&quot;&gt;%(X-U_k)sigma.*(X-U_k),tmp是个N*1的向量&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;            coef = (&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;*&lt;span class=&quot;built_in&quot;&gt;pi&lt;/span&gt;)^(-D/&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;) * &lt;span class=&quot;built_in&quot;&gt;sqrt&lt;/span&gt;(det(inv_pSigma));&lt;span class=&quot;comment&quot;&gt;%前面的参数&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;            Px(:, k) = coef * &lt;span class=&quot;built_in&quot;&gt;exp&lt;/span&gt;(&lt;span class=&quot;number&quot;&gt;-0.5&lt;/span&gt;*tmp);&lt;span class=&quot;comment&quot;&gt;%把数据点 x 带入到 Gaussian model 里得到的值&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;end&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;end&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;end&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;%repmat 通过拓展向量到矩阵&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;%inv 求逆&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;%min 求矩阵最小值，可以返回标签&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;%X(labels == k, : ) 对行做筛选&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;% size(Xk, 1) 求矩阵的长或宽&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;%scatter 对二维向量绘图&lt;/span&gt;&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;&lt;span style=&quot;color: #ff0000;&quot;&gt;注意：&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;pluskid大神这里最后返回的是px，我觉得非常奇怪，因为PRML里对点做hard assignment时是根据后验概率来判别的。于是我在大神博客上问了一下，他的解释是最大似然和最大后验的区别，前者是挑x被各个模型产生的概率最大的那个，而后者加上了先验知识，各有道理。一句话就茅塞顿开，真大神也~
    
    </summary>
    
      <category term="学术" scheme="http://crescentluna.github.io/categories/%E5%AD%A6%E6%9C%AF/"/>
    
    
      <category term="高斯混合模型" scheme="http://crescentluna.github.io/tags/%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B/"/>
    
      <category term="matlab" scheme="http://crescentluna.github.io/tags/matlab/"/>
    
  </entry>
  
  <entry>
    <title>高斯混合模型参数估计详细推导过程</title>
    <link href="http://crescentluna.github.io/2013/04/02/%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1%E8%AF%A6%E7%BB%86%E6%8E%A8%E5%AF%BC%E8%BF%87%E7%A8%8B/"/>
    <id>http://crescentluna.github.io/2013/04/02/高斯混合模型参数估计详细推导过程/</id>
    <published>2013-04-02T03:44:10.000Z</published>
    <updated>2017-02-21T18:24:27.407Z</updated>
    
    <content type="html"><![CDATA[<p>已知多元高斯分布的公式: <span class="math display">\[N(x|\mu,\Sigma)=\frac{1}{(2\pi)^{D/2}|\Sigma|^{1/2}}\exp(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu))\]</span> 其中<span class="math inline">\(D\)</span>为维度，<span class="math inline">\(x\)</span>和<span class="math inline">\(\mu\)</span>均为<span class="math inline">\(D\)</span>维向量，协方差<span class="math inline">\(\Sigma\)</span>为D维矩阵。我们求得后验概率： <span class="math display">\[w^{(i)}_j=Q_i(Z^{i}=j)=P(z^{(i)}=j|x^{(i)};\phi,\mu,\Sigma)\]</span> 在E步，<span class="math inline">\(w^{(i)}_j\)</span>是一个固定值，然后我们用它来估计似然函数<span class="math inline">\(L(X,Z;\theta)\)</span>(这里<span class="math inline">\(\theta=(\phi,\mu,\Sigma)\)</span>)在分布<span class="math inline">\(Z\sim P(Z|X;\theta)\)</span>上的期望<span class="math inline">\(E_{Z|X,\theta_t}[L(X,Z;\theta)]\)</span>（式子1）: <span class="math display">\[\begin{split} &amp; \sum^m_{i=1}\sum_{z^{(i)}} Q_i(z^{(i)})\log{\frac{p(x^{(i)},z^{(i)};\phi,\mu,\Sigma)}{Q_i(z^{(i)})}} \\&amp; =\sum^m_{i=1}\sum^k_{j=1} Q_i(z^{(i)}=j)\log{\frac{p(x^{(i)}|z^{(i)}=j;\mu,\Sigma)p(z^{(i)}=j;\phi)}{Q_i(z^{(i)})}} \\&amp; =\sum^m_{i=1}\sum^k_{j=1} w^{(i)}_j\log{\frac{\frac{1}{(2\pi)^{D/2}|\Sigma|^{1/2}}\exp(-\frac{1}{2}(x^{(i)}-\mu_j)^T\Sigma_j^{-1}(x^{(i)}-\mu_j))\cdot\phi_j}{ w^{(i)}_j}} \\\end{split}\]</span> 由于分母<span class="math inline">\(w^{(i)}_j\)</span>在取对数之后是常数，与参数无关，求导时自然会变成0，所以我们写公式的时候为了简便舍去分母。<a id="more"></a></p>
<h3 id="求mu">求<span class="math inline">\(\mu\)</span></h3>
<p>首先求第<span class="math inline">\(l\)</span>个模型的<span class="math inline">\(\mu_l\)</span>，对式子1求<span class="math inline">\(\mu_l\)</span>的偏导，得到式子2： <span class="math display">\[\begin{split}&amp; =\nabla_{\mu_l} \sum^m_{i=1}\sum^k_{j=1}w^{(i)}_j\log{\frac{1}{(2\pi)^{D/2}|\Sigma|^{1/2}}\exp(-\frac{1}{2}(x^{(i)}-\mu_j)^T\Sigma_j^{-1}(x^{(i)}-\mu_j))\cdot\phi_j} \\&amp;= -\nabla_{\mu_l}\sum^m_{i=1}\sum^k_{j=1}w^{(i)}_j\frac{1}{2}(x^{(i)}-\mu_j)^T\Sigma_j^{-1}(x^{(i)}-\mu_j) \\&amp;= \frac{1}{2} \sum^m_{i=1}w^{(i)}_l \nabla_{\mu_l}(2\mu_l^T\Sigma^{-1}_l x^{(i)} - \mu_l^T\Sigma^{-1}_j\mu_l)\\&amp;=\sum^m_{i=1}w^{(i)}_l(\Sigma^{-1}x^{(i)}-\Sigma^{-1}\mu_l)\end{split}\]</span> 以上式子推导所需的前提知识有：</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(\Sigma_j\)</span>是对称阵（这里其实<span class="math inline">\(\Sigma_j\)</span>不一定是对称的，但是在计算之后他和对称阵的效果是一样的，这里CS229上没讲清楚，详见pluskid大神的<a href="http://freemind.pluskid.org/machine-learning/regularized-gaussian-covariance-estimation/#7de08bf962fca45b9699432818b939067d7c7327" target="_blank" rel="external">Regularized Gaussian Covariance Estimation</a>）。因为<span class="math inline">\(\Sigma_j\)</span>是对称阵，易得<span class="math inline">\(\Sigma_j^{-1}\)</span>也是对称阵。</p></li>
<li><p><span class="math inline">\(1 \times 1\)</span>矩阵的转置不变。因为$ x^{(i)} ^{-1}_l _l $是<span class="math inline">\(1 \times 1\)</span>的矩阵，所以 <span class="math display">\[x^{(i)} \Sigma^{-1}_l \mu_l=(x^{(i)} \Sigma^{-1}_l \mu_l)^T=\mu_l^T\Sigma^{-1}_l x^{(i)}\]</span></p></li>
<li><p>矩阵求导的公式（参考1和2）。设<span class="math inline">\(x,u,v\)</span>为列向量,有以下式子成立： <span class="math display">\[\begin{split}\frac{d(x^T)}{dx} &amp;=I \\\frac{d(Ax)^T}{dx} &amp;=A^T \\\frac{d(u^Tv)}{dx} &amp;=\frac{d(u^T)}{dx}v+\frac{d(v^T)}{dx}u^T \\\end{split}\]</span> 所以 <span class="math display">\[\begin{split}\frac{d (\mu_l^T\Sigma^{-1}_j\mu_l)}{d u_l}&amp;= \frac{d (\mu_l^T)}{d \mu_l} \Sigma^{-1}_j\mu_l+\frac{d (\Sigma^{-1}_j\mu_l)^T}{d \mu_l} (\mu_l^T)^T \\&amp;= 2 \mu_l^T\Sigma^{-1}_j\mu_l\end{split}\]</span></p></li>
</ol>
<p>令之前的式子2等于0，我们解得 <span class="math display">\[\mu_l=\frac{\sum^m_{i=1}w^{(i)}_l x^{(i)}}{\sum^m_{i=1}w^{(i)}_l}\]</span></p>
<h3 id="求phi">求<span class="math inline">\(\phi\)</span></h3>
<p>然后我们求<span class="math inline">\(\phi_j\)</span>，这个式子1对它偏导之后就形式更加简单： <span class="math display">\[\sum^m_{i=1}\sum^k_{j=1} w^{(i)}_j\log{\phi_j}\]</span> 但是需要注意的是，<span class="math inline">\(\phi_j\)</span>存在一个约束条件<span class="math inline">\(\sum_{j=1}^K \phi_j =1\)</span>，它代表的是取<span class="math inline">\(k\)</span>个高斯模型中的一个的概率总和为1。为了求得条件极值，我们引入拉格朗日乘子<span class="math inline">\(\beta\)</span>, 设拉格朗日函数为 <span class="math display">\[\mathcal{L(\phi)}= \sum^m_{i=1}\sum^k_{j=1} w^{(i)}_j\log{\phi_j}+\beta(\sum^k_{j=1}(\phi_j-1)\]</span> 求导等于0之后我们得到 <span class="math display">\[\phi_j=-\frac{\sum^m_{i=1} w^{(i)}_j}{\beta}\]</span> 事情到这里还没完，这个<span class="math inline">\(\beta\)</span>还能求出来，因为后验概率<span class="math inline">\(w^{(i)}_j\)</span>之和为1，所以 <span class="math display">\[\sum_{j=1}^k \phi_j=-\frac{\sum^m_{i=1} \sum_{j=1}^k w^{(i)}_j}{\beta}=-\frac{\sum^m_{i=1}1}{\beta}=-\frac{m}{\beta}=1\]</span> 得到<span class="math inline">\(\beta=-m\)</span>,最后有 <span class="math display">\[\phi_j=\frac{1}{m}\sum^m_{i=1} w^{(i)}_j\]</span></p>
<h3 id="求">求$ $</h3>
<p>最后，我们来求$ _j<span class="math inline">\(。CS229的教材上说求这个\)</span>_j$是“entirely straightforward”，真是高估了像我这样数学不好的人- -。</p>
<p>对式子1求偏导，我们有式子3： <span class="math display">\[\begin{split}&amp;= -\nabla_{\Sigma_j}\sum^m_{i=1}w^{(i)}_j(\frac{1}{2}\log{|\Sigma_j|}- \frac{1}{2}(x^{(i)}-\mu_j)^T\Sigma_j^{-1}(x^{(i)}-\mu_j))\\&amp;= \sum^m_{i=1}w^{(i)}_j\Sigma^{-1}_j- \sum^m_{i=1}w^{(i)}_j\Sigma_j^{-1}(x^{(i)}-\mu_j)(x^{(i)}-\mu_j)^T\Sigma_j^{-1}\end{split}\]</span></p>
<pre><code>1. 需要矩阵求导公式:(wikipedia中对multivariate normal distribution的解释（参考3）以及其第6篇参考文献（参考4）,&lt;del&gt;其实不用找，PRML的附录C里有。。。&lt;/del&gt;)</code></pre>
<p><span class="math display">\[\begin{split}\frac{ \partial }{\partial \Sigma}\log{|\Sigma|} &amp; =\Sigma^{-1} \\\frac{ \partial }{\partial \Sigma}(x-\mu)^T\Sigma^{-1}(x-\mu) &amp; =-\Sigma^{-1}(x-\mu)^T(x-\mu)\Sigma^{-1} \\\end{split}\]</span> 第二条公式应该是根据以下两个公式推出来的： <span class="math display">\[\begin{split}\frac{d(a^T Xb)}{d X} &amp;=ab^T \\\frac{d(Y^{-1})}{d Y} &amp;=-Y^{-1}\frac{d Y}{ d X}Y^{-1} \\\end{split}\]</span> 令式子3等于0，我们求得</p>
<p><span class="math display">\[\Sigma_j=\frac{\sum^m_{i=1}w^{(i)}_j(x^{(i)}-\mu_j)(x^{(i)}-\mu_j)^T}{\sum^m_{i=1}w^{i}_j}\]</span></p>
<p>把他们写在一起，就得到了高斯混合模型的参数估计： <span class="math display">\[ \begin{split} &amp; \mu_j=\frac{\sum^m_{i=1}w^{(i)}_jx^{(i)}}{\sum^m_{i=1}w^{(i)}_j} \\&amp; \phi_j=\frac{1}{m}\sum^{m}_{i=1}w^{(i)}_j \\&amp; \Sigma_j=\frac{\sum^m_{i=1}w^{(i)}_j(x^{(i)}-\mu_j)(x^{(i)}-\mu_j)^T}{\sum^m_{i=1}w^{i}_j} \\\end{split}\]</span> 参考文献： 1.<a href="http://blog.sina.com.cn/s/blog_51c4baac0100xuww.html" target="_blank" rel="external">矩阵求导</a> 2.<a href="http://www.psi.toronto.edu/matrix/calculus.html" target="_blank" rel="external">Matrix Reference Manual</a> 3.<a href="http://en.wikipedia.org/wiki/Estimation_of_covariance_matrices" target="_blank" rel="external">Estimation of covariance matrices</a> 4.^ Dwyer, Paul S. (June 1967). “Some applications of matrix derivatives in multivariate analysis”. Journal of the American Statistical Association (Journal of the American Statistical Association, Vol. 62, No. 318) 62 (318): 607–625. doi:10.2307/2283988. JSTOR 2283988. 5.stanford的CS229课程</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;已知多元高斯分布的公式: &lt;span class=&quot;math display&quot;&gt;\[N(x|\mu,\Sigma)=\frac{1}{(2\pi)^{D/2}|\Sigma|^{1/2}}\exp(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu))\]&lt;/span&gt; 其中&lt;span class=&quot;math inline&quot;&gt;\(D\)&lt;/span&gt;为维度，&lt;span class=&quot;math inline&quot;&gt;\(x\)&lt;/span&gt;和&lt;span class=&quot;math inline&quot;&gt;\(\mu\)&lt;/span&gt;均为&lt;span class=&quot;math inline&quot;&gt;\(D\)&lt;/span&gt;维向量，协方差&lt;span class=&quot;math inline&quot;&gt;\(\Sigma\)&lt;/span&gt;为D维矩阵。我们求得后验概率： &lt;span class=&quot;math display&quot;&gt;\[w^{(i)}_j=Q_i(Z^{i}=j)=P(z^{(i)}=j|x^{(i)};\phi,\mu,\Sigma)\]&lt;/span&gt; 在E步，&lt;span class=&quot;math inline&quot;&gt;\(w^{(i)}_j\)&lt;/span&gt;是一个固定值，然后我们用它来估计似然函数&lt;span class=&quot;math inline&quot;&gt;\(L(X,Z;\theta)\)&lt;/span&gt;(这里&lt;span class=&quot;math inline&quot;&gt;\(\theta=(\phi,\mu,\Sigma)\)&lt;/span&gt;)在分布&lt;span class=&quot;math inline&quot;&gt;\(Z\sim P(Z|X;\theta)\)&lt;/span&gt;上的期望&lt;span class=&quot;math inline&quot;&gt;\(E_{Z|X,\theta_t}[L(X,Z;\theta)]\)&lt;/span&gt;（式子1）: &lt;span class=&quot;math display&quot;&gt;\[\begin{split} &amp;amp; \sum^m_{i=1}\sum_{z^{(i)}} Q_i(z^{(i)})\log{\frac{p(x^{(i)},z^{(i)};\phi,\mu,\Sigma)}{Q_i(z^{(i)})}} \\&amp;amp; =\sum^m_{i=1}\sum^k_{j=1} Q_i(z^{(i)}=j)\log{\frac{p(x^{(i)}|z^{(i)}=j;\mu,\Sigma)p(z^{(i)}=j;\phi)}{Q_i(z^{(i)})}} \\&amp;amp; =\sum^m_{i=1}\sum^k_{j=1} w^{(i)}_j\log{\frac{\frac{1}{(2\pi)^{D/2}|\Sigma|^{1/2}}\exp(-\frac{1}{2}(x^{(i)}-\mu_j)^T\Sigma_j^{-1}(x^{(i)}-\mu_j))\cdot\phi_j}{ w^{(i)}_j}} \\\end{split}\]&lt;/span&gt; 由于分母&lt;span class=&quot;math inline&quot;&gt;\(w^{(i)}_j\)&lt;/span&gt;在取对数之后是常数，与参数无关，求导时自然会变成0，所以我们写公式的时候为了简便舍去分母。
    
    </summary>
    
      <category term="学术" scheme="http://crescentluna.github.io/categories/%E5%AD%A6%E6%9C%AF/"/>
    
    
      <category term="EM" scheme="http://crescentluna.github.io/tags/EM/"/>
    
      <category term="高斯混合模型" scheme="http://crescentluna.github.io/tags/%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B/"/>
    
  </entry>
  
  <entry>
    <title>社区发现及其发展方向简介（未完）</title>
    <link href="http://crescentluna.github.io/2013/03/26/Introdcution%20of%20Community%20Detection/"/>
    <id>http://crescentluna.github.io/2013/03/26/Introdcution of Community Detection/</id>
    <published>2013-03-26T00:48:14.000Z</published>
    <updated>2017-02-21T18:24:27.407Z</updated>
    
    <content type="html"><![CDATA[<h2 id="社区发现简介">1. 社区发现简介</h2>
<p>社区，从直观上来看，是指网络中的一些密集群体，每个社区内部的结点间的联系相对紧密，但是各个社区之间的连接相对来说却比较稀疏（图1，当然社区的定义不止有这一种）。这样的社区现象被研究已经很多年了，最早期的记录甚至来自于80年前。</p>
<p><a href="http://7sbo5n.com1.z0.glb.clouddn.com/aaa.png" target="_blank" rel="external"><img src="http://7sbo5n.com1.z0.glb.clouddn.com/aaa.png" alt="aaa"></a></p>
<p>比较经典的社区研究案例包括对空手道俱乐部(karate club),科学家合作网络(Collaboration network) 和斑马群体(zebras) 的社交行为研究等（见图2），其中著名的空手道俱乐部社区已经成为通常检验社区发现算法效果的标准(benchmark)之一。</p>
<a id="more"></a>
<p><a href="http://7sbo5n.com1.z0.glb.clouddn.com/b.png" target="_blank" rel="external"><img src="http://7sbo5n.com1.z0.glb.clouddn.com/b.png" alt="b"></a></p>
<p>随着互联网和在线社交网站的兴起，在Twitter,Facebook，Flickr这样的用户生成内容（UCG）网站上使用社区发现的技术已经成为热潮。在这些社区中用户相互的交流与反馈，能为传统的社区带来丰富的内容信息和新的结构，从而使社区发现有了新的发展。</p>
<h2 id="社区发现算法介绍">2. 社区发现算法介绍</h2>
<p>因为社区发现的算法很多很多，下图列出了比较核心的社区发现算法介绍（在参考1给的目录上稍作修改，包含但不限于，五花八门的太多了）：</p>
<p><a href="http://7sbo5n.com1.z0.glb.clouddn.com/无标题11.png" target="_blank" rel="external"><img src="http://7sbo5n.com1.z0.glb.clouddn.com/无标题11.png" alt="无标题1"></a></p>
<p>对上图所涉及的算法作简单介绍：</p>
<h3 id="图分割">2.1 图分割</h3>
<p>社区可以看做密集子图结构，使用图分割算法来解决。图分割问题的目标是把图中的节点分成<span class="math inline">\(g\)</span>个预定大小的群组，这些群组之间的边数目最小，这个问题是NP-hard 的。</p>
<h4 id="二分图">2.1.1 二分图</h4>
<p>早期的分割都是二分图，社区发现也是基于二分的，遇到多分的情况就把其中一个子图再分割。比较经典的有谱二分法，利用拉普拉斯矩阵的第二小特征值<span class="math inline">\(\lambda_2\)</span>对社区二分类，这其实是属于谱方法的一种特例。</p>
<h4 id="kl算法">2.1.2 KL算法</h4>
<p>KL算法通过基于贪婪优化的启发式过程把网络分解为2个规模已知的社区。该算法为网络的划分引入一个增益函数，定义为两个社区内部的边数与两个社区边数之间的差，寻求Q的最大划分办法。</p>
<h4 id="最大流算法">2.1.3 最大流算法</h4>
<p>基于最大流的算法是G.W.Flake提出的。他给网络加了虚拟源节点<span class="math inline">\(s\)</span>和终点节点<span class="math inline">\(t\)</span>，并证明了经过最大流算法之后，包含源点<span class="math inline">\(s\)</span>的社区恰好满足社区内节点链接比与社区外的链接要多的性质。</p>
<p><a href="http://7sbo5n.com1.z0.glb.clouddn.com/maxflow.png" target="_blank" rel="external"><img src="http://7sbo5n.com1.z0.glb.clouddn.com/maxflow.png" alt="maxflow"></a></p>
<h3 id="聚类">2.2 聚类</h3>
<p>当社区的边非常密集，数目远大于点时，图分割可能就不太好使了，这时候社区发现可能更接近于聚类。我们把社区发现看做一组内容相似的物体集合，使用聚类算法。和图中的社区发现相比，图中的社区点与点之间可以用边来表示联系的紧密，而聚类中的社区，需要定义点之间的相似度，比如说根据邻接关系定义： <span class="math display">\[d_{ij}=\sqrt{\sum_{k\neq i,j}(A_{ik}-A_{jk})^2}\]</span> 其中<span class="math inline">\(A\)</span>为邻接矩阵，<span class="math inline">\(i\)</span>和<span class="math inline">\(j\)</span>的邻居越多，节点相似度越高。 聚类算法和网络发现（聚类相关的）算法可以很容易地互相转化。另外，社区发现可以是局部的，而聚类是全网络的。</p>
<h4 id="层次聚类">2.2.1 层次聚类</h4>
<p>层次聚类假设社区是存在层次结构的（其实不一定额，可能是中心结构），计算网络中每一对节点的相似度。 然后分为凝聚法和分裂法两种：</p>
<ul>
<li><p>凝聚法：根据相似度从强到弱连接相应节点对，形成树状图（Dendrogram），根据需求对树状图进行横切，获得社区结构。<a href="http://7sbo5n.com1.z0.glb.clouddn.com/dendrogram.png" target="_blank" rel="external"><img src="http://7sbo5n.com1.z0.glb.clouddn.com/dendrogram.png" alt="dendrogram"></a></p></li>
<li><p>分裂法：找出相互关联最弱的节点，并删除他们之间的边，通过这样的反复操作将网络划分为越来越小的组件，连通的网络构成社区。</p></li>
</ul>
<h4 id="划分聚类扁平聚类">2.2.2 划分聚类/扁平聚类</h4>
<p>像k-means（<del datetime="2013-04-16T12:35:38+00:00">如何弄到欧氏空间中是个问题</del>，<span style="color: #ff0000;">可以使用隐含空间模型，比如MDS</span>），k-medoids什么的就很好，可以使用上面的相似度来聚类 。</p>
<h4 id="谱聚类">2.2.3 谱聚类</h4>
<p>图分割中的如 Ratio Cut和Normalized Cut其实和谱聚类是等价的（见参考3），所以谱聚类也能用在社区发现上。</p>
<h3 id="分裂法">2.3 分裂法</h3>
<p>这里的分裂法和层次聚类中的类似，区别是前者不计算节点相似度，而是删除是两个社区之间的关联边，这些边上的两点的相似度不一定很低。其中最著名的算法就是Girvan-Newman算法，根据以下假设：社区之间所存在的少数几个连接应该是社区间通信的瓶颈，是社区间通信时通信流量的必经之路。如果我们考虑网络中某种形式的通信并且寻找到具有最高通信流量（比如最小路径条数）的边，该边就应该是连接不同社区的通道。Girvan-Newman算法就是这样，迭代删除边介数（Edge Betweenness）最大的边。</p>
<h3 id="谱方法">2.4 谱方法</h3>
<p>基于谱分析的社区算法基于如下事实，在同一个社区内的节点，它在拉普拉斯矩阵中的特征向量近似。将节点对应的矩阵特征向量（与特征值和特征向量有关的都叫谱）看成空间坐标，将网络节点映射到多维向量空间去，然后就可以运用传统的聚类算法将它们聚集成社团。这种方法不可避免的要计算矩阵的特征值，开销很大，但是因为能直接使用很多传统的向量聚类的成果，灵活性很高。</p>
<h3 id="基于模块度的方法">2.5 基于模块度的方法</h3>
<p>模块度不仅仅作为优化的目标函数提出，它也是目前是最流行的用来衡量社区结果好坏的标准之一（它的提出被称作社区发现研究历史上的里程碑)。我们知道，社区是节点有意识地紧密联系所造成的，它内部边的紧密程度总比一个随机的网络图来的紧密一些，模块度的定义就是基于此，它表示所有被划分到同一个社区的边所占的比例，再减除掉完全随机情况时被划分到同一个社区的边所占的比例： <span class="math display">\[Q=\sum^K_{c=1}[\frac{A(V_i,V_i)}{m}-(\frac{degree(V_i)}{2m})^2]\]</span> 其中<span class="math inline">\(V_i\)</span>是第<span class="math inline">\(i\)</span>个社区，<span class="math inline">\(m\)</span>是整个图中边的数目。模块度的一个优点是好坏与社区中点的数目无关。模块度真是个好东西，第一次对社区这个模糊的概念提出了量化的衡量标准（不过据说对于小粒度的不太准）。所以对模块度的算法优化多种多样，从贪心到模拟退火等应有尽有。</p>
<h3 id="动态算法">2.6 动态算法</h3>
<p>自旋模型和同步算法应该是物理学家提出来的算法<del>（完全看不懂）</del>，话说物理学家在社区发现领域十分活跃，发了不少论文。随机游走是基于以下思想：如果存在很强的社区结构，那么随机游走器（random walker)会在社区内部停留更长的时间，因为社区内部的边密度比较高。</p>
<h3 id="基于统计推断的算法">2.7 基于统计推断的算法</h3>
<p>基于统计推断的方法包括观察到的数据集和对模型的假设。如果数据集是图，模型假设对节点之间如何联系的描述就要符合真实的图结构。</p>
<h3 id="其他">2.8 其他</h3>
<p>个人觉得重叠和动态社区都很难成为一个类别，因为具体算法各有不同，用共同点“重叠”或“动态”来作为一类又太广泛了，比较适合作为特征或维度来描述。 而Web社区特指Web页面相互连接而成的集合,这又是一个大类，底下有不少算法。</p>
<h2 id="社区发现算法特征">3. 社区发现算法特征</h2>
<p>下面我从不同的角度来描绘社区发现算法的一些特征<del>（叫维度比较好？）</del>，这些特征可以用来对社区发现算法进行分类：</p>
<h3 id="优化目标">3.1 优化目标</h3>
<p>有一些社区发现算法比如谱方法，KL算法，以及基于最大流的社区发现方法等，给出明确的的目标函数，并提出算法来最优化目标函数。 常用的优化目标函数有：</p>
<h4 id="normailized-cut和conductance">3.1.1 Normailized Cut和conductance</h4>
<p>如果我们将图划分为<span class="math inline">\(S\)</span>和<span class="math inline">\(\bar{S}=V-S\)</span>两个部分，那么<span class="math inline">\(S\)</span>与图中的剩下部分联系越少，说明<span class="math inline">\(S\)</span>越独立，越有可能是一个内部紧密的社区。我们用<span class="math inline">\(cut(S)\)</span>来表示两者 之间的联系数目： <span class="math display">\[cut(S)=\sum_{i\in S,j\in \bar{S}}A(i,j)\]</span> 为了避免孤立节点的产生，我们分别除以它的权值（内部度数之和），来达到相对平均一些的分割。这就是Normailized Cut: <span class="math display">\[Ncut(S)=\frac{\sum_{i\in S,j\in \bar{S}}A(i,j)}{\sum_{i\in S}degree(i)}+\frac{\sum_{i\in S,j\in\bar{S}}A(i,j)}{\sum_{j\in \bar{S}}degree(j)}\]</span> 连通度(conductance)也是类似的定义: <span class="math display">\[ Conductance(S)=\frac{\sum_{i\in S,j\in \bar{S}}A(i,j)}{\min{(\sum_{i\in S}degree(i),\sum_{j\in\bar{S}}degree(j))}}\]</span> 当涉及到多个划分<span class="math inline">\(V_1,\ldots,V_k\)</span>时，Normalilized Cut和连通度就是它们之和。</p>
<h4 id="kernighan-lin-object">3.1.2 Kernighan-Lin object</h4>
<p>KL目标函数旨在使两个相同大小的社区之间的边联系最小： <span class="math display">\[KLObj(V_1,\ldots,V_k)=\sum_{i\neq j}A(V_i,V_j)\]</span> 其中<span class="math inline">\(A(V_i,V_j)=\sum_{u \in V_i,v \in V_j}A(u,v), |V_1|=|V_2|=\ldots=|V_k|\)</span>。</p>
<h4 id="modularity">3.1.3 Modularity</h4>
<p>模块度在2.5已提过，这里就不说了。</p>
<h3 id="粒度控制社区数目可不可控">3.2 粒度控制（社区数目可不可控）</h3>
<p>对于有层次的社区发现算法来说的，比如某些二分社区算法，是通过不断递归的划分子社区来获得预定的社区数目。而某些算法，像层次聚类和MCL，基于概率模型的社区发现算法等，允许用户通过调节参数来间接控制输出社区的数目。</p>
<p>另一些算法，像模块度优化算法，它的社区数目是由优化函数决定的，不需要用户来设定社区的数目。</p>
<h3 id="规模">3.3 规模</h3>
<p>很多算法在设计的时候，并没有特别地考虑伸缩性，在面对整个Web以及大型社交网络时动辄百万甚至千万个点时效果不佳。比如GN算法，需要计算即通过每条边的最短路径数目（edge betweeness)，复杂度相当高，像谱聚类算法，能处理10K个点和70M条边就不错了。</p>
<p>所以，有些算法比如Shingling算法等，使用的方法相对简单，从而能适合大规模的社区发现的运行要求。</p>
<h3 id="局部社区发现">3.4 局部社区发现</h3>
<p>所谓的局部社区发现，是指只根据临近的邻居节点发现社区结构，而不考虑全局的网络，这与全局社区发现中对图中的每一个节点都打上社区标签的做法相对应。</p>
<p>在整个网络图很大，数据集不能全部加载到内存时，使用局部社区发现可以只加载图的一部分，发现一个局部社区，然后迭代地调用该方法来逐一地提取社区结构。</p>
<h3 id="重叠社区">3.5 重叠社区</h3>
<p>很多社区发现算法，比如图分割算法，将整个网络划分为多个独立的社区结构。但是在现实中，许多网络并不存在绝对的彼此独立的社团结构，相反，它们是由许多彼此重叠互相关联的社团构成，比如说在社交网络中，一个人根据兴趣的不同，有可能属于多个不同的小组等。所以，很多类似派系过滤算法（CPM）这样旨在发现重叠社区的算法也被不断地提出来。</p>
<h3 id="动态社区发现">3.6 动态社区发现</h3>
<p>在4.1有讲，这里就不讲了。</p>
<h3 id="评价标准待改">3.7 评价标准（待改）</h3>
<p>社区发现算法常用的评价标准有：</p>
<h4 id="准确率召回率f1值">3.7.1 准确率，召回率，F1值</h4>
<p>一个大规模数据集合中检索文档的时，可把文档分成四组：系统检索到的相关文档（A），系统检索到的不相关文档（B），相关但是系统没有检索到的文档（C），不相关且没有被系统检索到的文档（D）： 准确度定义为： <span class="math display">\[pr=\frac{A}{A+C}\]</span> 召回率定义为： <span class="math display">\[rc=\frac{A}{A+B}\]</span> F-measure是准确率和召回率协调之后的结果，定义为： <span class="math display">\[PWF=\frac{2\times pr \times rc}{pr+rc}\]</span> 同理，社区也可以用这个概念。</p>
<h4 id="平均聚类纯度">3.7.2 平均聚类纯度</h4>
<p>平均聚类纯度，average cluster purity。假设算法发现了<span class="math inline">\(C=\{C_1,\ldots,C_K\}\)</span>个社区，我们假设社区<span class="math inline">\(C_i\)</span>有<span class="math inline">\(n_i\)</span>个点，每个点分别为<span class="math inline">\(\{v_{1,i},\ldots,v_{n_i,i}\}\)</span>。令<span class="math inline">\(M_{l,i}\)</span>为点<span class="math inline">\(v_{l,i}\)</span> 真实归属的标签，平均聚类纯度为定义为 <span class="math display">\[ACP=\frac{1}{k}\sum_{i=1}^k\sum_{l=1}^{n_i}\frac{\delta(dom_i\in M_{l,i})}{n_i}\]</span> 即社区<span class="math inline">\(C_i\)</span>中主要标签的点占社区所有点的数目比例。</p>
<h4 id="互信息">3.7.3 互信息</h4>
<p>首先来回顾熵的定义,在一个分布内包含的信息为熵： <span class="math display">\[H(X)=-\sum_{x \in X}p(x)\log p(x)\]</span> 互信息（mutual information)描述了两个分布之间的相关性 <span class="math display">\[I(X;Y)=\sum_{y \in Y}\sum_{x \in X}p(x,y)\log(\frac{p(x,y)}{p(x)p(y)})\]</span> 我们有 <span class="math display">\[I(X;Y)=H(X)-H(X|Y)\]</span> 所谓两个事件相关性的量化度量，就是在了解其中一个Y的前提下，对消除另一个X不确定性所提供的信息量。 规范化的互信息定义为 <span class="math display">\[NMI(X;Y)=\frac{I(X;Y)}{\sqrt{H(X)H(Y)}}\]</span> 我们将划分当做一个结点落在社区的概率分布吗，然后计算社区划分结果和真实情况的NMI值，具体例子见参考4.</p>
<h2 id="社区发现的趋势">4. 社区发现的趋势</h2>
<h3 id="动态社区发现-dynamic-networks">4.1 动态社区发现 (Dynamic Networks)</h3>
<p>很多社区算法都把社区看做静态的图，但是事实上的社交网络是随着时间逐渐演变的。这些社区如何形成和消解，它们的的动态变化该如何处理，确实是一个研究热点。</p>
<p><a href="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130326165736.png" target="_blank" rel="external"><img src="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130326165736.png" alt="QQ截图20130326165736"></a></p>
<h3 id="异构网络上的社区发现heterogeneous-network">4.2 异构网络上的社区发现(Heterogeneous Network)</h3>
<p>日常算法中我们都假定网络中的点和边属于同一类型。但是现实中也有很多异构网络（Heterogeneous Networks)它的点和边的类型不同。比如说IMDB网络，它的实体可能包括电影，导演，演员，而他们之间的关系也不同。如何在这样的网络上做社区发现也是一大挑战。</p>
<p><a href="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130326183011.png" target="_blank" rel="external"><img src="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130326183011.png" alt="QQ截图20130326183011"></a></p>
<h3 id="有向图上的社区发现directed-networks">4.3 有向图上的社区发现(Directed Networks)</h3>
<p>一般的社区发现都把整个网络当做无向图来处理。但是很多网络它的有向性比较特殊，比如说Web社区，论文之间的引用关系，以及Twitter用户之间的关注关系。简单地忽略这些网络中的方向性，会导致信息的损失，算法可能会得出错误的结果。</p>
<p><a href="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130326183118.png" target="_blank" rel="external"><img src="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130326183118.png" alt="QQ截图20130326183118"></a></p>
<h3 id="内容与链接关系结合的社区发现content-and-relationship-information">4.4 内容与链接关系结合的社区发现(Content and Relationship Information)</h3>
<p>像前面说过的一样，新兴社交网络中的关系，不仅仅再是一条有向边，其中可以包括很多内容信息（文本，图像，地理信息），这又将社区发现带向了新的领域——如何联合这些关系和内容信息来发现社区。</p>
<p><a href="http://7sbo5n.com1.z0.glb.clouddn.com/social-network.jpg" target="_blank" rel="external"><img src="http://7sbo5n.com1.z0.glb.clouddn.com/social-network.jpg" alt="social-network"></a></p>
<h3 id="交叉领域的社区发现crosscutting-issues">4.5 交叉领域的社区发现(Crosscutting Issues)</h3>
<ul>
<li><p>伸缩性。比如说并行和分布式算法，利用GPU来运算等。</p></li>
<li><p>可视化。如何展示数以十亿计节点，以及如何处理动态信息。</p></li>
<li><p>交叉领域上的社区发现。生物学什么的。</p></li>
<li><p>做排名。</p></li>
</ul>
<p>总而言之结论就是，社区发现（特别是社交网络上的）还是比较新的，还是有一大把理论上和实践上的开放问题可做的。<del>（我才不信呢&gt;_&lt;！）</del></p>
<p>参考文献：</p>
<p>1. Fortunato, S. (2010). “Community detection in graphs.” Physics Reports <strong>486</strong>(3): 75-174.</p>
<p>2.《 Social Network Data Analytics》第四章，<em>DOI 10.1007/978-1-4419-8462-3_4</em></p>
<p>3.<a href="http://blog.pluskid.org/?p=287" class="uri" target="_blank" rel="external">http://blog.pluskid.org/?p=287</a></p>
<p>4.《社会计算：社区发现和社会媒体挖掘》,Page 58</p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;社区发现简介&quot;&gt;1. 社区发现简介&lt;/h2&gt;
&lt;p&gt;社区，从直观上来看，是指网络中的一些密集群体，每个社区内部的结点间的联系相对紧密，但是各个社区之间的连接相对来说却比较稀疏（图1，当然社区的定义不止有这一种）。这样的社区现象被研究已经很多年了，最早期的记录甚至来自于80年前。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://7sbo5n.com1.z0.glb.clouddn.com/aaa.png&quot;&gt;&lt;img src=&quot;http://7sbo5n.com1.z0.glb.clouddn.com/aaa.png&quot; alt=&quot;aaa&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;比较经典的社区研究案例包括对空手道俱乐部(karate club),科学家合作网络(Collaboration network) 和斑马群体(zebras) 的社交行为研究等（见图2），其中著名的空手道俱乐部社区已经成为通常检验社区发现算法效果的标准(benchmark)之一。&lt;/p&gt;
    
    </summary>
    
      <category term="学术" scheme="http://crescentluna.github.io/categories/%E5%AD%A6%E6%9C%AF/"/>
    
    
      <category term="社区发现" scheme="http://crescentluna.github.io/tags/%E7%A4%BE%E5%8C%BA%E5%8F%91%E7%8E%B0/"/>
    
  </entry>
  
  <entry>
    <title>LDA学习笔记---来自《Parameter estimation for text analysis》</title>
    <link href="http://crescentluna.github.io/2013/03/12/Parameter%20estimation%20for%20text%20analysis/"/>
    <id>http://crescentluna.github.io/2013/03/12/Parameter estimation for text analysis/</id>
    <published>2013-03-11T18:20:25.000Z</published>
    <updated>2017-02-21T18:24:27.407Z</updated>
    
    <content type="html"><![CDATA[<p><span style="color: #ff0000;">2013年10月10日更新。</span></p>
<p>LDA的概率图如下图1所示：<a href="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130312094645.png" target="_blank" rel="external"><img src="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130312094645.png" alt="QQ截图20130312094645"></a></p>
<p>参数的意思如图2所示：</p>
<p><a href="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130312094711.png" target="_blank" rel="external"><img src="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130312094711.png" alt="QQ截图20130312094711"></a> 根据模型，文章m的第n个词t是这样生成的：先从文章m的doc-topic分布中生成一个topic编号<span class="math inline">\(z_{m,n}\)</span>，在根据编号第<span class="math inline">\(z_{m,n}\)</span>个的topic-word分布中生成这个词，总够有<span class="math inline">\(K\)</span>个topic，所以总的概率为： <span class="math display">\[ p(w_{m,n}=t|\vec{\theta}_m,\underline{\Phi})=\sum^K_{k=1}p(w_{m,n}=t|\vec{\phi}_k)p(z_{m,n}=k|\vec{\theta}_m)\]</span> 如果我们写出这篇文章的complete-data的联合分布（<span style="color: #ff0000;">意思就是所以变量都已知的情况下</span>），那么式子就是这样的：</p>
<p><a href="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130312094748.png" target="_blank" rel="external"><img src="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130312094748.png" alt="QQ截图20130312094748"></a></p>
<p>通过对<span class="math inline">\(\vec{\vartheta_m}\)</span>（doc-topic分布）和<span class="math inline">\(\underline{\Phi}\)</span>（topic-word分布）积分以及<span class="math inline">\(z_{m,n}\)</span>求和，我们可以求得<span class="math inline">\(\vec{w_m}\)</span>的边缘分布：</p>
<p><a href="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130312094757.png" target="_blank" rel="external"><img src="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130312094757.png" alt="QQ截图20130312094757"></a></p>
<p>(<span style="color: #ff0000;">实际上这个边缘分布是求不出来的</span>，因为<span class="math inline">\(z_{m,n}\)</span>是隐藏变量，从而导致<span class="math inline">\(\underline{\vartheta}\)</span>与<span class="math inline">\(\underline{\Phi}\)</span>存在耦合现象，无法积分得到。要注意联合分布和边缘分布对Z乘积与加和的区别）</p>
<p>因为一个语料库有很多篇文章，而且文章之间都是相互独立的，所以整个语料库的似然为 <span class="math display">\[ p(\mathcal{W}|\vec{\alpha},\vec{\beta})=\prod^{M}_{m=1}p(\vec{w_m}|\vec{\alpha},\vec{\beta})\]</span></p>
<p>虽然LDA（latent Dirichlet allocation)是个相对简单的模型，对它直接推断一般也是不可行的，所以我们要采用近似推断的方法，比如Gibbs sampling。</p>
<a id="more"></a>
<h3 id="gibbs-sampling">Gibbs sampling</h3>
<p>Gibbs sampling是MCMC(Markov-chain Monte Carlo)算法的一种特殊情况，经常用于处理高维模型的近似推断。MCMC方法可以通过马尔科夫链的平稳分布模拟高维的概率分布<span class="math inline">\(p(\vec{x})\)</span>。当马尔科夫链经过了burn-in阶段，消除了初始参数的影响，进入平稳状态之后，它的每次转移都能生成一个<span class="math inline">\(p(\vec{x})\)</span>的样本。Gibbs samppling 是MCMC的特殊情况，它每次固定一个维度的<span class="math inline">\(x_i\)</span>,然后通过其他维度的数据（<span class="math inline">\(\vec{x}_{\neg i})\)</span>生成这个维度的样本。算法如下：</p>
<ol style="list-style-type: decimal">
<li><p>choose dimension i(random by permutation)。</p></li>
<li><p>sample <span class="math inline">\(x_i\)</span> from $p(x_i|_{i}) $。</p></li>
</ol>
<p>为了构造Gibbs抽样，我们必须知道条件概率<span class="math inline">\(p(x_i|\vec{x}_{\neg i})\)</span>，这个概率可以通过以下公式获得: <span class="math display">\[p(x_i|\vec{x}_{\neg i})=\frac{p(x_i,\vec{x}_{\neg i})}{p(\vec{x}_{\neg i})}=\frac{p(x_i,\vec{x}_{\neg i})}{\int{p(\vec{x})d x_i}}\]</span> 对于那些含有隐藏变量<span class="math inline">\(\vec{z}\)</span>的模型来说，通常需要求得他们的后验概率<span class="math inline">\(p(\vec{z}|\vec{x})\)</span>，对于这样的模型，Gibbs sampler的式子如下： <span class="math display">\[ p(z_i|\vec{z}_{\neg i},\vec{x})=\frac{p(\vec{z},\vec{x})}{p(\vec{z}_{\neg i},\vec{x})}=\frac{p(\vec{z},\vec{x})}{\int_z{p(\vec{z},\vec{x})d x_i}}\]</span> 当样本<span class="math inline">\(\tilde{\vec{z_r}},r\in[1,R]\)</span>的数量足够多时，隐藏变量的后验概率可以用以下式子来估计： <span class="math display">\[ p(\vec{z}|\vec{x})=\frac{1}{R}\sum^R_{r=1}\delta(\vec{z}-\tilde{\vec{z_r}})\]</span> 其中Kronecker delta $()={1 $ if <span class="math inline">\(\vec{u}=0;0\)</span> otherwise $ }$。</p>
<p>为了构造LDA的采样器，我们首先确定模型中的隐含变量为<span class="math inline">\(z_{m,n}\)</span>。而参数<span class="math inline">\(\underline{\Theta}\)</span>和<span class="math inline">\(\underline{\Phi}\)</span>都可以用观察到的<span class="math inline">\(w_{m,n}\)</span>和对应的<span class="math inline">\(z_{m,n}\)</span>求积分得到(<span style="color: #ff0000;">(这个方法叫collapsed Gibbs Sampling，即通过求积分去掉一些未知变量，使Gibbs Sampling的式子更加简单)</span>)。贝叶斯推断的目标是分布<span class="math inline">\(p(\vec{z}|\vec{w})\)</span>，它与联合分布成正比: <span class="math display">\[p(\vec{z}|\vec{w})=\frac{p(\vec{z},\vec{w})}{p(\vec{w})}=\frac{\prod^W_{i=1}p(z_i,w_i)}{\prod^W_{i=1}\sum^K_{k=1}p(z_i=k,w_i)}\]</span> 这里忽略了超参数（hyperparameter)<span class="math inline">\(\vec{\alpha}\)</span>和<span class="math inline">\(\vec{\beta}\)</span>。可以看到分母部分(也就是<span class="math inline">\(p(\vec{w}|\vec{\alpha},\vec{\beta})\)</span>)十分难求，它包括了<span class="math inline">\(K^W\)</span>个项的求和。所以我们使用Gibbs Sample方法，通过全部的条件分布<span class="math inline">\(p(z_i|\vec{z}_{\neg i},\vec{w})\)</span>来模拟得到<span class="math inline">\(p(\vec{z}|\vec{w})\)</span>。</p>
<h3 id="lda的联合分布">LDA的联合分布</h3>
<p>LDA的联合分布可以写成如下的式子（要记得这是个联合分布，所以Z都是已知的，所以<span class="math inline">\(\underline{\Theta}\)</span>和<span class="math inline">\(\underline{\Phi}\)</span>都被积分积掉了） <span class="math display">\[p(\vec{z},\vec{w}|\vec{\alpha},\vec{\beta})=p(\vec{w}|\vec{z},\vec{\beta})p(\vec{z}|\vec{\alpha})\]</span> 因为式子中的第一部分与<span class="math inline">\(\alpha\)</span>独立，第二部分与<span class="math inline">\(\beta\)</span>独立，所以两个式子可以分别处理。先看第一个分布<span class="math inline">\(p(\vec{w}|\vec{z})\)</span>，可以从观察到的词以及其主题的多项分布中生成： <span class="math display">\[ p(\vec{w}|\vec{z},\underline{\Phi})=\prod^W_{i=1}p(w_i|z_i)=\prod^W_{i=1}\varphi_{z_i,w_i}\]</span> 意思是，语料中的<span class="math inline">\(W\)</span>个词是根据主题<span class="math inline">\(z_i\)</span>观察到的独立多项分布。(我们把每个词看做独立的多项分布产生的结果，忽略顺序因素，所以没有多项分布的系数）。<span class="math inline">\(\varphi_{z_i,w_i}\)</span>是一个<span class="math inline">\(K\ast V\)</span>的矩阵，把词划分成主题和词汇表，公式如下： <span class="math display">\[ p(\vec{w}|\vec{z},\underline{\Phi})=\prod^K_{k=1}\prod_{i:z_i=k}p(w_i=t|z_i=k)=\prod^K_{k=1}\prod^V_{t=1}\varphi^{n^{(t)}_k}_{k,t}\]</span> <span class="math inline">\(n^{(t)}_k\)</span>代表了主题<span class="math inline">\(k\)</span>下词<span class="math inline">\(t\)</span>出现的次数。目标分布<span class="math inline">\(p(\vec{w}|\vec{z},\vec{\beta})\)</span>可以通过对<span class="math inline">\(\underline{\Phi}\)</span>求狄利克雷积分得到:</p>
<p><a href="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130312094831.png" target="_blank" rel="external"><img src="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130312094831.png" alt="QQ截图20130312094831"></a></p>
<p>类似地，主体分布<span class="math inline">\(p(\vec{z}|\vec{a})\)</span>也可以通过这种方法产生，<span class="math inline">\(\underline{\Theta}\)</span>为<span class="math inline">\(D*K\)</span>的矩阵，公式如下： <span class="math display">\[p(\vec{z}|\underline{\Theta})=\prod^W_{i=1}p(z_i|d_i)=\prod^M_{m=1}\prod^K_{k=1}p(z_i=k|d_i=m)=\prod^M_{m=1}\prod^K_{k=1}\theta^{n^{(k)}_m}_{m,k}\]</span> <span class="math inline">\(n^{(k)}_m\)</span>代表了文章<span class="math inline">\(m\)</span>下主题<span class="math inline">\(k\)</span>出现的次数。对<span class="math inline">\(\underline{\Theta}\)</span>求积分，我们得到:</p>
<p><a href="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130312094838.png" target="_blank" rel="external"><img src="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130312094838.png" alt="QQ截图20130312094838"></a></p>
<p>然后联合分布就变成了 <span class="math display">\[p(\vec{z},\vec{w}|\vec{\alpha},\vec{\beta})=\prod^K_{z=1}\frac{\Delta(\vec{n_z}+\vec{\beta})}{\Delta(\vec{\beta})}\cdot\prod^M_{m=1}\frac{\Delta(\vec{n_m}+\vec{\alpha})}{\Delta(\vec{\alpha})}\]</span></p>
<h3 id="完全条件分布full-conditional">完全条件分布(full conditional)</h3>
<p>我们令<span class="math inline">\(i=(m,n)\)</span>代表第<span class="math inline">\(m\)</span>篇文章中的第<span class="math inline">\(n\)</span>个词，<span class="math inline">\(\neg i\)</span>代表除去这个词之后剩下的其他词，令<span class="math inline">\(\vec{w}=\{w_i=t,\vec{w}_{\neg i}\}\)</span>，<span class="math inline">\(\vec{z}=\{z_i=k,\vec{z}_{\neg i}\}\)</span>，我们求得</p>
<p><a href="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130312094849.png" target="_blank" rel="external"><img src="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130312094849.png" alt="QQ截图20130312094849"></a></p>
<p>这个式子需要注意的：</p>
<ol style="list-style-type: decimal">
<li><p>因为忽略了<span class="math inline">\(p(w_i)\)</span>这个常数，所以后来的式子是<span class="math inline">\(\propto\)</span>成正比。</p></li>
<li><p>对于第<span class="math inline">\(m\)</span>篇文章中的第<span class="math inline">\(n\)</span>个词，其主题为<span class="math inline">\(k\)</span>。<span class="math inline">\(n^{(t)}_k=n^{(t)}_{k,\neg i}+1,n^{(k)}_m=n^{(k)}_{m,\neg i}+1\)</span>，对于其他文档和其他主题都没有影响。</p></li>
</ol>
<p>这个公式很漂亮，右边是<span class="math inline">\(p(topic|doc)\cdot p(word|topic)\)</span>，这个概率其实就是<span class="math inline">\(doc\rightarrow topic \rightarrow word\)</span>的路径概率，所以Gibbs Sampling 公式的物理意义就是在K条路径中采样。（图）</p>
<p><a href="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130312101904.png" target="_blank" rel="external"><img src="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130312101904.png" alt="QQ截图20130312101904"></a></p>
<h3 id="多项分布参数">多项分布参数</h3>
<p><a href="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130312100040.png" target="_blank" rel="external"><img src="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130312100040.png" alt="QQ截图20130312100040"></a></p>
<p><a href="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130312100050.png" target="_blank" rel="external"><img src="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130312100050.png" alt="QQ截图20130312100050"></a></p>
<p>根据图3和图4的Dirichlet-Multinomial结构，我们知道<span class="math inline">\(\vec{\theta_m}\)</span>和<span class="math inline">\(\vec{\phi_k}\)</span>的后验概率为:(令<span class="math inline">\(\mathcal{M}=\{\vec{w},\vec{z}\}\)</span>)（备注1）：</p>
<p><a href="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130312095426.png" target="_blank" rel="external"><img src="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130312095426.png" alt="QQ截图20130312095426"></a></p>
<p>最后，根据狄利克雷分布的期望<span class="math inline">\(&lt;Dir(\vec{a})&gt;=a_i/\sum_i{a_i}\)</span>（备注2），我们得到 <span class="math display">\[\phi_{k,t}=\frac{n^{(t)}_k+\beta_t}{\sum^V_{t=1}n^{(t)}_k+\beta_t}\]</span> <span class="math display">\[\theta_{m,k}=\frac{n^{(k)}_m+\alpha_k}{\sum^K_{k=1}n^{(k)}_m+\alpha_k}\]</span></p>
<p>最后，整个LDA算法的流程图为</p>
<p><a href="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130312094950.png" target="_blank" rel="external"><img src="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130312094950.png" alt="QQ截图20130312094950"></a> 备注： 1.狄利克雷分布的后验概率公式：</p>
<p><a href="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130312100417.png" target="_blank" rel="external"><img src="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130312100417.png" alt="QQ截图20130312100417"></a> 2.由于狄利克雷分布为： <span class="math display">\[Dir(\vec{p}|\vec{\alpha})=\frac{\Gamma(\sum^K_{k=1}\alpha_k)}{\prod^K_{k=1}\Gamma{(\alpha_k)}}\prod^K_{k=1}p_k^{\alpha_k-1}\]</span> 对于<span class="math inline">\(\vec{p}\)</span>中一项<span class="math inline">\(p_i\)</span>的期望为： <span class="math display">\[\begin{split} E(p_i) &amp;=\int^1_0 p_i\cdot Dir(\vec{p}|\vec{\alpha})dp \\ &amp;=\frac{\Gamma(\sum^K_{k=1}\alpha_k)}{\Gamma(\alpha_i)}\cdot\frac{\Gamma(\alpha_i+1)}{\Gamma(\sum^K_{k=1}\alpha_k+1)} \\ &amp;=\frac{\alpha_i}{\sum^K_{k=1}\alpha_k} \\\end{split}\]</span> 参考文献： 1.主要来自《Parameter estimation for text analysis》 2.《LDA数学八卦》</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;span style=&quot;color: #ff0000;&quot;&gt;2013年10月10日更新。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;LDA的概率图如下图1所示：&lt;a href=&quot;http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130312094645.png&quot;&gt;&lt;img src=&quot;http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130312094645.png&quot; alt=&quot;QQ截图20130312094645&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;参数的意思如图2所示：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130312094711.png&quot;&gt;&lt;img src=&quot;http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130312094711.png&quot; alt=&quot;QQ截图20130312094711&quot; /&gt;&lt;/a&gt; 根据模型，文章m的第n个词t是这样生成的：先从文章m的doc-topic分布中生成一个topic编号&lt;span class=&quot;math inline&quot;&gt;\(z_{m,n}\)&lt;/span&gt;，在根据编号第&lt;span class=&quot;math inline&quot;&gt;\(z_{m,n}\)&lt;/span&gt;个的topic-word分布中生成这个词，总够有&lt;span class=&quot;math inline&quot;&gt;\(K\)&lt;/span&gt;个topic，所以总的概率为： &lt;span class=&quot;math display&quot;&gt;\[ p(w_{m,n}=t|\vec{\theta}_m,\underline{\Phi})=\sum^K_{k=1}p(w_{m,n}=t|\vec{\phi}_k)p(z_{m,n}=k|\vec{\theta}_m)\]&lt;/span&gt; 如果我们写出这篇文章的complete-data的联合分布（&lt;span style=&quot;color: #ff0000;&quot;&gt;意思就是所以变量都已知的情况下&lt;/span&gt;），那么式子就是这样的：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130312094748.png&quot;&gt;&lt;img src=&quot;http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130312094748.png&quot; alt=&quot;QQ截图20130312094748&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;通过对&lt;span class=&quot;math inline&quot;&gt;\(\vec{\vartheta_m}\)&lt;/span&gt;（doc-topic分布）和&lt;span class=&quot;math inline&quot;&gt;\(\underline{\Phi}\)&lt;/span&gt;（topic-word分布）积分以及&lt;span class=&quot;math inline&quot;&gt;\(z_{m,n}\)&lt;/span&gt;求和，我们可以求得&lt;span class=&quot;math inline&quot;&gt;\(\vec{w_m}\)&lt;/span&gt;的边缘分布：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130312094757.png&quot;&gt;&lt;img src=&quot;http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130312094757.png&quot; alt=&quot;QQ截图20130312094757&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;(&lt;span style=&quot;color: #ff0000;&quot;&gt;实际上这个边缘分布是求不出来的&lt;/span&gt;，因为&lt;span class=&quot;math inline&quot;&gt;\(z_{m,n}\)&lt;/span&gt;是隐藏变量，从而导致&lt;span class=&quot;math inline&quot;&gt;\(\underline{\vartheta}\)&lt;/span&gt;与&lt;span class=&quot;math inline&quot;&gt;\(\underline{\Phi}\)&lt;/span&gt;存在耦合现象，无法积分得到。要注意联合分布和边缘分布对Z乘积与加和的区别）&lt;/p&gt;
&lt;p&gt;因为一个语料库有很多篇文章，而且文章之间都是相互独立的，所以整个语料库的似然为 &lt;span class=&quot;math display&quot;&gt;\[ p(\mathcal{W}|\vec{\alpha},\vec{\beta})=\prod^{M}_{m=1}p(\vec{w_m}|\vec{\alpha},\vec{\beta})\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;虽然LDA（latent Dirichlet allocation)是个相对简单的模型，对它直接推断一般也是不可行的，所以我们要采用近似推断的方法，比如Gibbs sampling。&lt;/p&gt;
    
    </summary>
    
      <category term="学术" scheme="http://crescentluna.github.io/categories/%E5%AD%A6%E6%9C%AF/"/>
    
    
      <category term="机器学习" scheme="http://crescentluna.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="LDA" scheme="http://crescentluna.github.io/tags/LDA/"/>
    
  </entry>
  
  <entry>
    <title>有趣的三门问题（蒙提霍尔问题）</title>
    <link href="http://crescentluna.github.io/2013/03/01/Monty%20Hall%20problem/"/>
    <id>http://crescentluna.github.io/2013/03/01/Monty Hall problem/</id>
    <published>2013-03-01T05:17:04.000Z</published>
    <updated>2017-02-21T18:24:27.407Z</updated>
    
    <content type="html"><![CDATA[<p>三门问题（Monty Hall problem），是一个源自博弈论的数学游戏问题，大致出自美国的电视游戏节目Let’s Make a Deal。问题的名字来自该节目的主持人蒙提·霍尔（Monty Hall）。问题非常的有意思^_^，给出叙述如下：</p>
<p>在三扇门中的某扇门以后有一个奖品，选中这扇门就能拿到门后的奖品。你选定了一扇门，具体地说，假设你选择了1号门。这时候主持人蒙提·霍尔会打开剩下两扇门的其中一扇，你看到门后没有奖品。这时候他给你一个机会选择要不要换另外一扇没有打开的门。你是选择换还是不换呢？</p>
<p>答：因为我之前就选了，换或者不换机会都是均等的，所以换不换无关紧要╮(╯▽╰)╭。 ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ 真的是这样么？仔细分析一下，游戏过程中你做了2个操作： 第一，你选择了一扇门。第二，你选择了换门或者不换。 定义事件<span class="math inline">\(A\)</span>为你第一次就选中奖品。 定义事件<span class="math inline">\(B\)</span>为你换门选中奖品。 那么<span class="math inline">\(A^c\)</span>为集合A的余集，即第一次没有选中奖品。同理，<span class="math inline">\(B^c\)</span>为换门没有选中奖品。整个游戏过程中，<span class="math inline">\(A\)</span>或<span class="math inline">\(A^c\)</span>先发生，<span class="math inline">\(B\)</span>或<span class="math inline">\(B^c\)</span>再发生。 显而易见的是 <span class="math display">\[P(A)=1/3,P(A^c)=2/3\]</span> 要注意的是，在第一次操作之后，还有一件事——主持人打开了一扇没有奖品的门。值得注意的是，这里主持人的动作是跟你第一次选择有关系的：</p>
<ol style="list-style-type: decimal">
<li><p>如果你一开始就选中了奖品，即事件<span class="math inline">\(A\)</span>发生了，那么他就在剩下的两扇没有奖品的门之间任选一门打开。接下来，如果你选择换门，那么抽中的概率为0，不换抽中的概率为1。 即 <span class="math display">\[P(B|A)=0,P(B^c|A)=1\]</span> 因为事件<span class="math inline">\(B\)</span>或<span class="math inline">\(B^c\)</span>是在事件A发生之后发生的，所以这里的概率是基于事件<span class="math inline">\(A\)</span>的条件概率。</p></li>
<li><p>如果你开始没有选中奖品，即事件<span class="math inline">\(A^c\)</span>发生了，那么他只能打开另一扇没有奖品的门。这时候，如果你选择换门，那么抽中的概率为1，不换抽中的概率为0。 即 <span class="math display">\[P(B|A^c)=1,P(B^c|A^c)=0\]</span> 这里的概率是基于事件<span class="math inline">\(A^c\)</span>的条件概率。</p></li>
</ol>
<p>由上我们可以发现一点，事件<span class="math inline">\(A\)</span>会影响事件<span class="math inline">\(B\)</span>的概率，即事件<span class="math inline">\(A\)</span>和事件<span class="math inline">\(B\)</span>并不是相互独立的。<span class="math inline">\(A^c\)</span>和<span class="math inline">\(B^c\)</span>也是同理。 于是，利用全概率公式，我们可以求得换门选中奖品的概率为 <span class="math display">\[P(B)=P(B|A)P(A)+P(B|A^c)P(A^c)=0*1/3+1*2/3=2/3\]</span> <span class="math display">\[P(B^c)=P(B^c|A)P(A)+P(B^c|A^c)P(A^c)=1*1/3+0*2/3=1/3\]</span> 所以事实上你换门能得到奖品的概率为2/3，是不换门的2倍<del>（懂点数学真好啊）</del>。是不是和直觉不太一样？个人认为，直观上觉得<span class="math inline">\(P(B)=1/2\)</span>的原因是忽略了第一次选择时，通过主持人的动作改变了换门的事件概率这一客观过程。 <span style="color: #ff0000;">2013年7月3日更新</span><br>
从信息论的角度来说，B事件的熵为H(B)，在A事件发生之后B事件的条件熵为H(B|A)，可以证明 <span class="math display">\[H(B) \geq H(B|A) \]</span> 也就是说，在给予了A的信息之后，B的不确定性下降了。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;三门问题（Monty Hall problem），是一个源自博弈论的数学游戏问题，大致出自美国的电视游戏节目Let’s Make a Deal。问题的名字来自该节目的主持人蒙提·霍尔（Monty Hall）。问题非常的有意思^_^，给出叙述如下：&lt;/p&gt;
&lt;p&gt;在三扇门中的
    
    </summary>
    
      <category term="杂项" scheme="http://crescentluna.github.io/categories/%E6%9D%82%E9%A1%B9/"/>
    
    
  </entry>
  
</feed>
