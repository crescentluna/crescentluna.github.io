<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>《Gibbs Sampling for the UniniTiated》阅读笔记(中)---一个朴素贝叶斯文档模型例子 | 心怀畏惧</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="《Gibbs Sampling for the UniniTiated》阅读笔记结构：

 参数估计方法及Gibbs Sampling简介
一个朴素贝叶斯文档模型例子
连续型参数求积分的思考



这篇是中篇，介绍一个非常简单的朴素贝叶斯文档模型生成的例子，用来说明Gibbs Sampler具体是如何构造的。
文档生成的建模过程
首先我们有一批文档，文档里面有很多单词，这些单词都是无顺序可交换的（">
<meta property="og:type" content="article">
<meta property="og:title" content="《Gibbs Sampling for the UniniTiated》阅读笔记(中)---一个朴素贝叶斯文档模型例子">
<meta property="og:url" content="http://crescentluna.github.io/2013/06/29/Gibbs Sampling for the UniniTiated-2/index.html">
<meta property="og:site_name" content="心怀畏惧">
<meta property="og:description" content="《Gibbs Sampling for the UniniTiated》阅读笔记结构：

 参数估计方法及Gibbs Sampling简介
一个朴素贝叶斯文档模型例子
连续型参数求积分的思考



这篇是中篇，介绍一个非常简单的朴素贝叶斯文档模型生成的例子，用来说明Gibbs Sampler具体是如何构造的。
文档生成的建模过程
首先我们有一批文档，文档里面有很多单词，这些单词都是无顺序可交换的（">
<meta property="og:image" content="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130629204136.png">
<meta property="og:image" content="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130629204149.png">
<meta property="og:image" content="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130629204201.png">
<meta property="og:updated_time" content="2017-02-21T18:24:27.407Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="《Gibbs Sampling for the UniniTiated》阅读笔记(中)---一个朴素贝叶斯文档模型例子">
<meta name="twitter:description" content="《Gibbs Sampling for the UniniTiated》阅读笔记结构：

 参数估计方法及Gibbs Sampling简介
一个朴素贝叶斯文档模型例子
连续型参数求积分的思考



这篇是中篇，介绍一个非常简单的朴素贝叶斯文档模型生成的例子，用来说明Gibbs Sampler具体是如何构造的。
文档生成的建模过程
首先我们有一批文档，文档里面有很多单词，这些单词都是无顺序可交换的（">
<meta name="twitter:image" content="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130629204136.png">
  
    <link rel="alternative" href="/atom.xml" title="心怀畏惧" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link rel="stylesheet" href="/css/style.css">
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
<script type="text/javascript">
var _bdhmProtocol = (("https:" == document.location.protocol) ? " https://" : " http://");
document.write(unescape("%3Cscript src='" + _bdhmProtocol + "hm.baidu.com/h.js%3F1fc57dc5ccfaeae31f8e295269b6fa04' type='text/javascript'%3E%3C/script%3E"));
</script>


  
<!-- Google Analytics -->
<script type="text/javascript">
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-57404093-1', 'auto');
ga('send', 'pageview');

</script>
<!-- End Google Analytics -->

  <script src="https://cdn1.lncld.net/static/js/av-min-1.2.1.js"></script>
  <script>AV.initialize("57nirzk8g437patyg9434lcvmpkat6e5lu5mixmrwqa8l3ao", "rsqg6iiq58eyeg8c6c6uue48crtwt5quuq5dmjaahfcjlkq8");</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">心怀畏惧</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">Do not go gentle into that good night</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">主页</a>
        
          <a class="main-nav-link" href="/archives">所有文章</a>
        
          <a class="main-nav-link" href="/about">关于</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="http://www.baidu.com/baidu" method="get" accept-charset="utf-8" class="search-form">
          <input type="search" name="word" maxlength="20" class="search-form-input" placeholder="Search">
          <input type="submit" value="" class="search-form-submit">
          <input name=tn type=hidden value="bds">
          <input name=cl type=hidden value="3">
          <input name=ct type=hidden value="2097152">
          <input type="hidden" name="si" value="crescentluna.github.io">
        </form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-Gibbs Sampling for the UniniTiated-2" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2013/06/29/Gibbs Sampling for the UniniTiated-2/" class="article-date">
  <time datetime="2013-06-29T04:49:12.000Z" itemprop="datePublished">2013-06-29</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/学术/">学术</a>►<a class="article-category-link" href="/categories/学术/Gibbs-Sampling笔记/">Gibbs Sampling笔记</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      《Gibbs Sampling for the UniniTiated》阅读笔记(中)---一个朴素贝叶斯文档模型例子
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>《Gibbs Sampling for the UniniTiated》阅读笔记结构：</p>
<ol style="list-style-type: decimal">
<li> <a href="http://www.crescentmoon.info/?p=504" target="_blank" rel="external">参数估计方法及Gibbs Sampling简介</a></li>
<li><a href="http://www.crescentmoon.info/?p=525" target="_blank" rel="external">一个朴素贝叶斯文档模型例子</a></li>
<li><a href="http://www.crescentmoon.info/?p=548" target="_blank" rel="external">连续型参数求积分的思考</a></li>
</ol>
<hr>

<p>这篇是中篇，介绍一个非常简单的朴素贝叶斯文档模型生成的例子，用来说明Gibbs Sampler具体是如何构造的。</p>
<h2 id="文档生成的建模过程">文档生成的建模过程</h2>
<p>首先我们有一批文档，文档里面有很多单词，这些单词都是无顺序可交换的（词袋模型），这些文档分成两类，类标签为0或者1。给予一篇未标记的文档<span class="math inline">\(W_j\)</span>，我们要做的工作就是预测文档的类标签是<span class="math inline">\(L_j=0\)</span>还是<span class="math inline">\(L_j=1\)</span>。为了方便起见，我们定了类标签所表示的类<span class="math inline">\(\mathbb{C}_0={W_j|L_j=0}\)</span>和<span class="math inline">\(\mathbb{C}_1={W_j|L_j=1}\)</span>。一般来说预测这种事都是选择最有可能发生的，即找到<span class="math inline">\(W_j\)</span>的后验概率<span class="math inline">\(P(L_j|W_j)\)</span>最大的标签<span class="math inline">\(L_j\)</span>。使用贝叶斯公式 <span class="math display">\[\begin{equation}
\begin{split}
L_j=\arg \max \limits_{L}P(L|W_j)&amp; =\arg \max \limits_{L}\frac{P(W_j|L)P(L)}{P(W_j)}\\&amp; =\arg \max \limits_{L} P(W_j|L)P(L) \\\end{split}
\end{equation}\]</span> 因为分母<span class="math inline">\(P(W_j)\)</span>与<span class="math inline">\(L\)</span>无关所以删去了。 通过贝叶斯公式的转换，我们可以想象这些文档的生成过程。首先，我们选择文档的类标签<span class="math inline">\(L_j\)</span>;假设这个过程是通过投硬币完成的（正面概率为<span class="math inline">\(\pi=P(L_j=1)\)</span> )，正式地来说，就是服从贝努利分布 <span class="math display">\[\begin{equation}L_j \sim Bernoulli(\pi)\end{equation}\]</span> 然后，对于文档上<span class="math inline">\(R_j\)</span>个“词位”中的每一个，我们根据一个概率分布<span class="math inline">\(\theta\)</span>，随机独立地抽样一个词<span class="math inline">\(w_i\)</span>。因为每个类生成词的<span class="math inline">\(\theta\)</span>分布都不同，所以应该有<span class="math inline">\(\theta_1\)</span>和<span class="math inline">\(\theta_2\)</span>，具体地生成词的时候，我们根据文档的标签<span class="math inline">\(L_j\)</span>来决定由哪个类来生成 <span class="math display">\[\begin{equation}
W_j \sim Multinomial(R_j,\theta_{L_j})
\end{equation}\]</span> <a id="more"></a></p>
<h2 id="先验">先验</h2>
<p>上面提到的参数<span class="math inline">\(\pi\)</span>和<span class="math inline">\(\theta\)</span>还有各自的先验知识。我们假设参数<span class="math inline">\(\pi\)</span>是从一个参数为<span class="math inline">\(\gamma_{\pi_1}\)</span>和<span class="math inline">\(\gamma_{\pi_0}\)</span>的Beta分布中采样出来的。这里<span class="math inline">\(\gamma_\pi=&lt;\gamma_{\pi_1},\gamma_{\pi_0}&gt;\)</span> 被称作超参数，因为它们是先验模型的参数，是为了确定模型的参数而存在的。类似地，就像Beta分布是贝努利分布的共轭先验一样，<span class="math inline">\(\theta\)</span> 参数的确定来自于参数为<span class="math inline">\(\gamma_\theta\)</span>的Dirichlet分布。公式如下： <span class="math display">\[\begin{equation}\pi \sim Beta(\gamma_\pi)\end{equation}\]</span><span class="math display">\[\begin{equation}\theta \sim Dirichlet(\gamma_\theta)\end{equation}\]</span> 选择Beta分布和Dirichlet分布作为先验分布，是为了数学上的方便。所以的参数定义见图1和图2(这个概率图非常好看，是从别人的博客对这篇文章的介绍中借来的，来自于[3]）.</p>
<p><a href="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130629204136.png" target="_blank" rel="external"><img src="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130629204136.png" alt="QQ截图20130629204136"></a></p>
<p><a href="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130629204149.png" target="_blank" rel="external"><img src="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130629204149.png" alt="QQ截图20130629204149"></a></p>
<h2 id="状态空间以及初始化">状态空间以及初始化</h2>
<p>状态空间。在前面提到过一样，Gibbs 抽样器要做的是遍历模型中<span class="math inline">\(k\)</span>维状态空间，对里面所有的变量采样。在当前模型的状态空间中存在以下变量：</p>
<ul>
<li><p>1个标量变量<span class="math inline">\(\pi\)</span></p></li>
<li><p>2个向量变量<span class="math inline">\(\theta_0\)</span>和<span class="math inline">\(\theta_1\)</span>。</p></li>
<li><p>N篇文档所对应的二元标签向量变量<span class="math inline">\(\mathbf{L}\)</span>。</p></li>
</ul>
<p>还有每篇文档里的词向量<span class="math inline">\(W_j\)</span>，但是他们是被观察到的数据，值已经知道了（这就是为啥图2中的概率图<span class="math inline">\(W_{jk}\)</span>是实心的）。</p>
<p>初始化。初始化就是一篇文档的生成过程，不细讲了。</p>
<h2 id="产生联合分布">产生联合分布</h2>
<p>对于之前提到的每次循环<span class="math inline">\(t=1\ldots T\)</span>，我们都像上篇的式子(11)中提到的那样，利用其他变量的条件分布对状态空间中的每个变量更新。具体过程如下：</p>
<ol style="list-style-type: decimal">
<li><p>写出所有变量的联合分布</p></li>
<li><p>简化联合分布的公式</p></li>
<li><p>确定上篇的式（11）中条件分布的公式</p></li>
<li><p>给出采样器伪码的最终形式</p></li>
</ol>
<h2 id="写出联合分布">写出联合分布</h2>
<p>根据模型，我们可以得到整个文档的联合分布<span class="math inline">\(P(\mathbb{C},\mathbf{L},\pi,\theta_0,\theta_1;\gamma_{\pi_1},\gamma_{\pi_0},\gamma_{\theta})\)</span>。 根据模型的生成过程和概率图，我们可以将联合分布分解: <span class="math display">\[\begin{equation}
P(\pi|\gamma_{\pi_1},\gamma_{\pi_0})P(\mathbf{L}|\pi)P(\theta_0|\gamma_{\theta})P(\theta_1|\gamma_{\theta})P(\mathbb{C}_0|\theta_0,\mathbf{L})P(\mathbb{C}_1|\theta_1,\mathbf{L})
\end{equation}\]</span> 然后将一个一个地来解释这些式子(从引用了一部分）：</p>
<ul>
<li><p><span class="math inline">\(P(\pi|\gamma_{\pi_1},\gamma_{\pi_0})\)</span>。这个式子是根据超参数为<span class="math inline">\(\gamma_{\pi_1}\)</span>和<span class="math inline">\(\gamma_{\pi_0}\)</span>的Beta分布中采样出<span class="math inline">\(\pi\)</span>的公式，根据Beta 分布的定义，概率为 <span class="math display">\[\begin{equation}P(\pi|\gamma_{\pi_1},\gamma_{\pi_0})=\frac{\Gamma(\gamma_{\pi_1}+\gamma_{\pi_0})}{\Gamma(\gamma_{\pi_1})\Gamma(\gamma_{\pi_0})}\pi^{\gamma_{\pi_1}-1}(1-\pi)^{\gamma_{\pi_0}-1} \end{equation}\]</span> 因为右边式子的<span class="math inline">\(\frac{\Gamma(\gamma_{\pi_1}+\gamma_{\pi_0})}{\Gamma(\gamma_{\pi_1})\Gamma(\gamma_{\pi_0})}\)</span> 是个常量(设为<span class="math inline">\(c\)</span>)，而整个联合分布最终要把所有的常量归一化，所以在这里先不管它，把式子写作 <span class="math display">\[\begin{equation}
P(\pi|\gamma_{\pi_1},\gamma_{\pi_0}) \propto \pi^{\gamma_{\pi_1}-1}(1-\pi)^{\gamma_{\pi_0}-1}
\end{equation}\]</span></p></li>
<li><p><span class="math inline">\(P(\mathbf{L}|\pi)\)</span>。第二个式子是由参数<span class="math inline">\(\pi\)</span>或<span class="math inline">\(1-\pi\)</span>生成标签变量1或0的过程，要注意一共有<span class="math inline">\(N\)</span>篇文档，每篇文档的标签生成都是相互独立的： <span class="math display">\[\begin{equation}
\begin{split}P(\mathbf{L}|\pi)&amp; =\prod^N_{n=1}\pi^{L_n}(1-\pi)^{N-L_n} \\ &amp;=\pi^{C_1}(1-\pi)^{C_0} \\ \end{split} \end{equation}\]</span> 其中<span class="math inline">\(C_0\)</span>是和<span class="math inline">\(C_1\)</span>分别是标签为0和1的文档数目。</p></li>
<li><p><span class="math inline">\(P(\theta_0|\gamma_{\theta})\)</span>和<span class="math inline">\(P(\theta_1|\gamma_{\theta})\)</span>。这个式子是根据超参数为<span class="math inline">\(\gamma_{\theta}\)</span>的Dirichlet分布中采样出<span class="math inline">\(\theta_0\)</span>和<span class="math inline">\(\theta_1\)</span>的词分布的过程。因为<span class="math inline">\(\theta_0\)</span>和<span class="math inline">\(\theta_1\)</span>相互独立，生成的过程是一样的，所以为了简单起见，我们暂时舍去类的下标，公式如下： <span class="math display">\[\begin{equation}
\begin{split}
P(\theta|\gamma_{\theta})&amp;=\frac{\Gamma{(\sum^V_{i=1}\gamma_{\theta_i})}}{\prod^V_{i=1}\Gamma{(\gamma_{\theta_i})}}\prod^V_{i=1}\theta_i^{\gamma_{\theta_i}-1}\\&amp; = c&#39;\prod^V_{i=1}\theta_i^{\gamma_{\theta_i}-1} \\&amp; \propto \prod^V_{i=1}\theta_i^{\gamma_{\theta_i}-1} \\\end{split} \end{equation}\]</span> <span class="math inline">\(\gamma_{\theta_i}\)</span>代表了向量<span class="math inline">\(\gamma_{\theta}\)</span>第<span class="math inline">\(i\)</span>维的值。类似地，<span class="math inline">\(\theta_i\)</span>代表向量<span class="math inline">\(\theta\)</span>地<span class="math inline">\(i\)</span>维的值，其物理意义就是该分布上第<span class="math inline">\(i\)</span>个词被生成的概率。<span class="math inline">\(c&#39;\)</span>是另一个归一化因子。</p></li>
<li><p><span class="math inline">\(P(\mathbb{C}_0|\theta_0,\mathbf{L})\)</span>和<span class="math inline">\(P(\mathbb{C}_1|\theta_1,\mathbf{L})\)</span>。这俩式子是具体地生成该类的所有文档中的所有词的过程。首先要求来看对于单独一个文档<span class="math inline">\(n\)</span>，产生所有word也就是<span class="math inline">\(W_n\)</span>的概率。假设对于某个文档，<span class="math inline">\(\theta=(0.2,0.5,0.3)\)</span>，意思就是word1产生概率为0.2，word2产生概率为0.5，word3的概率为0.3。假如这个文档里word1有2个，word2有3个，word3有2个，则这个文档的产生概率就是(0.2<em>0.2)</em>(0.5<em>0.5</em>0.5)<em>(0.3</em>0.3)。所以按照这个道理，一个文档生成的联合概率如下： <span class="math display">\[\begin{equation}P(W_n|\mathbf{L},\theta_{L_n})=\prod^V_{i=1}\theta_i^{W_{ni}} \end{equation}\]</span> 这里<span class="math inline">\(\theta_{L_n}\)</span>代表了文档<span class="math inline">\(n\)</span>所在的类（<span class="math inline">\(\theta_0\)</span>或<span class="math inline">\(\theta_1\)</span>），<span class="math inline">\(\theta_i\)</span>像之前提过的那样，代表产生第<span class="math inline">\(i\)</span>个词的概率，而指数<span class="math inline">\(W_{ni}\)</span>为该词出现的频率。现在我们有一篇文档的生成概率了，然后我们把一个类别下面的所有文档都合并起来(因为都是相互独立的么) <span class="math display">\[\begin{equation}\begin{split} P(\mathbb{C}_x|\mathbf{L},\theta_x)&amp; =\prod_{n\in \mathbb{C}_x} \prod^V_{i=1}\theta_i^{W_{ni}} \\ &amp;=\prod^V_{i=1}\theta_{x,i}^{\mathcal{N}_{\mathbb{C}_x(i)}}\end{split}\end{equation}\]</span> 其中<span class="math inline">\(x\)</span>代表了类的标号（这里是0或1），<span class="math inline">\(\mathcal{N}_{\mathbb{C}_x}\)</span>代表了类标号为<span class="math inline">\(x\)</span>的所有文档生成词<span class="math inline">\(i\)</span>的数目。</p></li>
</ul>
<h3 id="先验选择和简化联合分布">先验选择和简化联合分布</h3>
<p>然后我们将先验知识 <span class="math inline">\(P(\mathbf{L}|\pi)\)</span>与观察到的evidence<span class="math inline">\(P(\pi|\gamma_{\pi_1},\gamma_{\pi_0})\)</span> 相乘，我们可以看到估计的参数<span class="math inline">\(\pi\)</span> 是如何被观察到的数据所影响的。利用式子(7) 和(9) 我们得到 <span class="math display">\[\begin{equation}
\begin{split}
P(\pi|\mathbf{L};\gamma_{\pi_1},\gamma_{\pi_0}) &amp;=P(\mathbf{L}|\pi)P(\pi|\gamma_{\pi_1},\gamma_{\pi_0}) \\&amp; \propto [\pi^{C_1}(1-\pi)^{C_0}][\pi^{\gamma_{\pi_1}-1}(1-\pi)^{\gamma_{\pi_0}-1}] \\&amp; \propto \pi^{C_1+\gamma_{\pi_1}-1}(1-\pi)^{C_0+\gamma_{\pi_0}-1} \\
\end{split}
\end{equation}\]</span> 对于<span class="math inline">\(\theta\)</span>,类似地我们结合(10）和（12）,有 <span class="math display">\[\begin{equation}
\begin{split}P(\theta|W_n;\gamma_\theta)&amp; =P(W_n|\theta)P(\theta|\gamma_\theta) \\&amp; \propto \prod^V_{i=1}\theta_i^{W_{ni}} \prod^V_{i=1}\theta_i^{\gamma_{\theta_i}-1} \\&amp; \propto \prod^V_{i=1}\theta_i^{W_{ni}+\gamma_{\theta_i}-1} \\\end{split}\end{equation}\]</span> 然后从一篇文章推广到某类下的所有文章，我们有 <span class="math display">\[\begin{equation}
P(\theta_x|\mathbb{C}_x;\gamma_\theta) \propto \prod^V_{i=1}\theta_{x,i}^{\mathcal{N}_{\mathbb{C}_x}(i)+\gamma_{\theta_i}-1}
\end{equation}\]</span> 最后，我们把所有的因式都乘起来，再令<span class="math inline">\(\mu=&lt;\gamma_{\pi_1},\gamma_{\pi_0},\gamma_{\theta}&gt;\)</span>,写出简化后的全联合概率: <span class="math display">\[\begin{equation}
P(\mathbb{C},\mathbf{L},\pi,\theta_0,\theta_1;\mu) \propto \pi^{C_1+\gamma_{\pi_1}-1}(1-\pi)^{C_0+\gamma_{\pi_0}-1} \prod^V_{i=1}\theta_{0,i}^{\mathcal{N}_{\mathbb{C}_0}+\gamma_{\theta_i}-1} \theta_{1,i}^{\mathcal{N}_{\mathbb{C}_1}+\gamma_{\theta_i}-1}
\end{equation}\]</span></p>
<h3 id="将隐含变量pi积出">将隐含变量<span class="math inline">\(\pi\)</span>积出</h3>
<p>为了进一步地简化模型，我们可以对参数<span class="math inline">\(\pi\)</span>求积分，从而消去这个参数。 <span class="math display">\[\begin{equation}
\begin{split}&amp; P(\mathbb{C},\mathbf{L},\theta_0,\theta_1;\mu)\\ =&amp;\int_\pi P(\mathbb{C},\mathbf{L},\pi,\theta_0,\theta_1;\mu) d\pi \\=&amp;\int_\pi P(\pi|\gamma_{\pi_1},\gamma_{\pi_0})P(\mathbf{L}|\pi)P(\theta_0|\gamma_{\theta})P(\theta_1|\gamma_{\theta})P(\mathbb{C}_0|\theta_0,\mathbf{L})P(\mathbb{C}_1|\theta_1,\mathbf{L}) d\pi \\ =&amp;P(\theta_0|\gamma_{\theta})P(\theta_1|\gamma_{\theta})P(\mathbb{C}_0|\theta_0,\mathbf{L})P(\mathbb{C}_1|\theta_1,\mathbf{L}) \int_\pi P(\pi|\gamma_{\pi_1},\gamma_{\pi_0})P(\mathbf{L}|\pi) d\pi \\\end{split}\end{equation}\]</span> 这里我们对积分式子内的<span class="math inline">\(\pi\)</span>求积分 <span class="math display">\[\begin{equation}
\begin{split}&amp;\int_\pi P(\pi|\gamma_{\pi_1},\gamma_{\pi_0})P(\mathbf{L}|\pi) d\pi \\ =&amp;\frac{\Gamma(\gamma_{\pi_1}+\gamma_{\pi_0})}{\Gamma(\gamma_{\pi_1})\Gamma(\gamma_{\pi_0})}\pi^{\gamma_{\pi_1}-1}(1-\pi)^{\gamma_{\pi_0}-1} \pi^{C_1}(1-\pi)^{C_0} d\pi \\=&amp; \frac{\Gamma(\gamma_{\pi_1}+\gamma_{\pi_0})}{\Gamma(\gamma_{\pi_1})\Gamma(\gamma_{\pi_0})} \int_\pi \pi^{C_1+\gamma_{\pi_1}-1}(1-\pi)^{C_0+\gamma_{\pi_0}-1} d\pi \\\end{split}\end{equation}\]</span> 要注意这里式子右边的积分部分其实是尚未归一化的参数为<span class="math inline">\(C_1+\gamma_{\pi_1}\)</span>和<span class="math inline">\(C_0+\gamma_{\pi_0}\)</span> 的Beta分布求积分，所以我们只需要先补上Beta(<span class="math inline">\(C_1+\gamma_{\pi_1}\)</span>,<span class="math inline">\(C_0+\gamma_{\pi_0}\)</span>)归一化的常数项即可: <span class="math display">\[\begin{equation}\frac{\Gamma(C_0+C_1+\gamma_{\pi_1}+\gamma_{\pi_0})}{\Gamma(C_1+\gamma_{\pi_1})\Gamma(C_0+\gamma_{\pi_0})}\end{equation}\]</span> 令<span class="math inline">\(N=C_0+C_1\)</span>，我们得到（<strong>注意这里应为Beta归一化的常数的倒数！！！原文中写反了</strong>） <span class="math display">\[\begin{equation}
\int_\pi P(\pi|\gamma_{\pi_1},\gamma_{\pi_0})P(\mathbf{L}|\pi) d\pi=\frac{\Gamma(\gamma_{\pi_1}+\gamma_{\pi_0})}{\Gamma(\gamma_{\pi_1})\Gamma(\gamma_{\pi_0})} \frac{\Gamma(C_1+\gamma_{\pi_1})\Gamma(C_0+\gamma_{\pi_0})}{\Gamma(C_0+C_1+\gamma_{\pi_1}+\gamma_{\pi_0})}
\end{equation}\]</span> 最后得出 <span class="math display">\[\begin{equation}P(\mathbb{C},\mathbf{L},\theta_0,\theta_1;\mu) \propto \frac{\Gamma(\gamma_{\pi_1}+\gamma_{\pi_0})}{\Gamma(\gamma_{\pi_1})\Gamma(\gamma_{\pi_0})} \frac{\Gamma(C_1+\gamma_{\pi_1})\Gamma(C_0+\gamma_{\pi_0})}{\Gamma(C_0+C_1+\gamma_{\pi_1}+\gamma_{\pi_0})}\prod^V_{i=1}\theta_{0,i}^{\mathcal{N}_{\mathbb{C}_0}(i)+\gamma_{\theta_i}-1} \theta_{1,i}^{\mathcal{N}_{\mathbb{C}_1}(i)+\gamma_{\theta_i}-1}\end{equation}\]</span> 这里很容易想到的是，既然我们可以通过对<span class="math inline">\(\pi\)</span>求积分来简化联合分布，为什么不用这种方法把里面的参数全积分了呢？详细请见下篇的解释。</p>
<h2 id="构建gibbs-sampler">构建Gibbs Sampler</h2>
<p>根据Gibbs 抽样的定义，我们每次都从条件分布中抽样出<span class="math inline">\(Z_i\)</span>的新值： <span class="math display">\[\begin{equation}
P(Z_i|z_1^{(t+1)},\ldots,z^{(t+1)}_{i-1},z^{(t)}_{i+1},\ldots,z^{t}_k)
\end{equation}\]</span> 替换成具体的例子，在分配<span class="math inline">\(L_1^{(t+1)}\)</span>的值时，我们需要计算条件分布 <span class="math display">\[\begin{equation}P(L_1|L_2^{(t)},\ldots,L_N^{(t)},\mathbb{C},\theta_0^{(t)},\theta_1^{(t)};\mu)\end{equation}\]</span> 然后我们算<span class="math inline">\(L_2{(t+1)}\)</span>的值， <span class="math display">\[\begin{equation}P(L_2|L_1^{(t+1)},L_3^{(t)},\ldots,L_N^{(t)},\mathbb{C},\theta_0^{(t)},\theta_1^{(t)};\mu)\end{equation}\]</span> 以此类推直到计算完<span class="math inline">\(L_N^{(t+1)}\)</span>为止。然后我们计算<span class="math inline">\(\theta_0\)</span>的值, <span class="math display">\[\begin{equation}P(\theta_0^{(t+1)}|L_1^{(t+1)},L_2^{(t+1)},\ldots,L_N^{(t+1)},\mathbb{C},\theta_1^{(t)};\mu)\end{equation}\]</span> 最后是<span class="math inline">\(\theta_1\)</span> <span class="math display">\[\begin{equation}P(\theta_1^{(t+1)}|L_1^{(t+1)},L_2^{(t+1)},\ldots,L_N^{(t+1)},\mathbb{C},\theta_0^{(t)};\mu)\end{equation}\]</span> 在循环<span class="math inline">\(t\)</span>开始的时候，我们拥有一些信息，这些信息包括：每个文档中词的数目，标签为0的文档数，标签为1的文档数，所有标签为0的文档的词数目，所有标签为1的文档的词数目，每个文档的当前标签，当前的<span class="math inline">\(\theta_1\)</span>和<span class="math inline">\(\theta_0\)</span>值，等等。当我们想得到文档<span class="math inline">\(j\)</span>的新标签时，我们暂时地移除所有当前文档的信息（包括词数目和标签信息），然后通过余下的信息得出<span class="math inline">\(L_j=0\)</span> 的条件概率和<span class="math inline">\(L_j=1\)</span>的条件概率，最后根据这俩概率的相对比例采样得到新的<span class="math inline">\(L_j{(t+1)}\)</span>。对<span class="math inline">\(\theta\)</span>采样时也是如此。</p>
<h3 id="对文档标签mathbfl采样">对文档标签<span class="math inline">\(\mathbf{L}\)</span>采样</h3>
<p>接下来我们具体地看看那如何对文档标签采样。根据之前的条件概率公式，我们得到 <span class="math display">\[\begin{equation}
\begin{split}
P(L_j|\mathbf{L}^{(-j)},\mathbb{C}^{(-j)},\theta_0,\theta_1;\mu)&amp; =\frac{P(L_j,W_j,\mathbf{L}^{(-j)},\mathbb{C}^{(-j)},\theta_0,\theta_1;\mu)}{P(\mathbf{L}^{(-j)},\mathbb{C}^{(-j)},\theta_0,\theta_1;\mu)} \\
&amp;=\frac{P(\mathbf{L},\mathbb{C},\theta_0,\theta_1;\mu)}{P(\mathbf{L}^{(-j)},\mathbb{C}^{(-j)},\theta_0,\theta_1;\mu)}
\end{split}
\end{equation}\]</span> <span class="math inline">\(\mathbf{L}^{(-j)}\)</span>是除了<span class="math inline">\(L_j\)</span>外所有的文档标签，然后<span class="math inline">\(\mathbb{C}^{(-j)}\)</span>是除了<span class="math inline">\(W_j\)</span>外所有的文档集合。这个分布有2个结果，<span class="math inline">\(L_j=0\)</span>或<span class="math inline">\(L_j=1\)</span>。 要注意这里分子就是式(32)中的全联合概率分布。而分母仅仅除去了文档<span class="math inline">\(W_j\)</span> 的信息。然后我们来看看除去了该文档对整个式子造成了什么影响。</p>
<p>式（32）中第一个因式<span class="math inline">\(\frac{\Gamma(\gamma_{\pi_1}+\gamma_{\pi_0})}{\Gamma(\gamma_{\pi_1})\Gamma(\gamma_{\pi_0})}\)</span>是个常数，与<span class="math inline">\(W_j\)</span>无关，分子分母都有一个，所以计算的时候被约掉了。第二个式子是 <span class="math display">\[\begin{equation}
\frac{\Gamma(N+\gamma_{\pi_1}+\gamma_{\pi_0})}{\Gamma(C_1+\gamma_{\pi_1})\Gamma(C_0+\gamma_{\pi_0})}
\end{equation}\]</span> 现在我们来看看去掉了一个文档<span class="math inline">\(W_j\)</span>，发生了什么变化。首先，语料的大小从<span class="math inline">\(N\)</span>减小到<span class="math inline">\(N-1\)</span>。 如果文档<span class="math inline">\(L_j=0\)</span>，那么<span class="math inline">\(C_0^{(-j)}=C_0-1,C_1^{(-j)}=C_1\)</span>。反之如果文档<span class="math inline">\(L_j=1\)</span>，那么<span class="math inline">\(C_1^{(-j)}=C_1-1,C_0^{(-j)}=C_0\)</span>。我们令<span class="math inline">\(x \in {0,1}\)</span>这样就可以统一以上的情况，<span class="math inline">\(C_x^{(-j)}=C_x-1\)</span>。然后我们重写分子和分母的这两个式子，从 <span class="math display">\[\begin{equation}
\frac{\frac{\Gamma(C_1+\gamma_{\pi_1}}{\Gamma(N+\gamma_{\pi_1}+\gamma_{\pi_0}))\Gamma(C_0+\gamma_{\pi_0})}}{\frac{\Gamma(C_1^{(-j)}+\gamma_{\pi_1})\Gamma(C_0^{(-j)}+\gamma_{\pi_0})}{\Gamma(N+\gamma_{\pi_1}+\gamma_{\pi_0}-1)}}
\end{equation}\]</span> 到 <span class="math display">\[\begin{equation}
\frac{\Gamma(C_x+\gamma_{\pi_x})\Gamma(N+\gamma_{\pi_1}+\gamma_{\pi_0}-1)}{\Gamma(N+\gamma_{\pi_1}+\gamma_{\pi_0})\Gamma(C_x+\gamma_{\pi_x}-1)}
\end{equation}\]</span> 利用伽马函数<span class="math inline">\(\Gamma(a+1)=a\Gamma(a)\)</span>的性质，我们简化上面的式子最终得到 <span class="math display">\[\begin{equation}
\frac{C_x+\gamma_{\pi_x}-1}{N+\gamma_{\pi_1}+\gamma_{\pi_0}-1}
\end{equation}\]</span> <strong>（注意！！！这里原文又写错了，分子项少减了一个1</strong>）这样我们就把伽马函数消去了。再来看剩下的式子 <span class="math display">\[\begin{equation}
\prod^V_{i=1}\theta_{0,i}^{\mathcal{N}_{\mathbb{C}_0}(i)+\gamma_{\theta_i}-1} \theta_{1,i}^{\mathcal{N}_{\mathbb{C}_1}(i)+\gamma_{\theta_i}-1}
\end{equation}\]</span> 当我们去掉了文档<span class="math inline">\(W_j\)</span>之后，可以发现文档不属于的另一类是不会有变化的，会上下约掉。而文档属于的那一类，<span class="math inline">\(C_x^{(-j)}=C_x-1\)</span>，与文档<span class="math inline">\(W_j\)</span>无关的都被約掉，只剩下<span class="math inline">\(W_j\)</span>中的词，最后 <span class="math display">\[\begin{equation}
\prod^V_{i=1} \frac{\theta_{x,i}^{\mathcal{N}_{\mathbb{C}_x}(i)+\gamma_{\theta_i}-1}}{\theta_{x,i}^{\mathcal{N}_{\mathbb{C}_x^{(-j)}}(i)+\gamma_{\theta_i}-1}}=\prod^V_{i=1} \theta_{x,i}^{W_{ji}}
\end{equation}\]</span> 然后把（42）式和（44）式相乘，就得到了最后的公式，对于<span class="math inline">\(x\in{0,1}\)</span> <span class="math display">\[\begin{equation}
P(L_j=x|\mathbf{L})^{(-j)},\mathbb{C}^{(-j)},\theta_0,\theta_1;\mu)=\frac{C_x+\gamma_{\pi_x}-1}{N+\gamma_{\pi_1}+\gamma_{\pi_0}-1}\prod^V_{i=1} \theta_{x,i}^{W_{ji}}
\end{equation}\]</span> 这个式子清楚地表现了标签是如何被选择的。这个式子中，前半部分其实只有<span class="math inline">\(C_x\)</span>是变量，所以如果<span class="math inline">\(C_0\)</span>大，则算出来的<span class="math inline">\(P(L_j=0)\)</span>的概率就会大一点，所以下一次Lj的值就会倾向于C0，反之就会倾向于C1。 而后半部分，是在判断当前<span class="math inline">\(\theta\)</span>参数的情况下，这些词<span class="math inline">\(W_j\)</span>是否符合该参数所对应的分布（表现为似然值更大），然后确定整个文档是更倾向于C0还是C1。</p>
<p>最后（终于结束了！），我们从得到的条件概率中抽样：</p>
<ul>
<li><p>令<span class="math inline">\(value0\)</span>为式(34)<span class="math inline">\(x=0\)</span>时的值。</p></li>
<li><p>令<span class="math inline">\(value1\)</span>为式(34)<span class="math inline">\(x=1\)</span>时的值。</p></li>
<li><p>令概率分布为<span class="math inline">\(&lt;\frac{value0}{value0+value1},\frac{value1}{value0+value1}&gt;\)</span>。</p></li>
<li><p>根据这个分布进行贝努利实验（投两边概率不同的硬币）得到新的<span class="math inline">\(L_j^{(t+1)}\)</span>的值。</p></li>
</ul>
<h3 id="对theta采样">对<span class="math inline">\(\theta\)</span>采样</h3>
<p>接下來看看如何对<span class="math inline">\(\theta_0\)</span>和<span class="math inline">\(\theta_1\)</span>采样。因为这俩参数都是相互独立的，我们再一次删去他们的下标，统一地看做<span class="math inline">\(\theta\)</span>。将式(25)中与当前<span class="math inline">\(\theta\)</span>无关的参数删去，我们可以观察到 <span class="math display">\[\begin{equation}P(\theta|\mathbb{C},\mathbf{L});\mu) \propto P(\mathbb{C},\mathbf{L}|\theta)P(\theta|\mu)\end{equation}\]</span> 其实这时候式子里只剩下<span class="math inline">\(\theta_{x,i}^{\mathcal{N}_{\mathbb{C}_x}(i)+\gamma_{\theta_i}-1}\)</span>而已。可以看到，因为先验分布<span class="math inline">\(P(\theta|\mu)\)</span>是共轭的Dirichlet分布，所以后验分布也是Dirichlet分布，只是每个词项<span class="math inline">\(i\)</span>的参数变成了<span class="math inline">\(\mathcal{N}_{\mathbb{C}_x}(i)+\gamma_{\theta_i}\)</span>而已，我们定义<span class="math inline">\(V\)</span> 维的向量<span class="math inline">\(\mathbf{t}\)</span>,令<span class="math inline">\(t_i=\mathcal{N}_{\mathbb{C}_x}(i)+\gamma_{\theta_i}\)</span>，就有 <span class="math display">\[\begin{equation}\theta \sim Dirichlet(\mathbf{t})\end{equation}\]</span> 那么如何从一个Dirichlet分布采样出<span class="math inline">\(\theta\)</span>呢？（见的11.1.2的Reject Sampling）为了从参数为<span class="math inline">\(&lt;\alpha_1,\ldots,\alpha_V&gt;\)</span>的<span class="math inline">\(V\)</span>维Dirichlet分布中抽样出一个随机向量<span class="math inline">\(\mathbf{a}=&lt;a_1,\ldots,a_V&gt;\)</span>，我们只要从<span class="math inline">\(V\)</span>个伽马分布中各自采样一个独立的样本<span class="math inline">\(y_1,\ldots,y_V\)</span>，其中每个伽马分布的密度函数为 <span class="math display">\[\begin{equation}Gamma(\alpha_i,1)=\frac{y_i^{\alpha_i-1}e^{-y_i}}{\Gamma(\alpha_i)}\end{equation}\]</span> 然后令<span class="math inline">\(a_i=y_i/\sum^V_{j=1}y_j\)</span>即可。</p>
<h3 id="利用已有标签的文档">利用已有标签的文档</h3>
<p>我们可以利用已经被标签过的文档来，只需要不对这些文档采样新标签即可。这些被标注的文档会作为背景信息为算法服务。</p>
<h3 id="整个过程">整个过程</h3>
<p>最终的算法图：<a href="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130629204201.png" target="_blank" rel="external"><img src="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130629204201.png" alt="QQ截图20130629204201"></a></p>
<p>需要注意的是每次新的标签<span class="math inline">\(L_j\)</span>产生的时候，都会对接下来的文档标签数目统计产生影响，这就是Gibbs Sampler的本质。</p>
<h3 id="从gibbs-sampler产生值">从Gibbs sampler产生值</h3>
<p>Gibbs Sampling在每个循环都会产生变量的值，像之前提到过的那样，在理论上，变量<span class="math inline">\(Z_i\)</span>可以通过<span class="math inline">\(T\)</span>次获得的值近似 <span class="math display">\[\begin{equation}\frac{1}{T}\sum^T_{t=1}z^{(t)}_i\end{equation}\]</span> 但是实际中一般不直接这样用。</p>
<h3 id="收敛和burn-in迭代">收敛和burn-in迭代</h3>
<p>根据选择初始值的不同，Gibbs sampler需要一定的迭代次数才能保证点<span class="math inline">\(&lt;z_1^{t},z_2^{t},\ldots,z_k^{t}&gt;\)</span>都是从马尔科夫链的平稳分布中生成的（换句话说就是马尔科夫链需要一定的次数才能收敛）。为了避免在这之前的估计对结果产生的影响，一般都丢弃<span class="math inline">\(t&lt;B\)</span>之前的结果，之前的阶段就被称为“burn-in”阶段，所以取平均值的时候是从<span class="math inline">\(B+1\)</span>次到<span class="math inline">\(T\)</span>次的。</p>
<h3 id="自相关和lag">自相关和lag</h3>
<p>式（38）里的近似假设<span class="math inline">\(Z_i\)</span>的那些样本都是相互独立的，而事实上我们知道他们不是，因为新的点都是从前面的点所给的条件所生成的。这个问题被称作自相关(autocorrelation)。为了避免这个问题，很多Gibbs Sampling在实现的时候取每<span class="math inline">\(L\)</span>个值的平均值，这个<span class="math inline">\(L\)</span>被称作滞后（lag，我也不知道怎么翻译）。具体的探讨请见原文。</p>
<p>还有多链问题和超参数的选择问题，以及原文推荐的一些有用的文章，这里就不详述了。</p>
<h2 id="参考文献">参考文献：</h2>
<ol style="list-style-type: decimal">
<li>《Pattern Recognition and Machine Learning》</li>
<li>《LDA数学八卦》</li>
<li>http://www.xperseverance.net/blogs/2013/03/1682/</li>
<li>《Gibbs Sampling for the UniniTiated》</li>
</ol>

      
    </div>
    <footer class="article-footer">
      
        <a data-url="http://crescentluna.github.io/2013/06/29/Gibbs Sampling for the UniniTiated-2/" data-id="cizha1r08001xg0o8m73d06b2" class="article-share-link" data-share="baidu">分享到</a>
      

      
        <a href="http://crescentluna.github.io/2013/06/29/Gibbs Sampling for the UniniTiated-2/#ds-thread" class="article-comment-link">Comments</a>
      

      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Gibbs-Sampling/">Gibbs Sampling</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2013/06/29/Gibbs Sampling for the UniniTiated-3/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          《Gibbs Sampling for the UniniTiated》阅读笔记(下)---连续型参数求积分的思考
        
      </div>
    </a>
  
  
    <a href="/2013/06/29/Gibbs Sampling for the UniniTiated-1/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">《Gibbs Sampling for the UniniTiated》阅读笔记(上)---参数估计方法及Gibbs Sampling简介</div>
    </a>
  
</nav>

  
</article>


  <section id="comments">
    <div id="ds-thread" class="ds-thread" data-thread-key="2013/06/29/Gibbs Sampling for the UniniTiated-2/" data-title="《Gibbs Sampling for the UniniTiated》阅读笔记(中)---一个朴素贝叶斯文档模型例子" data-url="http://crescentluna.github.io/2013/06/29/Gibbs Sampling for the UniniTiated-2/"></div>
  </section>


</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">分类</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/学术/">学术</a><span class="category-list-count">18</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/学术/Gibbs-Sampling笔记/">Gibbs Sampling笔记</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/学术/变分推断笔记/">变分推断笔记</a><span class="category-list-count">3</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/机器学习/">机器学习</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/杂项/">杂项</a><span class="category-list-count">6</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/杂项/闲话/">闲话</a><span class="category-list-count">2</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/算法/">算法</a><span class="category-list-count">3</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签云</h3>
    <div class="widget tagcloud">
      <a href="/tags/DP/" style="font-size: 13.33px;">DP</a> <a href="/tags/EM/" style="font-size: 20px;">EM</a> <a href="/tags/Gibbs-Sampling/" style="font-size: 20px;">Gibbs Sampling</a> <a href="/tags/Hexo/" style="font-size: 13.33px;">Hexo</a> <a href="/tags/LDA/" style="font-size: 10px;">LDA</a> <a href="/tags/PRML/" style="font-size: 13.33px;">PRML</a> <a href="/tags/Variational-Inference/" style="font-size: 16.67px;">Variational Inference</a> <a href="/tags/kNN/" style="font-size: 10px;">kNN</a> <a href="/tags/matlab/" style="font-size: 10px;">matlab</a> <a href="/tags/tensorflow/" style="font-size: 10px;">tensorflow</a> <a href="/tags/wordpress/" style="font-size: 10px;">wordpress</a> <a href="/tags/二叉树/" style="font-size: 10px;">二叉树</a> <a href="/tags/决策树/" style="font-size: 10px;">决策树</a> <a href="/tags/动态规划/" style="font-size: 10px;">动态规划</a> <a href="/tags/参数估计/" style="font-size: 10px;">参数估计</a> <a href="/tags/多项式分布/" style="font-size: 10px;">多项式分布</a> <a href="/tags/数据挖掘/" style="font-size: 16.67px;">数据挖掘</a> <a href="/tags/机器学习/" style="font-size: 16.67px;">机器学习</a> <a href="/tags/概率图/" style="font-size: 10px;">概率图</a> <a href="/tags/盘子/" style="font-size: 10px;">盘子</a> <a href="/tags/社区发现/" style="font-size: 10px;">社区发现</a> <a href="/tags/背包问题/" style="font-size: 13.33px;">背包问题</a> <a href="/tags/贝叶斯方法/" style="font-size: 10px;">贝叶斯方法</a> <a href="/tags/高斯混合模型/" style="font-size: 16.67px;">高斯混合模型</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">归档</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/02/">二月 2017</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2014/12/">十二月 2014</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2014/02/">二月 2014</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2013/10/">十月 2013</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2013/08/">八月 2013</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2013/07/">七月 2013</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2013/06/">六月 2013</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2013/04/">四月 2013</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2013/03/">三月 2013</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2013/02/">二月 2013</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2013/01/">一月 2013</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2012/12/">十二月 2012</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2012/10/">十月 2012</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2012/09/">九月 2012</a><span class="archive-list-count">2</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">浏览数目</h3>
    <div class="widget">
      <ul class="popularlist">
      </ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">近期文章</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2017/02/23/install-tensorflow-with-gpu-support-for-ubuntu/">在Ubuntu上搭建GPU加速的TensorFlow环境</a>
          </li>
        
          <li>
            <a href="/2014/12/12/variational-inference-3/">变分推断学习笔记(3)——三硬币问题的变分推断解法</a>
          </li>
        
          <li>
            <a href="/2014/12/11/popular-widget/">使用LeanCloud平台为Hexo博客添加文章浏览量统计组件</a>
          </li>
        
          <li>
            <a href="/2014/12/11/relocation/">博客迁移小记</a>
          </li>
        
          <li>
            <a href="/2014/02/24/概率图模型简介/">概率图模型简介</a>
          </li>
        
      </ul>
    </div>
  </div>


  
    
<div class="widget-wrap">
  <h3 class="widget-title">最近评论</h3>
  <ul class="widget ds-recent-comments" data-num-items="5" data-show-avatars="0" data-show-title="1" data-show-time="1"></ul>
</div>
<!-- 需要多说的公用代码 -->


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">友情链接</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="https://github.com/xiangming/landscape-plus" target="_blank">landscape-plus主题</a>
          </li>
        
          <li>
            <a href="http://www.smallqiao.com/" target="_blank">小桥流水</a>
          </li>
        
          <li>
            <a href="http://www.ahathinking.com/" target="_blank">勇幸|Thinking</a>
          </li>
        
          <li>
            <a href="http://www.socona.me/" target="_blank">socona的博客</a>
          </li>
        
          <li>
            <a href="http://www.clarkchen.com/" target="_blank">陈曦师兄的博客</a>
          </li>
        
          <li>
            <a href="http://www.fengyafei.com/" target="_blank">小飞的博客</a>
          </li>
        
          <li>
            <a href="http://ariwaranosai.xyz/" target="_blank">今天拒绝负能量!</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2017 Chen Hao<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/xiangming/landscape-plus" target="_blank">Landscape-plus</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">主页</a>
  
    <a href="/archives" class="mobile-nav-link">所有文章</a>
  
    <a href="/about" class="mobile-nav-link">关于</a>
  
</nav>
    
<!-- 多说公共js代码 start -->
<script type="text/javascript">
var duoshuoQuery = {short_name:"crescent"};
  (function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0] 
     || document.getElementsByTagName('body')[0]).appendChild(ds);
  })();
  </script>
<!-- 多说公共js代码 end -->


<!-- 百度分享 start -->

<div id="article-share-box" class="article-share-box">
  <div id="bdshare" class="bdsharebuttonbox article-share-links">
    <a class="article-share-weibo" data-cmd="tsina" title="分享到新浪微博"></a>
    <a class="article-share-weixin" data-cmd="weixin" title="分享到微信"></a>
    <a class="article-share-qq" data-cmd="sqq" title="分享到QQ"></a>
    <a class="article-share-renren" data-cmd="renren" title="分享到人人网"></a>
    <a class="article-share-more" data-cmd="more" title="更多"></a>
  </div>
</div>
<script>window._bd_share_config={"common":{},"share":{"bdCustomStyle":"nocss.css"}};with(document)0[(getElementsByTagName("head")[0]||body).appendChild(createElement("script")).src="http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion="+~(-new Date()/36e5)];</script>

<!-- 百度分享 end -->

<script src="//libs.baidu.com/jquery/1.11.1/jquery.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<! -- mathjax config similar to math.stackexchange -->
<! -- add autoNumber settting -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
                processEscapes: true
                    
}
  
        });
</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
                  
}
    
        });
</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Queue(function() {
            var all = MathJax.Hub.getAllJax(), i;
            for(i=0; i < all.length; i += 1) {
                            all[i].SourceElement().parentNode.className += ' has-jax';
                                    
            }
                
        });
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<script src="/js/script.js"></script>

<!--page counter part-->
<script>
function addCount (Counter) {
        url=$('.article-date').attr('href').trim();
        title = $('.article-title').text().trim();
       // alert(title);
        var query=new AV.Query(Counter);
        //use url as unique idnetfication
        query.equalTo("url",url);
        query.find({
            success: function(results){
                if(results.length>0)
                {
                    var counter=results[0];
                    counter.fetchWhenSave(true); //get recent result
                    counter.increment("time");
                    counter.save();
                    //alert('find '+title+' and visit time is now '+ counter.get("time"));
                }
                else
                {
                    var newcounter=new Counter();
                    newcounter.set("title",title);
                    newcounter.set("url",url);
                    newcounter.set("time",1);
                    newcounter.save(null,{
                        success: function(newcounter){
                        //alert('New object created');
                        },
                        error: function(newcounter,error){
                        alert('Failed to create');
                        }
                        });
                }
            },
            error: function(error){
                //find null is not a error
                alert('Error:'+error.code+" "+error.message);
            }
        });
}

$(function(){
        var Counter=AV.Object.extend("Counter");
        //only increse visit counting when intering a page
        if ($('.article-title').length == 1)
           addCount(Counter);
        var query=new AV.Query(Counter);
        query.descending("time");
        query.limit(10);
        query.find({
            success: function(results){
                    for(var i=0;i<results.length;i++)    
                    {
                        var counter=results[i];
                        //alert(counter.get("title")+'-'+counter.get("url"));
                        title=counter.get("title");
                        url=counter.get("url");
                        time=counter.get("time");
                        // add to the popularlist widget
                        showcontent=title+" ("+time+")";
                        //notice the "" in href
                        $('.popularlist').append('<li><a href="'+url+'">'+showcontent+'</a></li>');
                    }
                },
            error: function(error){
                alert("Error:"+error.code+" "+error.message);
            }
            }
        )
        });
</script>


  </div><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="custom_mathjax_source">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
</body>
</html>