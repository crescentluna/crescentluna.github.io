<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>LDA学习笔记---来自《Parameter estimation for text analysis》 | 心怀畏惧</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="2013年10月10日更新。
LDA的概率图如下图1所示：
参数的意思如图2所示：
 根据模型，文章m的第n个词t是这样生成的：先从文章m的doc-topic分布中生成一个topic编号\(z_{m,n}\)，在根据编号第\(z_{m,n}\)个的topic-word分布中生成这个词，总够有\(K\)个topic，所以总的概率为： \[ p(w_{m,n}=t|\vec{\theta}_m,\un">
<meta property="og:type" content="article">
<meta property="og:title" content="LDA学习笔记---来自《Parameter estimation for text analysis》">
<meta property="og:url" content="http://crescentluna.github.io/2013/03/12/Parameter estimation for text analysis/index.html">
<meta property="og:site_name" content="心怀畏惧">
<meta property="og:description" content="2013年10月10日更新。
LDA的概率图如下图1所示：
参数的意思如图2所示：
 根据模型，文章m的第n个词t是这样生成的：先从文章m的doc-topic分布中生成一个topic编号\(z_{m,n}\)，在根据编号第\(z_{m,n}\)个的topic-word分布中生成这个词，总够有\(K\)个topic，所以总的概率为： \[ p(w_{m,n}=t|\vec{\theta}_m,\un">
<meta property="og:image" content="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130312094645.png">
<meta property="og:image" content="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130312094711.png">
<meta property="og:image" content="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130312094748.png">
<meta property="og:image" content="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130312094757.png">
<meta property="og:image" content="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130312094831.png">
<meta property="og:image" content="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130312094838.png">
<meta property="og:image" content="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130312094849.png">
<meta property="og:image" content="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130312101904.png">
<meta property="og:image" content="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130312100040.png">
<meta property="og:image" content="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130312100050.png">
<meta property="og:image" content="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130312095426.png">
<meta property="og:image" content="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130312094950.png">
<meta property="og:image" content="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130312100417.png">
<meta property="og:updated_time" content="2017-02-21T18:24:27.407Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="LDA学习笔记---来自《Parameter estimation for text analysis》">
<meta name="twitter:description" content="2013年10月10日更新。
LDA的概率图如下图1所示：
参数的意思如图2所示：
 根据模型，文章m的第n个词t是这样生成的：先从文章m的doc-topic分布中生成一个topic编号\(z_{m,n}\)，在根据编号第\(z_{m,n}\)个的topic-word分布中生成这个词，总够有\(K\)个topic，所以总的概率为： \[ p(w_{m,n}=t|\vec{\theta}_m,\un">
<meta name="twitter:image" content="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130312094645.png">
  
    <link rel="alternative" href="/atom.xml" title="心怀畏惧" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link rel="stylesheet" href="/css/style.css">
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
<script type="text/javascript">
var _bdhmProtocol = (("https:" == document.location.protocol) ? " https://" : " http://");
document.write(unescape("%3Cscript src='" + _bdhmProtocol + "hm.baidu.com/h.js%3F1fc57dc5ccfaeae31f8e295269b6fa04' type='text/javascript'%3E%3C/script%3E"));
</script>


  
<!-- Google Analytics -->
<script type="text/javascript">
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-57404093-1', 'auto');
ga('send', 'pageview');

</script>
<!-- End Google Analytics -->

  <script src="https://cdn1.lncld.net/static/js/av-min-1.2.1.js"></script>
  <script>AV.initialize("57nirzk8g437patyg9434lcvmpkat6e5lu5mixmrwqa8l3ao", "rsqg6iiq58eyeg8c6c6uue48crtwt5quuq5dmjaahfcjlkq8");</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">心怀畏惧</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">Do not go gentle into that good night</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">主页</a>
        
          <a class="main-nav-link" href="/archives">所有文章</a>
        
          <a class="main-nav-link" href="/about">关于</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="http://www.baidu.com/baidu" method="get" accept-charset="utf-8" class="search-form">
          <input type="search" name="word" maxlength="20" class="search-form-input" placeholder="Search">
          <input type="submit" value="" class="search-form-submit">
          <input name=tn type=hidden value="bds">
          <input name=cl type=hidden value="3">
          <input name=ct type=hidden value="2097152">
          <input type="hidden" name="si" value="crescentluna.github.io">
        </form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-Parameter estimation for text analysis" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2013/03/12/Parameter estimation for text analysis/" class="article-date">
  <time datetime="2013-03-11T18:20:25.000Z" itemprop="datePublished">2013-03-12</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/学术/">学术</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      LDA学习笔记---来自《Parameter estimation for text analysis》
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p><span style="color: #ff0000;">2013年10月10日更新。</span></p>
<p>LDA的概率图如下图1所示：<a href="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130312094645.png" target="_blank" rel="external"><img src="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130312094645.png" alt="QQ截图20130312094645"></a></p>
<p>参数的意思如图2所示：</p>
<p><a href="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130312094711.png" target="_blank" rel="external"><img src="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130312094711.png" alt="QQ截图20130312094711"></a> 根据模型，文章m的第n个词t是这样生成的：先从文章m的doc-topic分布中生成一个topic编号<span class="math inline">\(z_{m,n}\)</span>，在根据编号第<span class="math inline">\(z_{m,n}\)</span>个的topic-word分布中生成这个词，总够有<span class="math inline">\(K\)</span>个topic，所以总的概率为： <span class="math display">\[ p(w_{m,n}=t|\vec{\theta}_m,\underline{\Phi})=\sum^K_{k=1}p(w_{m,n}=t|\vec{\phi}_k)p(z_{m,n}=k|\vec{\theta}_m)\]</span> 如果我们写出这篇文章的complete-data的联合分布（<span style="color: #ff0000;">意思就是所以变量都已知的情况下</span>），那么式子就是这样的：</p>
<p><a href="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130312094748.png" target="_blank" rel="external"><img src="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130312094748.png" alt="QQ截图20130312094748"></a></p>
<p>通过对<span class="math inline">\(\vec{\vartheta_m}\)</span>（doc-topic分布）和<span class="math inline">\(\underline{\Phi}\)</span>（topic-word分布）积分以及<span class="math inline">\(z_{m,n}\)</span>求和，我们可以求得<span class="math inline">\(\vec{w_m}\)</span>的边缘分布：</p>
<p><a href="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130312094757.png" target="_blank" rel="external"><img src="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130312094757.png" alt="QQ截图20130312094757"></a></p>
<p>(<span style="color: #ff0000;">实际上这个边缘分布是求不出来的</span>，因为<span class="math inline">\(z_{m,n}\)</span>是隐藏变量，从而导致<span class="math inline">\(\underline{\vartheta}\)</span>与<span class="math inline">\(\underline{\Phi}\)</span>存在耦合现象，无法积分得到。要注意联合分布和边缘分布对Z乘积与加和的区别）</p>
<p>因为一个语料库有很多篇文章，而且文章之间都是相互独立的，所以整个语料库的似然为 <span class="math display">\[ p(\mathcal{W}|\vec{\alpha},\vec{\beta})=\prod^{M}_{m=1}p(\vec{w_m}|\vec{\alpha},\vec{\beta})\]</span></p>
<p>虽然LDA（latent Dirichlet allocation)是个相对简单的模型，对它直接推断一般也是不可行的，所以我们要采用近似推断的方法，比如Gibbs sampling。</p>
<a id="more"></a>
<h3 id="gibbs-sampling">Gibbs sampling</h3>
<p>Gibbs sampling是MCMC(Markov-chain Monte Carlo)算法的一种特殊情况，经常用于处理高维模型的近似推断。MCMC方法可以通过马尔科夫链的平稳分布模拟高维的概率分布<span class="math inline">\(p(\vec{x})\)</span>。当马尔科夫链经过了burn-in阶段，消除了初始参数的影响，进入平稳状态之后，它的每次转移都能生成一个<span class="math inline">\(p(\vec{x})\)</span>的样本。Gibbs samppling 是MCMC的特殊情况，它每次固定一个维度的<span class="math inline">\(x_i\)</span>,然后通过其他维度的数据（<span class="math inline">\(\vec{x}_{\neg i})\)</span>生成这个维度的样本。算法如下：</p>
<ol style="list-style-type: decimal">
<li><p>choose dimension i(random by permutation)。</p></li>
<li><p>sample <span class="math inline">\(x_i\)</span> from $p(x_i|_{i}) $。</p></li>
</ol>
<p>为了构造Gibbs抽样，我们必须知道条件概率<span class="math inline">\(p(x_i|\vec{x}_{\neg i})\)</span>，这个概率可以通过以下公式获得: <span class="math display">\[p(x_i|\vec{x}_{\neg i})=\frac{p(x_i,\vec{x}_{\neg i})}{p(\vec{x}_{\neg i})}=\frac{p(x_i,\vec{x}_{\neg i})}{\int{p(\vec{x})d x_i}}\]</span> 对于那些含有隐藏变量<span class="math inline">\(\vec{z}\)</span>的模型来说，通常需要求得他们的后验概率<span class="math inline">\(p(\vec{z}|\vec{x})\)</span>，对于这样的模型，Gibbs sampler的式子如下： <span class="math display">\[ p(z_i|\vec{z}_{\neg i},\vec{x})=\frac{p(\vec{z},\vec{x})}{p(\vec{z}_{\neg i},\vec{x})}=\frac{p(\vec{z},\vec{x})}{\int_z{p(\vec{z},\vec{x})d x_i}}\]</span> 当样本<span class="math inline">\(\tilde{\vec{z_r}},r\in[1,R]\)</span>的数量足够多时，隐藏变量的后验概率可以用以下式子来估计： <span class="math display">\[ p(\vec{z}|\vec{x})=\frac{1}{R}\sum^R_{r=1}\delta(\vec{z}-\tilde{\vec{z_r}})\]</span> 其中Kronecker delta $()={1 $ if <span class="math inline">\(\vec{u}=0;0\)</span> otherwise $ }$。</p>
<p>为了构造LDA的采样器，我们首先确定模型中的隐含变量为<span class="math inline">\(z_{m,n}\)</span>。而参数<span class="math inline">\(\underline{\Theta}\)</span>和<span class="math inline">\(\underline{\Phi}\)</span>都可以用观察到的<span class="math inline">\(w_{m,n}\)</span>和对应的<span class="math inline">\(z_{m,n}\)</span>求积分得到(<span style="color: #ff0000;">(这个方法叫collapsed Gibbs Sampling，即通过求积分去掉一些未知变量，使Gibbs Sampling的式子更加简单)</span>)。贝叶斯推断的目标是分布<span class="math inline">\(p(\vec{z}|\vec{w})\)</span>，它与联合分布成正比: <span class="math display">\[p(\vec{z}|\vec{w})=\frac{p(\vec{z},\vec{w})}{p(\vec{w})}=\frac{\prod^W_{i=1}p(z_i,w_i)}{\prod^W_{i=1}\sum^K_{k=1}p(z_i=k,w_i)}\]</span> 这里忽略了超参数（hyperparameter)<span class="math inline">\(\vec{\alpha}\)</span>和<span class="math inline">\(\vec{\beta}\)</span>。可以看到分母部分(也就是<span class="math inline">\(p(\vec{w}|\vec{\alpha},\vec{\beta})\)</span>)十分难求，它包括了<span class="math inline">\(K^W\)</span>个项的求和。所以我们使用Gibbs Sample方法，通过全部的条件分布<span class="math inline">\(p(z_i|\vec{z}_{\neg i},\vec{w})\)</span>来模拟得到<span class="math inline">\(p(\vec{z}|\vec{w})\)</span>。</p>
<h3 id="lda的联合分布">LDA的联合分布</h3>
<p>LDA的联合分布可以写成如下的式子（要记得这是个联合分布，所以Z都是已知的，所以<span class="math inline">\(\underline{\Theta}\)</span>和<span class="math inline">\(\underline{\Phi}\)</span>都被积分积掉了） <span class="math display">\[p(\vec{z},\vec{w}|\vec{\alpha},\vec{\beta})=p(\vec{w}|\vec{z},\vec{\beta})p(\vec{z}|\vec{\alpha})\]</span> 因为式子中的第一部分与<span class="math inline">\(\alpha\)</span>独立，第二部分与<span class="math inline">\(\beta\)</span>独立，所以两个式子可以分别处理。先看第一个分布<span class="math inline">\(p(\vec{w}|\vec{z})\)</span>，可以从观察到的词以及其主题的多项分布中生成： <span class="math display">\[ p(\vec{w}|\vec{z},\underline{\Phi})=\prod^W_{i=1}p(w_i|z_i)=\prod^W_{i=1}\varphi_{z_i,w_i}\]</span> 意思是，语料中的<span class="math inline">\(W\)</span>个词是根据主题<span class="math inline">\(z_i\)</span>观察到的独立多项分布。(我们把每个词看做独立的多项分布产生的结果，忽略顺序因素，所以没有多项分布的系数）。<span class="math inline">\(\varphi_{z_i,w_i}\)</span>是一个<span class="math inline">\(K\ast V\)</span>的矩阵，把词划分成主题和词汇表，公式如下： <span class="math display">\[ p(\vec{w}|\vec{z},\underline{\Phi})=\prod^K_{k=1}\prod_{i:z_i=k}p(w_i=t|z_i=k)=\prod^K_{k=1}\prod^V_{t=1}\varphi^{n^{(t)}_k}_{k,t}\]</span> <span class="math inline">\(n^{(t)}_k\)</span>代表了主题<span class="math inline">\(k\)</span>下词<span class="math inline">\(t\)</span>出现的次数。目标分布<span class="math inline">\(p(\vec{w}|\vec{z},\vec{\beta})\)</span>可以通过对<span class="math inline">\(\underline{\Phi}\)</span>求狄利克雷积分得到:</p>
<p><a href="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130312094831.png" target="_blank" rel="external"><img src="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130312094831.png" alt="QQ截图20130312094831"></a></p>
<p>类似地，主体分布<span class="math inline">\(p(\vec{z}|\vec{a})\)</span>也可以通过这种方法产生，<span class="math inline">\(\underline{\Theta}\)</span>为<span class="math inline">\(D*K\)</span>的矩阵，公式如下： <span class="math display">\[p(\vec{z}|\underline{\Theta})=\prod^W_{i=1}p(z_i|d_i)=\prod^M_{m=1}\prod^K_{k=1}p(z_i=k|d_i=m)=\prod^M_{m=1}\prod^K_{k=1}\theta^{n^{(k)}_m}_{m,k}\]</span> <span class="math inline">\(n^{(k)}_m\)</span>代表了文章<span class="math inline">\(m\)</span>下主题<span class="math inline">\(k\)</span>出现的次数。对<span class="math inline">\(\underline{\Theta}\)</span>求积分，我们得到:</p>
<p><a href="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130312094838.png" target="_blank" rel="external"><img src="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130312094838.png" alt="QQ截图20130312094838"></a></p>
<p>然后联合分布就变成了 <span class="math display">\[p(\vec{z},\vec{w}|\vec{\alpha},\vec{\beta})=\prod^K_{z=1}\frac{\Delta(\vec{n_z}+\vec{\beta})}{\Delta(\vec{\beta})}\cdot\prod^M_{m=1}\frac{\Delta(\vec{n_m}+\vec{\alpha})}{\Delta(\vec{\alpha})}\]</span></p>
<h3 id="完全条件分布full-conditional">完全条件分布(full conditional)</h3>
<p>我们令<span class="math inline">\(i=(m,n)\)</span>代表第<span class="math inline">\(m\)</span>篇文章中的第<span class="math inline">\(n\)</span>个词，<span class="math inline">\(\neg i\)</span>代表除去这个词之后剩下的其他词，令<span class="math inline">\(\vec{w}=\{w_i=t,\vec{w}_{\neg i}\}\)</span>，<span class="math inline">\(\vec{z}=\{z_i=k,\vec{z}_{\neg i}\}\)</span>，我们求得</p>
<p><a href="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130312094849.png" target="_blank" rel="external"><img src="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130312094849.png" alt="QQ截图20130312094849"></a></p>
<p>这个式子需要注意的：</p>
<ol style="list-style-type: decimal">
<li><p>因为忽略了<span class="math inline">\(p(w_i)\)</span>这个常数，所以后来的式子是<span class="math inline">\(\propto\)</span>成正比。</p></li>
<li><p>对于第<span class="math inline">\(m\)</span>篇文章中的第<span class="math inline">\(n\)</span>个词，其主题为<span class="math inline">\(k\)</span>。<span class="math inline">\(n^{(t)}_k=n^{(t)}_{k,\neg i}+1,n^{(k)}_m=n^{(k)}_{m,\neg i}+1\)</span>，对于其他文档和其他主题都没有影响。</p></li>
</ol>
<p>这个公式很漂亮，右边是<span class="math inline">\(p(topic|doc)\cdot p(word|topic)\)</span>，这个概率其实就是<span class="math inline">\(doc\rightarrow topic \rightarrow word\)</span>的路径概率，所以Gibbs Sampling 公式的物理意义就是在K条路径中采样。（图）</p>
<p><a href="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130312101904.png" target="_blank" rel="external"><img src="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130312101904.png" alt="QQ截图20130312101904"></a></p>
<h3 id="多项分布参数">多项分布参数</h3>
<p><a href="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130312100040.png" target="_blank" rel="external"><img src="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130312100040.png" alt="QQ截图20130312100040"></a></p>
<p><a href="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130312100050.png" target="_blank" rel="external"><img src="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130312100050.png" alt="QQ截图20130312100050"></a></p>
<p>根据图3和图4的Dirichlet-Multinomial结构，我们知道<span class="math inline">\(\vec{\theta_m}\)</span>和<span class="math inline">\(\vec{\phi_k}\)</span>的后验概率为:(令<span class="math inline">\(\mathcal{M}=\{\vec{w},\vec{z}\}\)</span>)（备注1）：</p>
<p><a href="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130312095426.png" target="_blank" rel="external"><img src="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130312095426.png" alt="QQ截图20130312095426"></a></p>
<p>最后，根据狄利克雷分布的期望<span class="math inline">\(&lt;Dir(\vec{a})&gt;=a_i/\sum_i{a_i}\)</span>（备注2），我们得到 <span class="math display">\[\phi_{k,t}=\frac{n^{(t)}_k+\beta_t}{\sum^V_{t=1}n^{(t)}_k+\beta_t}\]</span> <span class="math display">\[\theta_{m,k}=\frac{n^{(k)}_m+\alpha_k}{\sum^K_{k=1}n^{(k)}_m+\alpha_k}\]</span></p>
<p>最后，整个LDA算法的流程图为</p>
<p><a href="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130312094950.png" target="_blank" rel="external"><img src="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130312094950.png" alt="QQ截图20130312094950"></a> 备注： 1.狄利克雷分布的后验概率公式：</p>
<p><a href="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130312100417.png" target="_blank" rel="external"><img src="http://7sbo5n.com1.z0.glb.clouddn.com/QQ截图20130312100417.png" alt="QQ截图20130312100417"></a> 2.由于狄利克雷分布为： <span class="math display">\[Dir(\vec{p}|\vec{\alpha})=\frac{\Gamma(\sum^K_{k=1}\alpha_k)}{\prod^K_{k=1}\Gamma{(\alpha_k)}}\prod^K_{k=1}p_k^{\alpha_k-1}\]</span> 对于<span class="math inline">\(\vec{p}\)</span>中一项<span class="math inline">\(p_i\)</span>的期望为： <span class="math display">\[\begin{split} E(p_i) &amp;=\int^1_0 p_i\cdot Dir(\vec{p}|\vec{\alpha})dp \\ &amp;=\frac{\Gamma(\sum^K_{k=1}\alpha_k)}{\Gamma(\alpha_i)}\cdot\frac{\Gamma(\alpha_i+1)}{\Gamma(\sum^K_{k=1}\alpha_k+1)} \\ &amp;=\frac{\alpha_i}{\sum^K_{k=1}\alpha_k} \\\end{split}\]</span> 参考文献： 1.主要来自《Parameter estimation for text analysis》 2.《LDA数学八卦》</p>

      
    </div>
    <footer class="article-footer">
      
        <a data-url="http://crescentluna.github.io/2013/03/12/Parameter estimation for text analysis/" data-id="cizha1qzu000zg0o840tmuepd" class="article-share-link" data-share="baidu">分享到</a>
      

      
        <a href="http://crescentluna.github.io/2013/03/12/Parameter estimation for text analysis/#ds-thread" class="article-comment-link">Comments</a>
      

      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/LDA/">LDA</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/机器学习/">机器学习</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2013/03/26/Introdcution of Community Detection/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          社区发现及其发展方向简介（未完）
        
      </div>
    </a>
  
  
    <a href="/2013/03/01/Monty Hall problem/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">有趣的三门问题（蒙提霍尔问题）</div>
    </a>
  
</nav>

  
</article>


  <section id="comments">
    <div id="ds-thread" class="ds-thread" data-thread-key="2013/03/12/Parameter estimation for text analysis/" data-title="LDA学习笔记---来自《Parameter estimation for text analysis》" data-url="http://crescentluna.github.io/2013/03/12/Parameter estimation for text analysis/"></div>
  </section>


</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">分类</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/学术/">学术</a><span class="category-list-count">18</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/学术/Gibbs-Sampling笔记/">Gibbs Sampling笔记</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/学术/变分推断笔记/">变分推断笔记</a><span class="category-list-count">3</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/机器学习/">机器学习</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/杂项/">杂项</a><span class="category-list-count">6</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/杂项/闲话/">闲话</a><span class="category-list-count">2</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/算法/">算法</a><span class="category-list-count">3</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签云</h3>
    <div class="widget tagcloud">
      <a href="/tags/DP/" style="font-size: 13.33px;">DP</a> <a href="/tags/EM/" style="font-size: 20px;">EM</a> <a href="/tags/Gibbs-Sampling/" style="font-size: 20px;">Gibbs Sampling</a> <a href="/tags/Hexo/" style="font-size: 13.33px;">Hexo</a> <a href="/tags/LDA/" style="font-size: 10px;">LDA</a> <a href="/tags/PRML/" style="font-size: 13.33px;">PRML</a> <a href="/tags/Variational-Inference/" style="font-size: 16.67px;">Variational Inference</a> <a href="/tags/kNN/" style="font-size: 10px;">kNN</a> <a href="/tags/matlab/" style="font-size: 10px;">matlab</a> <a href="/tags/tensorflow/" style="font-size: 10px;">tensorflow</a> <a href="/tags/wordpress/" style="font-size: 10px;">wordpress</a> <a href="/tags/二叉树/" style="font-size: 10px;">二叉树</a> <a href="/tags/决策树/" style="font-size: 10px;">决策树</a> <a href="/tags/动态规划/" style="font-size: 10px;">动态规划</a> <a href="/tags/参数估计/" style="font-size: 10px;">参数估计</a> <a href="/tags/多项式分布/" style="font-size: 10px;">多项式分布</a> <a href="/tags/数据挖掘/" style="font-size: 16.67px;">数据挖掘</a> <a href="/tags/机器学习/" style="font-size: 16.67px;">机器学习</a> <a href="/tags/概率图/" style="font-size: 10px;">概率图</a> <a href="/tags/盘子/" style="font-size: 10px;">盘子</a> <a href="/tags/社区发现/" style="font-size: 10px;">社区发现</a> <a href="/tags/背包问题/" style="font-size: 13.33px;">背包问题</a> <a href="/tags/贝叶斯方法/" style="font-size: 10px;">贝叶斯方法</a> <a href="/tags/高斯混合模型/" style="font-size: 16.67px;">高斯混合模型</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">归档</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/02/">二月 2017</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2014/12/">十二月 2014</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2014/02/">二月 2014</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2013/10/">十月 2013</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2013/08/">八月 2013</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2013/07/">七月 2013</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2013/06/">六月 2013</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2013/04/">四月 2013</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2013/03/">三月 2013</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2013/02/">二月 2013</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2013/01/">一月 2013</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2012/12/">十二月 2012</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2012/10/">十月 2012</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2012/09/">九月 2012</a><span class="archive-list-count">2</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">浏览数目</h3>
    <div class="widget">
      <ul class="popularlist">
      </ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">近期文章</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2017/02/23/install-tensorflow-with-gpu-support-for-ubuntu/">在Ubuntu上搭建GPU加速的TensorFlow环境</a>
          </li>
        
          <li>
            <a href="/2014/12/12/variational-inference-3/">变分推断学习笔记(3)——三硬币问题的变分推断解法</a>
          </li>
        
          <li>
            <a href="/2014/12/11/popular-widget/">使用LeanCloud平台为Hexo博客添加文章浏览量统计组件</a>
          </li>
        
          <li>
            <a href="/2014/12/11/relocation/">博客迁移小记</a>
          </li>
        
          <li>
            <a href="/2014/02/24/概率图模型简介/">概率图模型简介</a>
          </li>
        
      </ul>
    </div>
  </div>


  
    
<div class="widget-wrap">
  <h3 class="widget-title">最近评论</h3>
  <ul class="widget ds-recent-comments" data-num-items="5" data-show-avatars="0" data-show-title="1" data-show-time="1"></ul>
</div>
<!-- 需要多说的公用代码 -->


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">友情链接</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="https://github.com/xiangming/landscape-plus" target="_blank">landscape-plus主题</a>
          </li>
        
          <li>
            <a href="http://www.smallqiao.com/" target="_blank">小桥流水</a>
          </li>
        
          <li>
            <a href="http://www.ahathinking.com/" target="_blank">勇幸|Thinking</a>
          </li>
        
          <li>
            <a href="http://www.socona.me/" target="_blank">socona的博客</a>
          </li>
        
          <li>
            <a href="http://www.clarkchen.com/" target="_blank">陈曦师兄的博客</a>
          </li>
        
          <li>
            <a href="http://www.fengyafei.com/" target="_blank">小飞的博客</a>
          </li>
        
          <li>
            <a href="http://ariwaranosai.xyz/" target="_blank">今天拒绝负能量!</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2017 Chen Hao<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/xiangming/landscape-plus" target="_blank">Landscape-plus</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">主页</a>
  
    <a href="/archives" class="mobile-nav-link">所有文章</a>
  
    <a href="/about" class="mobile-nav-link">关于</a>
  
</nav>
    
<!-- 多说公共js代码 start -->
<script type="text/javascript">
var duoshuoQuery = {short_name:"crescent"};
  (function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0] 
     || document.getElementsByTagName('body')[0]).appendChild(ds);
  })();
  </script>
<!-- 多说公共js代码 end -->


<!-- 百度分享 start -->

<div id="article-share-box" class="article-share-box">
  <div id="bdshare" class="bdsharebuttonbox article-share-links">
    <a class="article-share-weibo" data-cmd="tsina" title="分享到新浪微博"></a>
    <a class="article-share-weixin" data-cmd="weixin" title="分享到微信"></a>
    <a class="article-share-qq" data-cmd="sqq" title="分享到QQ"></a>
    <a class="article-share-renren" data-cmd="renren" title="分享到人人网"></a>
    <a class="article-share-more" data-cmd="more" title="更多"></a>
  </div>
</div>
<script>window._bd_share_config={"common":{},"share":{"bdCustomStyle":"nocss.css"}};with(document)0[(getElementsByTagName("head")[0]||body).appendChild(createElement("script")).src="http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion="+~(-new Date()/36e5)];</script>

<!-- 百度分享 end -->

<script src="//libs.baidu.com/jquery/1.11.1/jquery.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<! -- mathjax config similar to math.stackexchange -->
<! -- add autoNumber settting -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
                processEscapes: true
                    
}
  
        });
</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
                  
}
    
        });
</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Queue(function() {
            var all = MathJax.Hub.getAllJax(), i;
            for(i=0; i < all.length; i += 1) {
                            all[i].SourceElement().parentNode.className += ' has-jax';
                                    
            }
                
        });
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<script src="/js/script.js"></script>

<!--page counter part-->
<script>
function addCount (Counter) {
        url=$('.article-date').attr('href').trim();
        title = $('.article-title').text().trim();
       // alert(title);
        var query=new AV.Query(Counter);
        //use url as unique idnetfication
        query.equalTo("url",url);
        query.find({
            success: function(results){
                if(results.length>0)
                {
                    var counter=results[0];
                    counter.fetchWhenSave(true); //get recent result
                    counter.increment("time");
                    counter.save();
                    //alert('find '+title+' and visit time is now '+ counter.get("time"));
                }
                else
                {
                    var newcounter=new Counter();
                    newcounter.set("title",title);
                    newcounter.set("url",url);
                    newcounter.set("time",1);
                    newcounter.save(null,{
                        success: function(newcounter){
                        //alert('New object created');
                        },
                        error: function(newcounter,error){
                        alert('Failed to create');
                        }
                        });
                }
            },
            error: function(error){
                //find null is not a error
                alert('Error:'+error.code+" "+error.message);
            }
        });
}

$(function(){
        var Counter=AV.Object.extend("Counter");
        //only increse visit counting when intering a page
        if ($('.article-title').length == 1)
           addCount(Counter);
        var query=new AV.Query(Counter);
        query.descending("time");
        query.limit(10);
        query.find({
            success: function(results){
                    for(var i=0;i<results.length;i++)    
                    {
                        var counter=results[i];
                        //alert(counter.get("title")+'-'+counter.get("url"));
                        title=counter.get("title");
                        url=counter.get("url");
                        time=counter.get("time");
                        // add to the popularlist widget
                        showcontent=title+" ("+time+")";
                        //notice the "" in href
                        $('.popularlist').append('<li><a href="'+url+'">'+showcontent+'</a></li>');
                    }
                },
            error: function(error){
                alert("Error:"+error.code+" "+error.message);
            }
            }
        )
        });
</script>


  </div><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="custom_mathjax_source">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
</body>
</html>